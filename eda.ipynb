{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": "1",
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Table of Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "662.717px",
        "left": "35px",
        "top": "111.133px",
        "width": "392.938px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "eda.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-2upAAKOMuE",
        "outputId": "80eee064-a76b-4ea3-e51c-733533a511ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_bill</th>\n",
              "      <th>tip</th>\n",
              "      <th>sex</th>\n",
              "      <th>smoker</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16.99</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.34</td>\n",
              "      <td>1.66</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.01</td>\n",
              "      <td>3.50</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.68</td>\n",
              "      <td>3.31</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24.59</td>\n",
              "      <td>3.61</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>29.03</td>\n",
              "      <td>5.92</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>27.18</td>\n",
              "      <td>2.00</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>22.67</td>\n",
              "      <td>2.00</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>17.82</td>\n",
              "      <td>1.75</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>18.78</td>\n",
              "      <td>3.00</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Thur</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     total_bill   tip     sex smoker   day    time  size\n",
              "0         16.99  1.01  Female     No   Sun  Dinner     2\n",
              "1         10.34  1.66    Male     No   Sun  Dinner     3\n",
              "2         21.01  3.50    Male     No   Sun  Dinner     3\n",
              "3         23.68  3.31    Male     No   Sun  Dinner     2\n",
              "4         24.59  3.61  Female     No   Sun  Dinner     4\n",
              "..          ...   ...     ...    ...   ...     ...   ...\n",
              "239       29.03  5.92    Male     No   Sat  Dinner     3\n",
              "240       27.18  2.00  Female    Yes   Sat  Dinner     2\n",
              "241       22.67  2.00    Male    Yes   Sat  Dinner     2\n",
              "242       17.82  1.75    Male     No   Sat  Dinner     2\n",
              "243       18.78  3.00  Female     No  Thur  Dinner     2\n",
              "\n",
              "[244 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VK6A0UQZhpW",
        "outputId": "5676f7e5-fdc3-4958-d0e0-8e5677210ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "pip install plotly==4.14.3"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting plotly==4.14.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/f6/bd3c17c8003b6641df1228e80e1acac97ed8402635e46c2571f8e1ef63af/plotly-4.14.3-py2.py3-none-any.whl (13.2MB)\n",
            "\u001b[K     |████████████████████████████████| 13.2MB 322kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==4.14.3) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly==4.14.3) (1.3.3)\n",
            "Installing collected packages: plotly\n",
            "  Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "Successfully installed plotly-4.14.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_plotly_utils",
                  "plotly"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg9Dl__yFM42"
      },
      "source": [
        "Studentised outlier detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dPVrJ4gir2V"
      },
      "source": [
        "# import ml_package as ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTlmBXQ2Yyvt"
      },
      "source": [
        "##Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-14T16:16:36.210199Z",
          "start_time": "2020-01-14T16:16:36.205423Z"
        },
        "id": "FxTpU3L9Yyvw"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "# Introduction <a name=\"intro\"></a>\n",
        "\n",
        "Exploratory Data Analysis (EDA) is an approach to analyzing data sets and summarizing their main characteristics with visualizations. It is an essential step before modeling in any data analytics project. In order to better [understand the data](#data_types) associated with the problem, we need to perform certain activities to ensure that we get relevant insights and decide on the appropriate next steps.\n",
        "\n",
        "\n",
        "\n",
        "## Installing the necessary packages (Mandatory)\n",
        "\n",
        "Please ensure all the packages are installed in your system. Follow the instructions in __README__ file before executing any cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KracYEUQYyvx"
      },
      "source": [
        "## Prerequisite Functions (for aesthetics)\n",
        "\n",
        "The following chunks of codes help improve the usability of the notebook. Please make sure you execute these codes before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPauXnrYyvx"
      },
      "source": [
        "The code in the following cell enables __suppressing warnings__ that can get generated across the Notebook and __initiating seed__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:08.816596Z",
          "start_time": "2020-12-14T14:28:08.804611Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "KsYcffZWYyvy",
        "outputId": "5fdb5f93-d48b-45ae-e4e0-75f18969354a"
      },
      "source": [
        "# # importing libraries for setting random seed and supress warnings\n",
        "\n",
        "# from IPython.display import HTML, display, Markdown, clear_output\n",
        "# import warnings\n",
        "# import random\n",
        "# from IPython import get_ipython\n",
        "\n",
        "# # supress warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # setting random seed - 60 in this case\n",
        "# random.seed(60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #1 for <b>suppressing warning</b> is EXECUTED!</span>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4YGGXXGYyvz"
      },
      "source": [
        "The code in the following cell __center aligns__ the plots and images generated in the notebook console and creates a __loading symbol__ to indicate a cell processing to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:10.098158Z",
          "start_time": "2020-12-14T14:28:10.090146Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "ELa7vVIIYyv0",
        "outputId": "f8d9e3c6-07f2-4828-fa77-41e4f6034ee9"
      },
      "source": [
        "# # setting the notebook output to be in the center and creating loading symbol\n",
        "\n",
        "# display(HTML(\"\"\"\n",
        "# <style>\n",
        "# .output_png img {\n",
        "#     display: block;\n",
        "#     margin-left: auto;\n",
        "#     margin-right: auto;\n",
        "# }\n",
        " \n",
        "# .loader {\n",
        "#   border: 5px solid #f3f3f3;\n",
        "#   border-radius: 50%;\n",
        "#   border-top: 5px solid teal;\n",
        "#   border-right: 5px solid grey;\n",
        "#   border-bottom: 5px solid maroon;\n",
        "#   border-left: 5px solid tan;\n",
        "#   width: 20px;\n",
        "#   height: 20px;\n",
        "#   -webkit-animation: spin 1s linear infinite;\n",
        "#   animation: spin 1s linear infinite;\n",
        "#   float: left;\n",
        "# }\n",
        "\n",
        "# @-webkit-keyframes spin {\n",
        "#   0% { -webkit-transform: rotate(0deg); }\n",
        "#   100% { -webkit-transform: rotate(360deg); }\n",
        "# }\n",
        "\n",
        "# @keyframes spin {\n",
        "#   0% { transform: rotate(0deg); }\n",
        "#   100% { transform: rotate(360deg); }\n",
        "# }\n",
        "\n",
        "# </style>\n",
        "# \"\"\"))\n",
        "\n",
        "# display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              ".output_png img {\n",
              "    display: block;\n",
              "    margin-left: auto;\n",
              "    margin-right: auto;\n",
              "}\n",
              " \n",
              ".loader {\n",
              "  border: 5px solid #f3f3f3;\n",
              "  border-radius: 50%;\n",
              "  border-top: 5px solid teal;\n",
              "  border-right: 5px solid grey;\n",
              "  border-bottom: 5px solid maroon;\n",
              "  border-left: 5px solid tan;\n",
              "  width: 20px;\n",
              "  height: 20px;\n",
              "  -webkit-animation: spin 1s linear infinite;\n",
              "  animation: spin 1s linear infinite;\n",
              "  float: left;\n",
              "}\n",
              "\n",
              "@-webkit-keyframes spin {\n",
              "  0% { -webkit-transform: rotate(0deg); }\n",
              "  100% { -webkit-transform: rotate(360deg); }\n",
              "}\n",
              "\n",
              "@keyframes spin {\n",
              "  0% { transform: rotate(0deg); }\n",
              "  100% { transform: rotate(360deg); }\n",
              "}\n",
              "\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjNvfz3DYyv0"
      },
      "source": [
        "The code in the following cell will enable to keep __track of the activities__ performed across the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:22.436900Z",
          "start_time": "2020-12-14T14:28:11.292064Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "EW6F-iYjYyv1",
        "outputId": "9e0ad61f-f7f4-4420-db61-c26c409d4e88"
      },
      "source": [
        "# # code for tracking cell execution\n",
        "# import os\n",
        "# # import getpass\n",
        "# import platform\n",
        "# # import json\n",
        "# # import requests\n",
        "# from datetime import datetime\n",
        "# from pytz import timezone \n",
        "# import time\n",
        "\n",
        "# # Language version\n",
        "# py_version = platform.python_version()\n",
        "\n",
        "# # Get current indian time\n",
        "# india_timezone = timezone('Asia/Kolkata')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #3 for <b>tracking cells</b> is EXECUTED!</span>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UcXJCK8Yyv2"
      },
      "source": [
        "The code in the following cell will help in __generating bokeh plots in the HTML report__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:27.362015Z",
          "start_time": "2020-12-14T14:28:22.442060Z"
        },
        "id": "thfx5-PpYyv3"
      },
      "source": [
        "# # enabling bokeh plots to be rendered in HTML report\n",
        "\n",
        "# from jinja2 import Template\n",
        "# from bokeh.embed import components\n",
        "\n",
        "# html_plot = Template(\"\"\"\n",
        "# <!DOCTYPE html>\n",
        "# <html lang=\"en-US\">\n",
        "\n",
        "# <link\n",
        "#     href=\"http://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.css\"\n",
        "#     rel=\"stylesheet\" type=\"text/css\"\n",
        "# >\n",
        "# <script src=\"http://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\"></script>\n",
        "# <script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-1.4.0.min.js\"></script>\n",
        "# <script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-tables-1.4.0.min.js\"></script>\n",
        "\n",
        "# <body>\n",
        "#     {{ script }}\n",
        "#     {{ div }}\n",
        "# </body>\n",
        "\n",
        "# </html>\n",
        "# \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANlUy2gNYyv4"
      },
      "source": [
        "The code in the following cell is a function that will help to __display dataframes using Plotly with scrollable rows__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:27.703013Z",
          "start_time": "2020-12-14T14:28:27.366608Z"
        },
        "code_folding": [],
        "id": "L05_yLCLYyv5"
      },
      "source": [
        "# import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# def display_data(data_table, sel_col=None):\n",
        "#     data_table_series = [data_table[i] for i in data_table.columns]\n",
        "    \n",
        "#     cell_color = []\n",
        "#     n = len(data_table)\n",
        "#     for col in data_table.columns:\n",
        "#         if sel_col is None:\n",
        "#             cell_color.append(['rgba(0,191,255,0.5)']*n)\n",
        "#         else:\n",
        "#             if col!=sel_col:\n",
        "#                 cell_color.append(['rgba(0,191,255,0.5)']*n)\n",
        "#             else:\n",
        "#                 cell_color.append(['paleturquoise']*n)\n",
        "\n",
        "#     fig = go.Figure(data=[go.Table(\n",
        "#         header=dict(values=list(data_table.columns),\n",
        "#                     fill_color='grey',\n",
        "#                     align='center',\n",
        "#                     font=dict(color='white', size=15)\n",
        "#                    ),\n",
        "#         cells=dict(values=data_table_series,\n",
        "#                    fill_color=cell_color,\n",
        "#                    align='center',\n",
        "#                    font=dict(color='black', size=10)\n",
        "#                   ))\n",
        "#     ])\n",
        "\n",
        "#     if data_table.shape[0] == 1:\n",
        "#         fig_ht = 75\n",
        "#     elif data_table.shape[0] >= 2 and data_table.shape[0] <= 9:\n",
        "#         fig_ht = 40*data_table.shape[0]\n",
        "#     else:\n",
        "#         fig_ht = 400\n",
        "    \n",
        "#     fig.update_layout(width=150*len(data_table.columns), \n",
        "#                       height=fig_ht,\n",
        "#                       margin=dict(l=0,r=0,b=0,t=0,pad=0))\n",
        "#     fig.show(config={'displaylogo': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGYgJKv3Yyv6"
      },
      "source": [
        "***\n",
        "\n",
        "# Import libraries \n",
        "\n",
        "## Importing libraries for data loading and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:37.255698Z",
          "start_time": "2020-12-14T14:28:27.707596Z"
        },
        "id": "jqIarkakYyv7"
      },
      "source": [
        "# from math import ceil, sqrt, pi, isnan\n",
        "# import cmath\n",
        "# import re\n",
        "# from pathlib import Path\n",
        "# from itertools import combinations, groupby\n",
        "# from io import StringIO\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# pd.set_option('display.max_columns', 500)\n",
        "\n",
        "# from scipy import stats\n",
        "# from scipy.spatial import Voronoi\n",
        "\n",
        "# import statsmodels.api as sm\n",
        "\n",
        "# from statsmodels.formula.api import ols\n",
        "# from statsmodels.stats.anova import anova_lm\n",
        "# from statsmodels.stats.multicomp import MultiComparison\n",
        "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# from statsmodels.multivariate.factor import Factor, FactorResults\n",
        "# from statsmodels.tools.tools import add_constant\n",
        "# from statsmodels.regression.linear_model import OLS\n",
        "\n",
        "# from factor_analyzer import FactorAnalyzer, Rotator\n",
        "# from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
        "\n",
        "# from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, MinMaxScaler\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# from sklearn.manifold import MDS\n",
        "\n",
        "# import pymssql\n",
        "# import psycopg2\n",
        "# import pandas.io.sql as psql\n",
        "\n",
        "# from matplotlib.cbook import boxplot_stats\n",
        "\n",
        "# from shapely.geometry import Point\n",
        "# from shapely.geometry.polygon import Polygon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zE00YToYyv7"
      },
      "source": [
        "## Importing libraries for charts and visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:28:41.294515Z",
          "start_time": "2020-12-14T14:28:37.259372Z"
        },
        "id": "z7E2qzSYYyv8"
      },
      "source": [
        "# from termcolor import colored\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "# from matplotlib import font_manager, rcParams\n",
        "# import matplotlib.colors as mcolors\n",
        "# import matplotlib.cm as cm\n",
        "\n",
        "# from statsmodels.graphics.mosaicplot import mosaic\n",
        "\n",
        "# from tqdm.notebook import tqdm\n",
        "\n",
        "# import networkx as nx \n",
        "\n",
        "# from IPython.display import display, Markdown, HTML\n",
        "\n",
        "# import statsmodels.api as sm\n",
        "\n",
        "# import plotly\n",
        "# import plotly.express as px\n",
        "# import plotly.graph_objects as go\n",
        "# from plotly.offline import plot, iplot\n",
        "# import plotly.io as pio\n",
        "# pio.renderers.default = \"notebook\"\n",
        "\n",
        "# from bokeh.models.widgets import Panel, Tabs, TextInput, DataTable, TableColumn\n",
        "# from bokeh.io import output_file, show, reset_output, output_notebook\n",
        "# from bokeh.plotting import figure\n",
        "# from bokeh.layouts import column, row\n",
        "# from bokeh.models import ColumnDataSource, Plot, LinearAxis, Grid, Range1d, Band, LinearColorMapper, CategoricalColorMapper, ColorBar, FactorRange\n",
        "# from bokeh.models.glyphs import Text\n",
        "# from bokeh.models.tools import HoverTool, SaveTool, ResetTool\n",
        "# from bokeh.palettes import Category20\n",
        "# from bokeh.transform import factor_cmap\n",
        "# from plotly.subplots import make_subplots\n",
        "# from itertools import combinations, groupby, product\n",
        "\n",
        "# reset_output()\n",
        "# output_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doax_hCiYyv8"
      },
      "source": [
        "***\n",
        "\n",
        "# Import Dataset\n",
        "\n",
        "Data Loading can be done in 3 ways from different data sources:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIMA0hJjYywA"
      },
      "source": [
        "***\n",
        "\n",
        "# Dataset Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYtyhlwyYywA"
      },
      "source": [
        "## Quick peek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:29:19.821950Z",
          "start_time": "2020-12-14T14:29:19.714421Z"
        },
        "id": "1e9Jf8LuYywA"
      },
      "source": [
        "# replacing any empty cell of data type string in any column with NaN\n",
        "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "# replacing inf with NaN\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print('_'*75+'\\n')  ### adding a separator\n",
        "display(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHzD3aIOYywB"
      },
      "source": [
        "## Formatting & renaming columns\n",
        "\n",
        "##### Formatting Columns\n",
        "\n",
        "Here we lower case every column name and if there is any _white space_ or _dot_ `.` or _comma_ `,` separating words in any column name, it is replaced with an underscore for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:29:21.865051Z",
          "start_time": "2020-12-14T14:29:21.428116Z"
        },
        "scrolled": true,
        "id": "gnRZ84BMYywB"
      },
      "source": [
        "# before formatting\n",
        "original_cols = df.columns\n",
        "cols_vis = ' || '.join(original_cols)\n",
        "print(colored(\"\\nColumn Names (Before Formatting):\",'magenta',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
        "\n",
        "# list of special characters to be handled while formatting column names to avoid any error\n",
        "special_chars = r'[?|$|#|@|#|%|!|*|(|)|{|}|^|,|.|-|/|>|<|;|:]'\n",
        "\n",
        "# lower-case and replace any white-space '_' for every column name\n",
        "df.columns = list(map(lambda x:re.sub(special_chars,r'',x.lower().replace(' ','_').replace(\"'\",\"\").replace(\"|\",\"_\")), df.columns))\n",
        "\n",
        "updated_cols = df.columns\n",
        "cols_vis = ' || '.join(updated_cols)\n",
        "print(colored(\"\\nColumn Names (After Formatting):\",'blue',attrs=['bold']),\"\\n{}\".format(cols_vis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYhTB_ZYywC"
      },
      "source": [
        "## Assigning a unique row identifier\n",
        "\n",
        "The column selected will be treated as the unique row identifier throughout the notebook. It will be neglected while performing any analysis on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:30:06.994284Z",
          "start_time": "2020-12-14T14:29:53.480692Z"
        },
        "id": "FLETUUEBYywD"
      },
      "source": [
        "# assigning row identifier\n",
        "value=\"Assigning row identifier\"\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        display(Markdown('\\nEnter the name of column to be the __row identifier__. Press `Enter` to skip.'))\n",
        "\n",
        "        # the list of columns in dataframe\n",
        "        original_cols = df.columns\n",
        "        cols_vis = ' || '.join(original_cols)\n",
        "        print(colored(\"\\nColumn Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
        "        print('_'*75)\n",
        "\n",
        "        while True:\n",
        "            row_id_input = input(\"\\nEnter the column name: \")\n",
        "\n",
        "            # if user enters 'None', then it will break the infinite loop and skip this operation\n",
        "            if row_id_input == '':\n",
        "                clear_output()\n",
        "                print('_'*50)\n",
        "                display(Markdown('__No unique row identifier selected!__'))\n",
        "                print('_'*50)\n",
        "                break\n",
        "            else:\n",
        "                if row_id_input not in df.columns:\n",
        "                    print(colored(\"\\nPlease enter the column name properly!\",\"red\",attrs=['bold']))\n",
        "                    continue\n",
        "                else:\n",
        "                    clear_output()\n",
        "                    display(Markdown(\"The column which will be considered as the __unique row identifier__ is: __{}__\".format(row_id_input)))\n",
        "                    break\n",
        "    else:\n",
        "        row_id_input = 'hotel_id'\n",
        "        display(Markdown(\"The column which will be considered as the __unique row identifier__ is: __{}__\".format(row_id_input)))\n",
        "\n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYnmBGaCYywD"
      },
      "source": [
        "##### Ignoring date-time column\n",
        "\n",
        "In this section, we will choose the date-time column (if any) for ignoring it from analyses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:30:38.969507Z",
          "start_time": "2020-12-14T14:30:08.938126Z"
        },
        "id": "o9a8brU7YywD"
      },
      "source": [
        "# ignoring date-time\n",
        "value=\"Ignoring date-time\"\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        while True:\n",
        "            dt_col = input(\"\\nIs there a Date-Time column in the dataset? (Y/N) \")\n",
        "\n",
        "            # if user enters 'n', then it will break the infinite loop and skip this operation\n",
        "            if dt_col.lower() == 'n' or dt_col == '':\n",
        "                clear_output()\n",
        "                display(Markdown(\"__Operation to ignore datetime column skipped!__\"))\n",
        "                date_time_col = ''\n",
        "                break\n",
        "            elif dt_col.lower() == 'y':\n",
        "                dt_col_in = input('\\nEnter the column names (comma `,` for multiple): ')\n",
        "\n",
        "                if ',' not in dt_col_in:\n",
        "                    if ' ' in dt_col_in:\n",
        "                        print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
        "                        continue\n",
        "                    else:\n",
        "                        if dt_col_in not in df.columns:\n",
        "                            print(colored(\"\\nINCORRECT COLUMN NAME. Please try again!\",\"red\",attrs=['bold']))\n",
        "                            continue\n",
        "                        else:\n",
        "                            date_time_col = dt_col_in\n",
        "                            print(colored(\"\\nThe date-time column to be ignored is:\",'green',attrs=['bold']),date_time_col)\n",
        "                            break\n",
        "                # if it is multiple column entry and properly entered, then it is stored in list\n",
        "                else:\n",
        "                    # split the string with ',' and convert it into list\n",
        "                    # check if any element is empty, then eliminate it\n",
        "                    date_time_col = [i.strip() for i in dt_col_in.split(',') if i]\n",
        "\n",
        "                    # if all the column names are correctly entered, then proceed\n",
        "                    if all(item in df.columns for item in date_time_col):\n",
        "                        print(colored(\"\\nThe date-time column to be ignored is:\",'green',attrs=['bold']),date_time_col)\n",
        "                        break\n",
        "                    else:\n",
        "                        print(colored(\"\\nINCORRECT COLUMN NAME(s). Please enter all the names again!\",\"red\",attrs=['bold']))\n",
        "                        continue        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E50ZJrfYywD"
      },
      "source": [
        "## Changing data type of columns\n",
        "\n",
        "In this section, data types for selected column(s) can be updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:30:54.321271Z",
          "start_time": "2020-12-14T14:30:41.155142Z"
        },
        "id": "vrYizHpYYywE"
      },
      "source": [
        "# type-cast columns for proper analysis\n",
        "value='Type Cast Columns'\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        # list of special characters to be ignored\n",
        "        special_chars = r'[?|$|#|@|#|%|!|*|(|)|{|}|^|,|.|-|/|>|<|;|:]'\n",
        "        \n",
        "        # displaying the instruction\n",
        "        display(Markdown('__PLEASE READ THE INSTRUCTION TO TYPE-CAST COLUMN BEFORE PROCEEDING!__'))\n",
        "        display(Markdown('Enter the column name from the list shown, press `Enter` and then give your data type.'))\n",
        "        display(Markdown('Enter the name of column(s) comma `,` separated for __multiple inputs__.'))\n",
        "        display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
        "        print('_'*75)\n",
        "\n",
        "        # the list of columns in dataframe\n",
        "        display(Markdown('__Original data type of columns:__'))\n",
        "        display(df.dtypes)\n",
        "        print('_'*75)\n",
        "\n",
        "        col_type_cast = []\n",
        "\n",
        "        # user will enter column names and it will keep on going for infinite loop \n",
        "        # until it is in correct format\n",
        "        while True:\n",
        "            col_type_cast_input = input(\"\\nEnter the column name(s): \")\n",
        "\n",
        "            # if user enters 'None', then it will break the infinite loop and skip this operation\n",
        "            if col_type_cast_input == '':\n",
        "                col_type_cast = []\n",
        "                break\n",
        "            else:\n",
        "                # if it is a single column entry, keep on asking for input unless correctly entered\n",
        "                if ',' not in col_type_cast_input:\n",
        "                    if ' ' in col_type_cast_input:\n",
        "                        print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
        "                        continue\n",
        "                    else:\n",
        "                        if col_type_cast_input in df.columns:\n",
        "                            col_type_cast = [col_type_cast_input]\n",
        "                            break\n",
        "                        else:\n",
        "                            print(colored(\"\\nIncorrect column name! Please try again.\",\"red\",attrs=['bold']))\n",
        "                            continue\n",
        "                # if it is multiple column entry and properly entered, then it is stored in list\n",
        "                else:\n",
        "                    # split the string with ',' and convert it into list\n",
        "                    # check if any element is empty, then eliminate it\n",
        "                    # remove extra white spaces from column name using `strip`\n",
        "\n",
        "                    col_type_cast = [i.strip() for i in col_type_cast_input.split(',') if i]\n",
        "                    break\n",
        "\n",
        "        if col_type_cast == []:\n",
        "            clear_output()\n",
        "            display(Markdown('__No data type conversion took place!__'))\n",
        "            display(Markdown('__Original data type of the columns is:__'))\n",
        "            print('_'*75)\n",
        "            pass\n",
        "        else:\n",
        "            for each_col in col_type_cast:\n",
        "                # ask the user the desired type cast\n",
        "                what_type = input('\\nEnter the data type (int/str/float) for column `{}`: '.format(each_col))\n",
        "                if 'int' in what_type:\n",
        "                    try:\n",
        "                        if df[each_col].dtype == object:\n",
        "                            df[each_col] = df[each_col].str.replace('|','')\n",
        "                            df[each_col] = df[each_col].map(lambda x:re.sub(special_chars,r'',x)).astype('int64')\n",
        "                        else:\n",
        "                            df[each_col] = df[each_col].astype('int64')\n",
        "                    except Exception as err:\n",
        "                        # display the error\n",
        "                        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "                    else:\n",
        "                        print(colored('\\nCONVERTED!:','green',attrs=['bold']))\n",
        "\n",
        "                elif 'str' in what_type:\n",
        "                    try:\n",
        "                        df[each_col] = df[each_col].astype('str')\n",
        "                    except Exception as err:\n",
        "                        # display the error\n",
        "                        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "                    else:\n",
        "                        print(colored('\\nCONVERTED!:','green',attrs=['bold']))\n",
        "\n",
        "                elif 'float' in what_type:\n",
        "                    try:\n",
        "                        if df[each_col].dtype == object:\n",
        "                            df[each_col] = df[each_col].str.replace('|','')\n",
        "                            df[each_col] = df[each_col].map(lambda x:re.sub(special_chars,r'',x)).astype('float64')\n",
        "                        else:\n",
        "                            df[each_col] = df[each_col].astype('float64')\n",
        "                    except Exception as err:\n",
        "                        # display the error\n",
        "                        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "                    else:\n",
        "                        print(colored('\\nCONVERTED!:','green',attrs=['bold']))\n",
        "\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            print(\"_\"*75)\n",
        "            display(Markdown('__Updated data type of columns:__'))\n",
        "\n",
        "        display(df.dtypes)\n",
        "        \n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    track_cell(value, flag)\n",
        "    \n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    clear_output()\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T6DRKmaYywE"
      },
      "source": [
        "## Checking and removing duplicates\n",
        "\n",
        "Presence of duplicate observations can be misleading, this section helps get rid of such rows in the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:30:56.842465Z",
          "start_time": "2020-12-14T14:30:56.481003Z"
        },
        "scrolled": true,
        "id": "7z0242RCYywE"
      },
      "source": [
        "# check for duplicate rows\n",
        "value=\"Check Duplicates\"\n",
        "\n",
        "try:\n",
        "    # length of original data\n",
        "    len_df_before = len(df)\n",
        "    display(Markdown('__The length of the original dataframe__ : {}'.format(len_df_before)))\n",
        "\n",
        "    # check for duplicate rows using parameter 'keep' having 3 possible values\n",
        "    # \n",
        "    # *first (Default) : considers (counts) duplicates except for the first occurrence.\n",
        "    # *last : considers (counts) duplicates except for the last occurrence.\n",
        "    # *False : considers (counts) all duplicates.\n",
        "\n",
        "    # collect duplicate using 'last'\n",
        "    df_duplicate_avoid_one_val = df[df.duplicated(keep='last')]\n",
        "\n",
        "    # collect all the duplicates\n",
        "    df_duplicate_all = df[df.duplicated(keep=False)]\n",
        "\n",
        "    # get unique count of duplicates\n",
        "    len_df_duplicate_avoid_one_val = len(df_duplicate_avoid_one_val)\n",
        "    len_df_duplicate_all = len(df_duplicate_all)\n",
        "    count_unique_val_with_duplicates = len_df_duplicate_all - len_df_duplicate_avoid_one_val\n",
        "    display(Markdown('\\n__The number of unique duplicates__ : {}'.format(count_unique_val_with_duplicates)))\n",
        "\n",
        "    if count_unique_val_with_duplicates == 0:\n",
        "        print(colored(\"NO DUPLICATES FOUND!\",'green',attrs=['bold']))\n",
        "    else:\n",
        "        print(colored(\"DUPLICATES ARE SPOTTED!\",'red',attrs=['bold']))\n",
        "        \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBrQhpZJYywE"
      },
      "source": [
        "### Deleting duplicate rows\n",
        "\n",
        "If there are duplicate values, then it is __recommended to delete__ those rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:30:58.383116Z",
          "start_time": "2020-12-14T14:30:58.184001Z"
        },
        "id": "4YAumarOYywF"
      },
      "source": [
        "# drop rows having missing values across all the columns\n",
        "value=\"Treat duplicate data\"\n",
        "try:\n",
        "    if count_unique_val_with_duplicates == 0:\n",
        "        print(colored('NO DUPLICATE VALUES to remove!','green',attrs=['bold']))\n",
        "    else:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Processing</h2></div>'))\n",
        "\n",
        "        print('Number of rows of the original dataframe: {}'.format(df.shape[0]))\n",
        "\n",
        "        # dropping the rows\n",
        "        df.drop_duplicates(keep='last',inplace=True)\n",
        "        clear_output()\n",
        "\n",
        "        print(colored('DUPLICATE VALUES removed.','red',attrs=['bold']))\n",
        "        print('\\nAfter removing duplicate values, the number of rows in the dataframe is: {}'.format(df.shape[0]))\n",
        "        display(Markdown('The __data type summary__ is shown below:'.format(file_name)))\n",
        "        df.info()\n",
        "        \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11NkkTQiYywF"
      },
      "source": [
        "## List of numerical & categorical column names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:31:00.146028Z",
          "start_time": "2020-12-14T14:31:00.138050Z"
        },
        "id": "GiBXj1suYywF"
      },
      "source": [
        "# the list of columns in dataframe\n",
        "df_cols = df.columns\n",
        "\n",
        "# obtain the list of numerical columns\n",
        "num_cols = df._get_numeric_data().columns.tolist()\n",
        "num_cols_vis = ' || '.join(num_cols)\n",
        "print(colored(\"Numerical Columns:\\nCount:\",'magenta',attrs=['bold']),\"{}\\n{}\".format(len(num_cols), \n",
        "                                                                                     num_cols_vis))\n",
        "\n",
        "# obtain the list of categorical columns\n",
        "cat_cols = list(set(df_cols) - set(num_cols))\n",
        "cat_cols_vis = ' || '.join(cat_cols)\n",
        "print(colored(\"\\nCategorical Columns:\\nCount:\",'blue',attrs=['bold']),\"{}\\n{}\".format(len(cat_cols), \n",
        "                                                                                      cat_cols_vis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inhyb_HhYywG"
      },
      "source": [
        "## Filtering dataset for analysis\n",
        "\n",
        "Here, the user can filter the data by rows by executing a query. Check the code for better understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FntMHGX7c9HO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:31:12.075094Z",
          "start_time": "2020-12-14T14:31:01.815439Z"
        },
        "scrolled": false,
        "id": "p6ludMaFYywG"
      },
      "source": [
        "# filtering datset\n",
        "value=\"Filtering data\"\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        display(Markdown('__NOTE__ : If you execute this code, this will filter out from the original data and __this filtered data will be used throughout the rest of the notebook__.'))\n",
        "\n",
        "        original_df_shape = df.shape\n",
        "\n",
        "        while True:\n",
        "            operation_input = input(\"Do you want to filter the data? (Y/N): \")\n",
        "\n",
        "            # if user enters 'n', then it will break the infinite loop and skip this operation\n",
        "            if operation_input == 'N' or operation_input =='n' or operation_input =='':\n",
        "                break\n",
        "            elif operation_input == 'Y' or operation_input =='y':\n",
        "                # add conditions to filter data below\n",
        "\n",
        "                display(Markdown('Please provide a query based on your dara to filter. Take a look at this __sample query__ :\\\n",
        "                *location_type == \"HIGHWAY\" and rooms1mile>=50*'))\n",
        "\n",
        "                filter_query = input('Enter your query: ')\n",
        "                if filter_query == '':\n",
        "                    display(Markdown('No query entered. _Breaking from the filter operation!_'))\n",
        "                    break\n",
        "                else:\n",
        "\n",
        "                    # sample query based filtering\n",
        "                    df_filtered = df.query(filter_query)\n",
        "                    if df_filtered.shape[0] == 0:\n",
        "                        print(colored('\\nIncorrect query! Please try again.\\n','red',attrs=['bold']))\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(colored('\\nFiltering operation successful!\\n','green',attrs=['bold']))\n",
        "                        df = df_filtered.copy()\n",
        "                        break\n",
        "            else:\n",
        "                print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
        "                continue\n",
        "\n",
        "        modified_df_shape = df.shape\n",
        "\n",
        "        if modified_df_shape[0] == original_df_shape[0]:\n",
        "            clear_output()\n",
        "            display(Markdown('__No filtering operation performed!__'))\n",
        "        else:\n",
        "            clear_output()\n",
        "            display(Markdown('__Filtered data__: {} rows and {} columns.'.format(modified_df_shape[0], modified_df_shape[1])))\n",
        "            display(df.info())\n",
        "            \n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    clear_output()\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fhTAORZYywG"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGZXB04oYywG"
      },
      "source": [
        "***\n",
        "\n",
        "# Missing Value Analysis <a name='missing_val'></a>\n",
        "\n",
        "Missing value in the training data can lead to a biased model because we have not analyzed the behavior and relationship of those values with other variables correctly. It can lead to a wrong prediction or classification. Missing values can be of 3 types:\n",
        "1. **Missing Completely At Random (MCAR)**: When missing data are MCAR, the presence/absence of data is completely independent of observable variables and parameters of interest. This is a case when the __probability of missing variables is the same__ for all observations. _For example, respondents of the data collection process decide that they will declare they're earning after tossing a fair coin. If a head occurs, the respondent declares his / her earnings & vice versa._\n",
        "2. **Missing At Random (MAR)**: When missing data is not random but can be related to an observed variable where there is complete information. This kind of missing data can induce a bias in the analysis, especially if it unbalances the data because of many missing values in a certain category. _For example, we are collecting data for age and female has higher missing value compare to male._\n",
        "3. **Missing Depending on Unobserved Predictors**: This is a case when the missing values are not random and are related to the unobserved input variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8klrq6ZYywH"
      },
      "source": [
        "The following code is to visualize the missing values (if any) using a stacked-bar plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:31:16.055766Z",
          "start_time": "2020-12-14T14:31:15.386341Z"
        },
        "id": "EKTxaretYywH"
      },
      "source": [
        "try:\n",
        "    # calculate the sum\n",
        "    total_msng_val = df.isnull().sum().sort_values(ascending=False)\n",
        "    \n",
        "    if sum(total_msng_val.tolist()) == 0:\n",
        "        print(colored('NO MISSING VALUES to visualize!','green',attrs=['bold']))\n",
        "    else:\n",
        "        if __name__ == '__main__':\n",
        "            \n",
        "            # calculate the percentage\n",
        "            percent_msng_val = ((df.isnull().sum()/df.isnull().count())*100).sort_values(ascending=False)\n",
        "\n",
        "            # generate a table for displaying the information\n",
        "            missing_data = pd.concat([total_msng_val, percent_msng_val], axis=1, keys=['Total', 'Percentage']).reset_index()\n",
        "\n",
        "            missing_data.rename(columns = {'index': 'columns'}, inplace=True)\n",
        "\n",
        "            missing_data_filtered = missing_data[missing_data['Percentage'] > 0]\n",
        "\n",
        "            if missing_data_filtered.shape[0] == 1:\n",
        "                display(Markdown(\"Only __{}__ column has __{}%__ of missing values!\".format(missing_data_filtered.iloc[0]['columns'], \n",
        "                                                                                             round(missing_data_filtered.iloc[0]['Percentage'],4))))\n",
        "                display(Markdown('<span style=\"color:red\">Plot generated only if you have at least 2 columns with missing values'))\n",
        "            else :\n",
        "                display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating plot</h2></div>'))\n",
        "\n",
        "                fig = go.Figure()\n",
        "                fig.add_trace(go.Bar(x=missing_data_filtered['Percentage'], \n",
        "                                     y=missing_data_filtered['columns'],\n",
        "                                     orientation='h'))\n",
        "                fig.update_traces(marker_color='maroon')\n",
        "                fig.update_xaxes(title = \"Percentage missing (%)\",range=[0, 100])\n",
        "                fig.update_yaxes(title = \"Columns\")\n",
        "\n",
        "                if missing_data_filtered.shape[1] < 5:\n",
        "                    fig_ht = 300\n",
        "                else:\n",
        "                    fig_ht = 5*missing_data_filtered.shape[1]+300\n",
        "                fig.update_layout(title =\"Percentage of Missing Value for every column\",\n",
        "                                  height = fig_ht, width = 900)\n",
        "\n",
        "                clear_output()\n",
        "                fig.show(config={'displaylogo': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6GNYWE5YywH"
      },
      "source": [
        "## Missing value treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYuUoA07YywH"
      },
      "source": [
        "### Drop column(s) with missing values\n",
        "\n",
        "The cell below accepts user input and drops the specified columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:31:27.690149Z",
          "start_time": "2020-12-14T14:31:17.850344Z"
        },
        "scrolled": false,
        "id": "ZsG5L-o-YywI"
      },
      "source": [
        "# eliminate the columns\n",
        "value=\"Deleting Cols missing\"\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        # columns with missing values\n",
        "        total_msng_val = df.isnull().sum()\n",
        "    \n",
        "        if total_msng_val.sum() == 0:\n",
        "            print(colored('No Columns with Missing Values to drop!','green',attrs=['bold']))\n",
        "        \n",
        "        else:\n",
        "            display(Markdown('__PLEASE READ THE INSTRUCTION TO DELETE COLUMN DUE TO MISSING VALUE BEFORE PROCEEDING!__'))\n",
        "            display(Markdown('Enter the column name(s) from the list shown _(comma `,` separated for multiple)_ and press `Enter`.'))\n",
        "            display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
        "            print('_'*75)\n",
        "        \n",
        "            missing_cols = missing_data[missing_data['Percentage']>0]['columns'].to_list()\n",
        "            display(Markdown(\"Columns with Missing Values: {}\".format(missing_cols)))\n",
        "        \n",
        "            col_to_drop = columns_to_delete(df_cols)\n",
        "            if col_to_drop == []:\n",
        "                clear_output()\n",
        "                display(Markdown('__No columns were dropped due to missing value!__'))\n",
        "            else:\n",
        "                clear_output()\n",
        "                df.drop(col_to_drop, axis=1, inplace=True)\n",
        "\n",
        "                # updated list of columns in dataframe\n",
        "                cols = df.columns\n",
        "\n",
        "                # updated list of numerical columns\n",
        "                num_cols = df._get_numeric_data().columns.tolist()\n",
        "\n",
        "                # updated list of categorical columns\n",
        "                cat_cols = list(set(cols) - set(num_cols))\n",
        "\n",
        "                display(Markdown('__Column(s) dropped__ : {}'.format(col_to_drop)))\n",
        "\n",
        "                # displaying the updated dataframe\n",
        "                print('_'*75)\n",
        "                display(Markdown('__Updated data type summary__ :'))\n",
        "                display(df.info())\n",
        "    else:\n",
        "        df.drop('building_type', axis=1, inplace=True)\n",
        "        display(Markdown('__Column dropped__ : building_type'))\n",
        "    \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    clear_output()\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VQ1BuV8YywI"
      },
      "source": [
        "### Drop row(s) with missing values\n",
        "\n",
        "\n",
        "If there are missing values, then it is __recommended to delete__ those rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:31:33.209671Z",
          "start_time": "2020-12-14T14:31:32.991836Z"
        },
        "scrolled": true,
        "id": "d5qSVakUYywI"
      },
      "source": [
        "# drop rows having missing values across all the columns\n",
        "value=\"Treat missing data\"\n",
        "\n",
        "try:\n",
        "    # calculate the sum\n",
        "    total_msng_val = df.isnull().sum()\n",
        "    \n",
        "    if sum(total_msng_val.tolist()) == 0:\n",
        "        print(colored('NO MISSING VALUES to remove!','green',attrs=['bold']))\n",
        "    else:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Processing</h2></div>'))\n",
        "\n",
        "        print('Number of rows of the original dataframe: {}'.format(df.shape[0]))\n",
        "\n",
        "        # dropping the rows\n",
        "        df.dropna(axis=0, inplace=True)\n",
        "        clear_output()\n",
        "\n",
        "        print(colored('MISSING VALUES removed!','green',attrs=['bold']))\n",
        "        print('\\nAfter removing missing values, the number of rows in the dataframe is: {}'.format(df.shape[0]))\n",
        "    \n",
        "    display(Markdown('The __data summary__ is shown below:'))\n",
        "    display(df.info())\n",
        "    \n",
        "    track_cell(value, flag)        \n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjXnLA1SYywJ"
      },
      "source": [
        "***\n",
        "\n",
        "# Final Dataset Summary\n",
        "\n",
        "All further operations will be performed on the following dataset.\n",
        "\n",
        "We will be ignoring the column name(s) entered as the __unique row identifier__ (and __date-time column__) for the rest of the analysis to flow properly.\n",
        "\n",
        "__NOTE__: The column is not deleted, just ignored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:31:36.504395Z",
          "start_time": "2020-12-14T14:31:35.209539Z"
        },
        "scrolled": false,
        "id": "XyYtAFwQYywJ"
      },
      "source": [
        "# displays the final data along with the datatype summary\n",
        "display(Markdown('Final dataframe contains __{} rows__ and __{} columns__.'.format(df.shape[0], df.shape[1])))\n",
        "\n",
        "if row_id_input == '' or row_id_input == None:\n",
        "    pass\n",
        "else:\n",
        "    display(Markdown('The unique row identifier is __{}__.'.format(row_id_input)))\n",
        "\n",
        "if df.shape[0] > 2500:\n",
        "    display_data(df.head(2500).round(4))\n",
        "else:\n",
        "    display_data(df.round(4))\n",
        "\n",
        "display(Markdown('The __data summary__ is shown below:'.format(file_name)))\n",
        "print('_'*75)\n",
        "df.info()\n",
        "\n",
        "# the list of columns in dataframe\n",
        "\n",
        "df_cols = df.columns\n",
        "\n",
        "# obtain the list of numerical columns\n",
        "num_cols = df._get_numeric_data().columns.tolist()\n",
        "\n",
        "# columns to ignore\n",
        "cols_ignore = [row_id_input]\n",
        "if date_time_col == '':\n",
        "    pass\n",
        "else:\n",
        "    cols_ignore.extend(date_time_col)\n",
        "\n",
        "for col_name in cols_ignore:\n",
        "    if col_name == '':\n",
        "        pass\n",
        "    else:\n",
        "        if col_name in num_cols:\n",
        "            num_cols.remove(col_name)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "num_cols_vis = ' || '.join(num_cols)\n",
        "print(colored(\"\\nUpdated Numerical Columns:\\nCount:\",'magenta',attrs=['bold']),\"{}\\n{}\".format(len(num_cols), \n",
        "                                                                                     num_cols_vis))\n",
        "\n",
        "# obtain the list of categorical columns\n",
        "cat_cols = list(set(df_cols) - set(num_cols))\n",
        "\n",
        "for col_name in cols_ignore:\n",
        "    if col_name == '':\n",
        "        pass\n",
        "    else:\n",
        "        if col_name in cat_cols:\n",
        "            cat_cols.remove(col_name)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "cat_cols_vis = ' || '.join(cat_cols)\n",
        "print(colored(\"\\nUpdated Categorical Columns:\\nCount:\",'blue',attrs=['bold']),\"{}\\n{}\".format(len(cat_cols), \n",
        "                                                                                      cat_cols_vis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWDfHaVLYywJ"
      },
      "source": [
        "***\n",
        "\n",
        "# Outlier Detection\n",
        "\n",
        "Outliers can impact the results of our analysis and statistical modeling drastically. But outliers aren’t always a bad thing. It’s very important to understand this. Simply removing outliers from your data without considering how they’ll impact the results might become a recipe for disaster.\n",
        "\n",
        "There are 3 types of anomalies:\n",
        "* __Global__: Global anomalies are the most common type of anomalies and correspond to those data points which deviate largely from the rest of the data points. A key challenge in detecting global anomalies is to figure out the exact amount of deviation which leads to a potential anomaly.\n",
        "* __Contextual__: Contextual anomalies are the anomaly getting generated depending on the contextual information which is governed by very domain-specific and behavioral attributes.\n",
        "* __Collective__: Data points collectively forming a region which substantially deviates from the rest of the data points creates collective anomalies, but may not be anomalies when those data are considered individually.\n",
        "\n",
        "Most common causes of outliers on a data set:\n",
        "* Data entry errors (human errors)\n",
        "* Measurement errors (instrument errors)\n",
        "* Experimental errors (data extraction or experiment planning/executing errors)\n",
        "* Intentional (dummy outliers made to test detection methods)\n",
        "* Data processing errors (data manipulation or data set unintended mutations)\n",
        "* Sampling errors (extracting or mixing data from wrong or various sources)\n",
        "* Natural (not an error, novelties in data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khgkcFfzYywK"
      },
      "source": [
        "## Univariate Outlier Detection \n",
        "\n",
        "Across all the methods, we will be selecting one column and obtain the results accordingly. Execute the following cell to select a column for univariate outlier detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:32:02.471479Z",
          "start_time": "2020-12-14T14:31:38.848283Z"
        },
        "id": "9bhVxz1QYywK"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "#     while True:\n",
        "#         print(colored(\"Numerical Columns:\",'magenta',attrs=['bold']),\"{}\".format(num_cols_vis))\n",
        "#         enter_col = input('Enter your numerical column: ')\n",
        "#         if enter_col in num_cols:\n",
        "#             break\n",
        "#         else:\n",
        "#             print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "#             continue\n",
        "# else:\n",
        "#     enter_col = 'occupancy'\n",
        "        \n",
        "# if __name__ == '__main__':\n",
        "#     clear_output()\n",
        "# display(Markdown('__`{}`__ column selected for _univariate outlier detection_.'.format(enter_col)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iNiongqYywK"
      },
      "source": [
        "###  Using Z-Score\n",
        "\n",
        "The __Z-score__, or standard score, is a way of describing a data point in terms of its relationship to the __mean and standard deviation__ of a group of points. Taking a Z-score is simply mapping the data onto a distribution whose mean is defined as 0 and whose standard deviation is defined as 1.\n",
        "\n",
        "The goal of taking Z-scores is to remove the effects of the location and scale of the data, allowing different datasets to be compared directly. The intuition behind the Z-score method of outlier detection is that, once we’ve centered and rescaled the data, _anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0KKodjXVqN",
        "outputId": "e129cc00-95c7-48c8-ebc8-3c6c2a2a1bfa"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/88/b25778f17e5320c1c58f8c5060fb5b037288e162bd7554c30799e9ea90db/lxml-4.6.2-cp37-cp37m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22616 sha256=44d394d3b0206e9a5c5554b9f5d002724950af73a9842961b891d7eb28d9723a\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn5TsDl8pVna"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import plotly.graph_objects as go\r\n",
        "import plotly.express as px"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMAQc5XjDlYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d212859b-bc26-47e2-885c-4dde2e95a85c"
      },
      "source": [
        "#Data Source\r\n",
        "import yfinance as yf\r\n",
        "df = yf.download(tickers='ONGC.NS', period='500d', interval='1d')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pDnOcCoGHAt"
      },
      "source": [
        "def z_score(data, rqrd_col, thresh):\r\n",
        "  z_mean = np.mean(data[rqrd_col])\r\n",
        "  z_std = np.std(data[rqrd_col])\r\n",
        "\r\n",
        "  df_z_outlier = data.copy()\r\n",
        "  df_z_outlier['z_score']= (data[rqrd_col] - z_mean)/z_std\r\n",
        "  label = []\r\n",
        "  for i in df_z_outlier['z_score']:\r\n",
        "    if abs(i) > thresh:\r\n",
        "      label.append('Anomaly')\r\n",
        "    else:\r\n",
        "      label.append('Normal')\r\n",
        "  df_z_outlier['outlier_flag'] = np.array(label)\r\n",
        "  \r\n",
        "  return df_z_outlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Qo1TiHp9yf"
      },
      "source": [
        "def plot_zscore(df_z_outlier, enter_col, z_thresh):\r\n",
        "  # scatter plot for column using z-score\r\n",
        "  fig = go.Figure()\r\n",
        "  fig = px.scatter(df_z_outlier, x=\"z_score\", y=enter_col, \r\n",
        "                    opacity=0.5, color=\"outlier_flag\")\r\n",
        "  fig.data[0]['marker'].update(color='green')\r\n",
        "  fig.data[1]['marker'].update(color='red')\r\n",
        "\r\n",
        "  # plotting the threshold entered by the user\r\n",
        "  fig.add_shape(type=\"line\", x0=z_thresh, y0=0,\r\n",
        "                x1=z_thresh, y1= max(df_z_outlier[enter_col]),\r\n",
        "                line=dict(color=\"red\", width=1, dash=\"dashdot\"))\r\n",
        "\r\n",
        "  fig.add_shape(type=\"line\", x0=-z_thresh, y0=0,\r\n",
        "                x1=-z_thresh, y1=max(df_z_outlier[enter_col]),\r\n",
        "                line=dict(color=\"red\",width=1,dash=\"dashdot\"))\r\n",
        "\r\n",
        "  fig.update_layout(title = \"Scatter Plot of Z-Score vs `{}`\".format(enter_col),\r\n",
        "                    title_x = 0.5)\r\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:32:22.906174Z",
          "start_time": "2020-12-14T14:32:07.764193Z"
        },
        "code_folding": [
          5
        ],
        "scrolled": false,
        "id": "fMGrOca2YywL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "467c2ffb-6f4d-43fb-fdce-ac6dca333f4f"
      },
      "source": [
        "enter_col = 'Open'\n",
        "z_thresh = 1.5\n",
        "\n",
        "temp = z_score(df, enter_col, z_thresh)\n",
        "plot_zscore(temp, enter_col, z_thresh)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ece94a72-8453-47fc-9ff9-8a0ea9618a41\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ece94a72-8453-47fc-9ff9-8a0ea9618a41\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ece94a72-8453-47fc-9ff9-8a0ea9618a41',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"outlier_flag=Normal<br>z_score=%{x}<br>Open=%{y}\", \"legendgroup\": \"outlier_flag=Normal\", \"marker\": {\"color\": \"green\", \"opacity\": 0.5, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"outlier_flag=Normal\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.7862297436129307, 0.751041467314302, 0.7627707435994419, 0.891795019952026, 0.9914952107053423, 1.0970604870444376, 1.1087897633295776, 1.0648043060954893, 1.1014588537905627, 0.9739008488344234, 1.1351808586927468, 1.1014588537905627, 1.2876641350588198, 1.2788669541233604, 1.1996932205906434, 1.1835651301161694, 1.2554079541098715, 1.17476794918071, 1.1381134014856362, 1.1938281350048643, 1.2847315922659304, 1.3125889590255444, 1.3331154162461436, 1.224618044557368, 1.196760677797754, 1.4020261448901654, 1.4357481497923494, 1.4313497830462243, 1.4460116021242537, 1.4078907830327354, 1.3888305972085808, 1.292062501804945, 1.314055230421989, 1.3668378685915368, 1.3902968686050257, 1.341912597181603, 1.3727025067341068, 1.4020261448901654, 1.3814996876695662, 1.3287170495000185, 1.456275054456158, 1.4518762402668237, 1.3727025067341068, 1.4899970593583418, 1.4738689688838678, 1.2084899540828937, 1.2275505873502575, 1.2260843159538126, 1.2260843159538126, 1.1381134014856362, 1.167437039641695, 1.2084899540828937, 1.1366471300891916, 0.976833391627313, 0.9621715725492835, 1.0354806679394306, 1.0032244869904823, 0.970968753484743, 0.9739008488344234, 0.9035242962371659, 0.885929934366247, 0.7774325626774713, 0.8008915626909602, 0.7275824673008131, 0.6337469146900668, 0.576565909774394, 0.5941598242021038, 0.5663024574424897, 0.6000244623446737, 0.5794980051240743, 0.5223170002084015, 0.4841961811168833, 0.49299336205234273, 0.4519402238895395, 0.40502244758416633, 0.2935925331025011, 0.15137293278993683, 0.37569880942810757, 0.3786311284993925, 0.4064887189806111, 0.25840425680387236, 0.36103699035007814, 0.27306607588190174, 0.18509516141372537, 0.2422761663293982, 0.4460755857469696, 0.4724669048317434, 0.5281816383509714, 0.44167699527923987, 0.44167699527923987, 0.5794980051240743, 0.5736333669815045, 0.5428434574290009, 0.46366972389628397, 0.387428309434852, 0.7701016531384566, 0.7349133768398278, 0.610287914676578, 0.6044232765340081, 0.7246499245079235, 0.5838968193134086, 0.5956260955985485, 0.49885800019491267, 0.5501743669680156, 0.5384450906828757, 0.4475416334218098, 0.42994749527249543, 0.4431432666756846, 0.5355125478899861, 0.756906105456872, 0.8448770199250484, 0.8771332008739967, 0.8478095627179378, 0.9475097534712541, 0.9695024820882983, 0.9005917534442763, 0.891795019952026, 0.8610051103995225, 0.9621715725492835, 0.8624713817959673, 0.8610051103995225, 0.9665699392954087, 1.0853307633160887, 1.0310823011933055, 0.9900289393088976, 0.9020580248407212, 0.7979590198980706, 0.8184859245618791, 0.7920943817555006, 0.7657032863923314, 0.7114543768263389, 0.6982588291447543, 0.6352131860865116, 0.660138010053236, 0.5809642765205192, 0.6645368242425704, 0.6058895479304528, 0.5633699146496002, 0.6220171909617178, 0.6410778242290815, 0.5824305479169639, 0.4959259048452323, 0.460737404824999, 0.531114181143861, 0.536978819286431, 0.460737404824999, 0.4460755857469696, 0.4372784048115101, 0.4959259048452323, 0.4475416334218098, 0.44900790481825453, 0.4094210380518961, 0.4226165857334807, 0.40795476665545133, 0.43434608574022515, 0.4020901285128814, 0.4020901285128814, 0.43434608574022515, 0.536978819286431, 0.5047226383374827, 0.4695343620388539, 0.5149860906693868, 0.4827301334420431, 0.5809642765205192, 0.5428434574290009, 0.4226165857334807, 0.40502244758416633, 0.4035561761877216, 0.38889435710969217, 0.38302971896712223, 0.418217995265751, 0.4064887189806111, 0.40502244758416633, 0.39182689990258174, 0.43434608574022515, 0.34784144266849354, 0.31705153311598994, 0.17776425187471065, 0.19975698049175475, 0.21002020910205443, 0.18656143281017012, 0.18069657094599562, 0.18656143281017012, 0.13524506603706724, -0.20051072508276876, -0.20784163462178346, -0.1403971773742064, -0.12866767736746196, -0.11400585828943255, -0.05975717244504469, -0.11840444875716229, -0.12280303922489202, -0.12280303922489202, -0.16532222506253544, -0.2840830490832154, -0.3295347777137484, -0.31927132538184416, -0.26942123000518603, -0.2547594109271566, -0.3661893254088219, -0.40430992077873557, -0.4541602398769983, -0.6125077069424322, -0.5040103352536565, -0.5949135687931177, -0.49814569711108647, -0.5230707447994155, -0.6359667069559211, -0.8514954026586322, -1.0098430934456706, -1.3074780654739884, -1.3983812990134499, -1.4687580753323117, -1.4540962562542825, -1.4379681657798082, -1.4115769585558366, -1.4819536230138963, -1.3983812990134499, -1.4658256444002244, -1.4467653467152677, -1.3250722036233027, -1.4702242348679544, -1.3543958417793616, -1.295748565467244, -1.2371012891551263, -1.1725893747004388, -1.1374008746802056, -1.0318358220627148, -0.9497295457371083, -1.0582271411474886, -1.0904830983748324, -0.9790531838931671, -1.050896231608474, -1.216574831934527, -1.3280045226945878, -1.2810867463892146, -1.2913499749995143, -1.2664249273111852, -1.2722895654537552, -1.225371789148382, -1.1315362365376356, -1.0318358220627148, -0.9467972266658233, -0.8749544026721211, -0.9643913648151378, -1.0201063220559703, -1.0318358220627148, -1.0039784553031008, -0.9438649075945383, -1.002512183906656, -0.9951812743676413, -1.0039784553031008, -1.078753598368088, -0.9966475457640861, -0.9511958171335531, -1.0039784553031008, -1.002512183906656, -0.9966475457640861, -0.9878503648286265, -0.9717222743541524, -0.7679230786581857, -0.7840511691326598, -0.737133169105682, -0.7679230786581857, -0.7649907595869007, -0.6213048878778916, -0.5641238829622188, -0.6037107497285772, -0.6711549832545497, -0.8690895408079466, -0.7972467168142444, -0.7532612595801562, -0.8089762168209889, -0.8265703549703032, -0.7532612595801562, -0.7356671214308418, -0.7532612595801562, -0.7312685309631121, -0.8485630835873473, -0.8236380358990182, -0.7987129882106891, -0.8119085358922739, -0.9438649075945383, -0.8749544026721211, -0.8426984454447773, -0.8251040835738584, -0.8045776263532591, -0.8558939931263619, -0.8676234931331064, -0.9145412694384796, -0.9380002694519685, -0.9585267266725678, -0.9731885457505972, -1.0201063220559703, -1.0230386411272554, -0.8852176312824208, -0.8969471312891653, -0.8339012645093179, -0.8412321740483326, -0.8529616740550771, -0.8734881312756764, -0.8852176312824208, -0.9204059075810496, -0.9145412694384796, -0.9746548171470419, -0.9629253171402975, -1.0230386411272554, -1.0171740029846854, -0.9775871362183269, -0.9687899552828675, -0.929203088516509, -0.9277368171200642, -0.9599929980690126, -0.957060455276123, -0.9526620885299979, -0.9878503648286265, -0.9218721789774943, -0.8896162217501505, -0.9438649075945383, -0.828036626366748, -0.8764204503469614, -0.8734881312756764, -0.8793529931398508, -0.8617586312689319, -0.90427804082818, -0.8852176312824208, -0.8339012645093179, -0.8808190408146911, -0.8998794503604501, -0.957060455276123, -0.9673239076080272, -1.0157077315882406, -1.0919493697712772, -1.1051449174528618, -1.1066111888493066, -1.094881688842562, -1.097814007913847, -1.0831521888358178, -1.1183404651344464, -1.0963477365174024, -1.0904830983748324, -1.1784540128430088, -1.216574831934527, -1.323605932226858, -1.2664249273111852, -1.2239057414735417, -1.1344685556089207, -1.225371789148382, -1.2810867463892146, -1.2371012891551263, -1.2077776509990676, -1.2283041082196668, -1.1813863319142937, -1.2371012891551263, -1.1989804700636082, -1.2341689700838414, -1.2517631082331557, -1.295748565467244, -1.3060117940775435, -1.2854853368569443, -1.222439470077097, -1.2664249273111852, -1.2840190654604995, -1.2312366510125565, -1.222439470077097, -1.2634926082399003, -1.2884176559282292, -1.323605932226858, -1.3543958417793616, -1.3426663417726172, -1.3250722036233027, -1.3162750226878432, -1.298680884538529, -1.2752221082466446, -1.247364517765426, -1.2356350177586817, -1.1652584651614242, -1.1344685556089207, -1.1725893747004388, -1.1286039174663507, -1.1593936032972496, -1.1637921937649793, -1.1359348270053653, -1.14913037468695, -1.0098430934456706, -0.9966475457640861, -0.8808190408146911, -0.8910822694249907, -0.9585267266725678, -0.8705558122043914, -0.7561935786514412, -0.6286357974169063, -0.587582659254103, -0.5333339734097152, -0.5773194306438034, -0.5919812497218329, -0.5450634734164597, -0.3793848730904065, -0.2547594109271566, -0.27235354907647097, -0.17851799646572467, -0.2840830490832154, -0.36032446354464737, -0.6491622546375057, -0.6066430687998622, -0.5479957924877447, -0.5040103352536565, -0.49814569711108647, -0.5186721543316858, -0.5245367924742558, -0.5113412447926711, -0.5025440638572116, -0.4307012398635094, -0.36032446354464737, -0.3867157826294212, -0.3588584158698072, -0.2840830490832154, -0.26942123000518603, -0.1829165869334544, -0.12280303922489202, -0.1741194059979949, -0.2870153681545004, -0.3940466921684359, -0.3573921444733624, -0.35592587307691764, -0.48641619710434203, -0.5318677020132704, -0.5802517497150884, -0.6506285260339504, -0.5993121592608475, -0.6506285260339504, -0.5479957924877447, -0.48641619710434203, -0.4966794257146417, -0.3588584158698072, -0.342730325395333, -0.33393314445987354, -0.26942123000518603, -0.3280685063173036, -0.36032446354464737, -0.41603942078548, -0.3500612349343477, -0.26942123000518603, -0.2195711346285279, -0.014305667536116303, -0.15505899645223578, -0.04216303429573033, 0.08246242786751956], \"xaxis\": \"x\", \"y\": [138.0, 136.8000030517578, 137.1999969482422, 141.60000610351562, 145.0, 148.60000610351562, 149.0, 147.5, 148.75, 144.39999389648438, 149.89999389648438, 148.75, 155.10000610351562, 154.8000030517578, 152.10000610351562, 151.5500030517578, 154.0, 151.25, 150.0, 151.89999389648438, 155.0, 155.9499969482422, 156.64999389648438, 152.9499969482422, 152.0, 159.0, 160.14999389648438, 160.0, 160.5, 159.1999969482422, 158.5500030517578, 155.25, 156.0, 157.8000030517578, 158.60000610351562, 156.9499969482422, 158.0, 159.0, 158.3000030517578, 156.5, 160.85000610351562, 160.6999969482422, 158.0, 162.0, 161.4499969482422, 152.39999389648438, 153.0500030517578, 153.0, 153.0, 150.0, 151.0, 152.39999389648438, 149.9499969482422, 144.5, 144.0, 146.5, 145.39999389648438, 144.3000030517578, 144.39999389648438, 142.0, 141.39999389648438, 137.6999969482422, 138.5, 136.0, 132.8000030517578, 130.85000610351562, 131.4499969482422, 130.5, 131.64999389648438, 130.9499969482422, 129.0, 127.69999694824219, 128.0, 126.5999984741211, 125.0, 121.19999694824219, 116.3499984741211, 124.0, 124.0999984741211, 125.05000305175781, 120.0, 123.5, 120.5, 117.5, 119.44999694824219, 126.4000015258789, 127.30000305175781, 129.1999969482422, 126.25, 126.25, 130.9499969482422, 130.75, 129.6999969482422, 127.0, 124.4000015258789, 137.4499969482422, 136.25, 132.0, 131.8000030517578, 135.89999389648438, 131.10000610351562, 131.5, 128.1999969482422, 129.9499969482422, 129.5500030517578, 126.44999694824219, 125.8499984741211, 126.30000305175781, 129.4499969482422, 137.0, 140.0, 141.10000610351562, 140.10000610351562, 143.5, 144.25, 141.89999389648438, 141.60000610351562, 140.5500030517578, 144.0, 140.60000610351562, 140.5500030517578, 144.14999389648438, 148.1999969482422, 146.35000610351562, 144.9499969482422, 141.9499969482422, 138.39999389648438, 139.10000610351562, 138.1999969482422, 137.3000030517578, 135.4499969482422, 135.0, 132.85000610351562, 133.6999969482422, 131.0, 133.85000610351562, 131.85000610351562, 130.39999389648438, 132.39999389648438, 133.0500030517578, 131.0500030517578, 128.10000610351562, 126.9000015258789, 129.3000030517578, 129.5, 126.9000015258789, 126.4000015258789, 126.0999984741211, 128.10000610351562, 126.44999694824219, 126.5, 125.1500015258789, 125.5999984741211, 125.0999984741211, 126.0, 124.9000015258789, 124.9000015258789, 126.0, 129.5, 128.39999389648438, 127.19999694824219, 128.75, 127.6500015258789, 131.0, 129.6999969482422, 125.5999984741211, 125.0, 124.94999694824219, 124.44999694824219, 124.25, 125.44999694824219, 125.05000305175781, 125.0, 124.55000305175781, 126.0, 123.05000305175781, 122.0, 117.25, 118.0, 118.3499984741211, 117.55000305175781, 117.3499984741211, 117.55000305175781, 115.80000305175781, 104.3499984741211, 104.0999984741211, 106.4000015258789, 106.80000305175781, 107.30000305175781, 109.1500015258789, 107.1500015258789, 107.0, 107.0, 105.55000305175781, 101.5, 99.94999694824219, 100.30000305175781, 102.0, 102.5, 98.69999694824219, 97.4000015258789, 95.69999694824219, 90.30000305175781, 94.0, 90.9000015258789, 94.19999694824219, 93.3499984741211, 89.5, 82.1500015258789, 76.75, 66.5999984741211, 63.5, 61.099998474121094, 61.599998474121094, 62.150001525878906, 63.04999923706055, 60.650001525878906, 63.5, 61.20000076293945, 61.849998474121094, 66.0, 61.04999923706055, 65.0, 67.0, 69.0, 71.19999694824219, 72.4000015258789, 76.0, 78.80000305175781, 75.0999984741211, 74.0, 77.80000305175781, 75.3499984741211, 69.69999694824219, 65.9000015258789, 67.5, 67.1500015258789, 68.0, 67.80000305175781, 69.4000015258789, 72.5999984741211, 76.0, 78.9000015258789, 81.3499984741211, 78.30000305175781, 76.4000015258789, 76.0, 76.94999694824219, 79.0, 77.0, 77.25, 76.94999694824219, 74.4000015258789, 77.19999694824219, 78.75, 76.94999694824219, 77.0, 77.19999694824219, 77.5, 78.05000305175781, 85.0, 84.44999694824219, 86.05000305175781, 85.0, 85.0999984741211, 90.0, 91.94999694824219, 90.5999984741211, 88.30000305175781, 81.55000305175781, 84.0, 85.5, 83.5999984741211, 83.0, 85.5, 86.0999984741211, 85.5, 86.25, 82.25, 83.0999984741211, 83.94999694824219, 83.5, 79.0, 81.3499984741211, 82.44999694824219, 83.05000305175781, 83.75, 82.0, 81.5999984741211, 80.0, 79.19999694824219, 78.5, 78.0, 76.4000015258789, 76.30000305175781, 81.0, 80.5999984741211, 82.75, 82.5, 82.0999984741211, 81.4000015258789, 81.0, 79.80000305175781, 80.0, 77.94999694824219, 78.3499984741211, 76.30000305175781, 76.5, 77.8499984741211, 78.1500015258789, 79.5, 79.55000305175781, 78.44999694824219, 78.55000305175781, 78.69999694824219, 77.5, 79.75, 80.8499984741211, 79.0, 82.94999694824219, 81.30000305175781, 81.4000015258789, 81.19999694824219, 81.80000305175781, 80.3499984741211, 81.0, 82.75, 81.1500015258789, 80.5, 78.55000305175781, 78.19999694824219, 76.55000305175781, 73.94999694824219, 73.5, 73.44999694824219, 73.8499984741211, 73.75, 74.25, 73.05000305175781, 73.80000305175781, 74.0, 71.0, 69.69999694824219, 66.05000305175781, 68.0, 69.44999694824219, 72.5, 69.4000015258789, 67.5, 69.0, 70.0, 69.30000305175781, 70.9000015258789, 69.0, 70.30000305175781, 69.0999984741211, 68.5, 67.0, 66.6500015258789, 67.3499984741211, 69.5, 68.0, 67.4000015258789, 69.19999694824219, 69.5, 68.0999984741211, 67.25, 66.05000305175781, 65.0, 65.4000015258789, 66.0, 66.30000305175781, 66.9000015258789, 67.69999694824219, 68.6500015258789, 69.05000305175781, 71.44999694824219, 72.5, 71.19999694824219, 72.69999694824219, 71.6500015258789, 71.5, 72.44999694824219, 72.0, 76.75, 77.19999694824219, 81.1500015258789, 80.80000305175781, 78.5, 81.5, 85.4000015258789, 89.75, 91.1500015258789, 93.0, 91.5, 91.0, 92.5999984741211, 98.25, 102.5, 101.9000015258789, 105.0999984741211, 101.5, 98.9000015258789, 89.05000305175781, 90.5, 92.5, 94.0, 94.19999694824219, 93.5, 93.30000305175781, 93.75, 94.05000305175781, 96.5, 98.9000015258789, 98.0, 98.94999694824219, 101.5, 102.0, 104.94999694824219, 107.0, 105.25, 101.4000015258789, 97.75, 99.0, 99.05000305175781, 94.5999984741211, 93.05000305175781, 91.4000015258789, 89.0, 90.75, 89.0, 92.5, 94.5999984741211, 94.25, 98.94999694824219, 99.5, 99.80000305175781, 102.0, 100.0, 98.9000015258789, 97.0, 99.25, 102.0, 103.69999694824219, 110.69999694824219, 105.9000015258789, 109.75, 114.0], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"outlier_flag=Anomaly<br>z_score=%{x}<br>Open=%{y}\", \"legendgroup\": \"outlier_flag=Anomaly\", \"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"outlier_flag=Anomaly\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1.5984944310471176, 1.676202340626599, 1.709924345528783, 1.6659388882946946, 1.704059707386213, 1.6908641597046283, 1.7245861646068124, 1.736315440891952, 1.765639079048011, 1.6483445264237757, 1.6776681645798346, 1.6424798882812057, 1.5662386975413785, 1.5310499737995407, 1.6219534310606065, 1.7245861646068124, 1.8741368981799957, 1.8668059886409811, 1.9738370889333121, 1.8418807172310474, 1.8418807172310474, 1.816955445821114, 1.838948174438158, 1.7832334409189299, 1.6659388882946946, 1.8125570790749888, 1.809624536282099, 1.7392479836848418, 1.7128568883216724, 1.6688714310875843, 1.573569607080393, 1.6674051596911394, 1.6893978883081835, 1.6937962550543086, 1.709924345528783, 1.5486443356704596, 1.630750611996066, 1.6160887929180365, 1.7759025313799153, 1.7583081695089962, 1.5926297929045476, 1.6102241547754665, 1.65274334061311, 1.7333833455422718, 1.6718035264372646, 1.5193206975144007, 1.601426973840007, 1.619020888267717, 1.6395477929315254, -1.51860817070897], \"xaxis\": \"x\", \"y\": [165.6999969482422, 168.35000610351562, 169.5, 168.0, 169.3000030517578, 168.85000610351562, 170.0, 170.39999389648438, 171.39999389648438, 167.39999389648438, 168.39999389648438, 167.1999969482422, 164.60000610351562, 163.39999389648438, 166.5, 170.0, 175.10000610351562, 174.85000610351562, 178.5, 174.0, 174.0, 173.14999389648438, 173.89999389648438, 172.0, 168.0, 173.0, 172.89999389648438, 170.5, 169.60000610351562, 168.10000610351562, 164.85000610351562, 168.0500030517578, 168.8000030517578, 168.9499969482422, 169.5, 164.0, 166.8000030517578, 166.3000030517578, 171.75, 171.14999389648438, 165.5, 166.10000610351562, 167.5500030517578, 170.3000030517578, 168.1999969482422, 163.0, 165.8000030517578, 166.39999389648438, 167.10000610351562, 59.400001525878906], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"shapes\": [{\"line\": {\"color\": \"red\", \"dash\": \"dashdot\", \"width\": 1}, \"type\": \"line\", \"x0\": 1.5, \"x1\": 1.5, \"y0\": 0, \"y1\": 178.5}, {\"line\": {\"color\": \"red\", \"dash\": \"dashdot\", \"width\": 1}, \"type\": \"line\", \"x0\": -1.5, \"x1\": -1.5, \"y0\": 0, \"y1\": 178.5}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Scatter Plot of Z-Score vs `Open`\", \"x\": 0.5}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"z_score\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Open\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ece94a72-8453-47fc-9ff9-8a0ea9618a41');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwh9BEKkYywL"
      },
      "source": [
        "### Using IQR \n",
        "\n",
        "Boxplot holds information about IQR of a column and using that, we can obtain outliers. The components of a __box plot__ are discussed below:\n",
        "\n",
        "![boxplot](https://github.com/ml-nest/EDA/blob/master/Source/boxplot.png?raw=1)\n",
        "\n",
        "* __Box__: It illustrates the interquartile spread of the distribution; its length is determined by the  $25^{th}$ ($Q_1$) and $75^{th}$ ($Q_3$) percentiles, known as the _interquartile range_ (IQR = $Q_3$ − $Q_1$). The vertical yellow line inside the box marks the median of the distribution.\n",
        "* __Whiskers__: They are the lines extending from the box which represent the entire scatter of data points, specifically the points that fall within the interval [$Q_1$ − ($1.5⋅IQR$),  $Q_3$ + ($1.5⋅IQR$)].\n",
        "* __Outliers__: They fall outside of the range bounded by the whiskers which are plotted individually as black points along the central axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq4LhSJTuxrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "a1803c93-15ce-47b5-8b3d-c58c1ae5c0a0"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-02-18</th>\n",
              "      <td>136.800003</td>\n",
              "      <td>138.899994</td>\n",
              "      <td>135.800003</td>\n",
              "      <td>137.100006</td>\n",
              "      <td>119.514633</td>\n",
              "      <td>10614715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-19</th>\n",
              "      <td>137.199997</td>\n",
              "      <td>140.350006</td>\n",
              "      <td>137.199997</td>\n",
              "      <td>139.149994</td>\n",
              "      <td>121.301697</td>\n",
              "      <td>9077045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-20</th>\n",
              "      <td>141.600006</td>\n",
              "      <td>145.350006</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>144.050003</td>\n",
              "      <td>125.573189</td>\n",
              "      <td>14236359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-21</th>\n",
              "      <td>145.000000</td>\n",
              "      <td>148.600006</td>\n",
              "      <td>144.199997</td>\n",
              "      <td>146.949997</td>\n",
              "      <td>128.101196</td>\n",
              "      <td>13876243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-22</th>\n",
              "      <td>148.600006</td>\n",
              "      <td>150.500000</td>\n",
              "      <td>147.100006</td>\n",
              "      <td>148.600006</td>\n",
              "      <td>129.539551</td>\n",
              "      <td>18273813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-19</th>\n",
              "      <td>110.699997</td>\n",
              "      <td>112.199997</td>\n",
              "      <td>103.849998</td>\n",
              "      <td>105.099998</td>\n",
              "      <td>105.099998</td>\n",
              "      <td>48225988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-22</th>\n",
              "      <td>105.900002</td>\n",
              "      <td>108.550003</td>\n",
              "      <td>105.300003</td>\n",
              "      <td>106.300003</td>\n",
              "      <td>106.300003</td>\n",
              "      <td>36592043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-23</th>\n",
              "      <td>109.750000</td>\n",
              "      <td>114.400002</td>\n",
              "      <td>109.449997</td>\n",
              "      <td>112.199997</td>\n",
              "      <td>112.199997</td>\n",
              "      <td>90860972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-24</th>\n",
              "      <td>114.000000</td>\n",
              "      <td>115.349998</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>113.599998</td>\n",
              "      <td>113.599998</td>\n",
              "      <td>26071364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-25</th>\n",
              "      <td>116.000000</td>\n",
              "      <td>120.500000</td>\n",
              "      <td>115.349998</td>\n",
              "      <td>119.050003</td>\n",
              "      <td>119.050003</td>\n",
              "      <td>62012356</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>498 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open        High  ...   Adj Close    Volume\n",
              "Date                                ...                      \n",
              "2019-02-18  136.800003  138.899994  ...  119.514633  10614715\n",
              "2019-02-19  137.199997  140.350006  ...  121.301697   9077045\n",
              "2019-02-20  141.600006  145.350006  ...  125.573189  14236359\n",
              "2019-02-21  145.000000  148.600006  ...  128.101196  13876243\n",
              "2019-02-22  148.600006  150.500000  ...  129.539551  18273813\n",
              "...                ...         ...  ...         ...       ...\n",
              "2021-02-19  110.699997  112.199997  ...  105.099998  48225988\n",
              "2021-02-22  105.900002  108.550003  ...  106.300003  36592043\n",
              "2021-02-23  109.750000  114.400002  ...  112.199997  90860972\n",
              "2021-02-24  114.000000  115.349998  ...  113.599998  26071364\n",
              "2021-02-25  116.000000  120.500000  ...  119.050003  62012356\n",
              "\n",
              "[498 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukMZtWO73jbo"
      },
      "source": [
        "q1, med, q3 = np.percentile(df['Open'], [25, 50, 75])\r\n",
        "iqr = q3 - q1\r\n",
        "min, max = q1-(1.5*iqr), q3+(1.5*iqr)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coUnZNoi5PvI",
        "outputId": "a2262e73-e93b-4811-cdb3-f1165c96d8b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "q1, med, q3, min, max"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79.04999923706055,\n",
              " 104.64999771118164,\n",
              " 138.4749984741211,\n",
              " -10.087499618530273,\n",
              " 227.6124973297119)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XE5DTGzMt1M",
        "outputId": "4768bdc6-3c42-4a89-9510-5300c6297556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['Open'].max()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "178.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xh-igd25iAw",
        "outputId": "c93f4d35-7f05-4bf9-f99b-ee12ac86fef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "iqr"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.42499923706055"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vmcKxiu5n-"
      },
      "source": [
        "from matplotlib.cbook import boxplot_stats\r\n",
        "\r\n",
        "enter_col = \"Open\"\r\n",
        "\r\n",
        "# obtaining the list of outliers\r\n",
        "iqr_outliers = [y for stat in boxplot_stats(df[enter_col]) for y in stat['fliers']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzS10shBu7IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508f10cf-020f-41b1-93f9-e145f50f1646"
      },
      "source": [
        "boxplot_stats(df[enter_col])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'cihi': 108.83074739752657,\n",
              "  'cilo': 100.46924802483672,\n",
              "  'fliers': array([], dtype=float64),\n",
              "  'iqr': 59.42499923706055,\n",
              "  'mean': 111.18785140409048,\n",
              "  'med': 104.64999771118164,\n",
              "  'q1': 79.04999923706055,\n",
              "  'q3': 138.4749984741211,\n",
              "  'whishi': 178.5,\n",
              "  'whislo': 59.400001525878906}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:32:30.551939Z",
          "start_time": "2020-12-14T14:32:29.964062Z"
        },
        "scrolled": false,
        "id": "RwmhqQo0YywL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7fae25b6-a5b0-4be5-9254-2e0accce10ce"
      },
      "source": [
        "from matplotlib.cbook import boxplot_stats\n",
        "\n",
        "enter_col = \"Open\"\n",
        "\n",
        "# obtaining the list of outliers\n",
        "iqr_outliers = [y for stat in boxplot_stats(df[enter_col]) for y in stat['fliers']]\n",
        "\n",
        "# obtain the rows having outliers for the selected column\n",
        "df_iqr_outlier = df.loc[df[enter_col].isin(iqr_outliers)]\n",
        "\n",
        "iqr_stats = boxplot_stats(df[enter_col])\n",
        "for items in iqr_stats:\n",
        "    q1 = items['q1']\n",
        "    q3 = items['q3']\n",
        "    iqr = items['iqr']\n",
        "    \n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Box(x=df[enter_col],name=enter_col))\n",
        "fig.update_layout(title=\"Box Plot for `{}` column\".format(enter_col),title_x=0.5)\n",
        "fig.show(config={'displaylogo': False})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"82d3dd79-b22e-4a5d-9b0c-49313c5657d8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"82d3dd79-b22e-4a5d-9b0c-49313c5657d8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '82d3dd79-b22e-4a5d-9b0c-49313c5657d8',\n",
              "                        [{\"name\": \"Open\", \"type\": \"box\", \"x\": [138.5, 133.5, 138.0, 136.8000030517578, 137.1999969482422, 141.60000610351562, 145.0, 148.60000610351562, 149.0, 147.5, 148.75, 144.39999389648438, 149.89999389648438, 148.75, 155.10000610351562, 154.8000030517578, 152.10000610351562, 151.5500030517578, 154.0, 151.25, 150.0, 151.89999389648438, 155.0, 155.9499969482422, 156.64999389648438, 152.9499969482422, 152.0, 159.0, 160.14999389648438, 160.0, 160.5, 159.1999969482422, 158.5500030517578, 155.25, 156.0, 157.8000030517578, 158.60000610351562, 156.9499969482422, 158.0, 159.0, 158.3000030517578, 156.5, 160.85000610351562, 160.6999969482422, 158.0, 165.6999969482422, 168.35000610351562, 169.5, 168.0, 169.3000030517578, 168.85000610351562, 170.0, 170.39999389648438, 171.39999389648438, 167.39999389648438, 168.39999389648438, 167.1999969482422, 162.0, 164.60000610351562, 163.39999389648438, 166.5, 170.0, 175.10000610351562, 174.85000610351562, 178.5, 174.0, 174.0, 173.14999389648438, 173.89999389648438, 172.0, 168.0, 173.0, 172.89999389648438, 170.5, 169.60000610351562, 168.10000610351562, 164.85000610351562, 168.0500030517578, 168.8000030517578, 168.9499969482422, 169.5, 164.0, 166.8000030517578, 166.3000030517578, 171.75, 171.14999389648438, 165.5, 166.10000610351562, 167.5500030517578, 170.3000030517578, 168.1999969482422, 163.0, 165.8000030517578, 166.39999389648438, 167.10000610351562, 161.4499969482422, 152.39999389648438, 153.0500030517578, 153.0, 153.0, 150.0, 151.0, 152.39999389648438, 149.9499969482422, 144.5, 144.0, 146.5, 145.39999389648438, 144.3000030517578, 144.39999389648438, 142.0, 141.39999389648438, 137.6999969482422, 138.5, 136.0, 132.8000030517578, 130.85000610351562, 131.4499969482422, 130.5, 131.64999389648438, 130.9499969482422, 129.0, 127.69999694824219, 128.0, 126.5999984741211, 125.0, 121.19999694824219, 116.3499984741211, 124.0, 124.0999984741211, 125.05000305175781, 120.0, 123.5, 120.5, 117.5, 119.44999694824219, 126.4000015258789, 127.30000305175781, 129.1999969482422, 126.25, 126.25, 130.9499969482422, 130.75, 129.6999969482422, 127.0, 124.4000015258789, 137.4499969482422, 136.25, 132.0, 131.8000030517578, 135.89999389648438, 131.10000610351562, 131.5, 128.1999969482422, 129.9499969482422, 129.5500030517578, 126.44999694824219, 125.8499984741211, 126.30000305175781, 129.4499969482422, 137.0, 140.0, 141.10000610351562, 140.10000610351562, 143.5, 144.25, 141.89999389648438, 141.60000610351562, 140.5500030517578, 144.0, 140.60000610351562, 140.5500030517578, 144.14999389648438, 148.1999969482422, 146.35000610351562, 144.9499969482422, 141.9499969482422, 138.39999389648438, 139.10000610351562, 138.1999969482422, 137.3000030517578, 135.4499969482422, 135.0, 132.85000610351562, 133.6999969482422, 131.0, 133.85000610351562, 131.85000610351562, 130.39999389648438, 132.39999389648438, 133.0500030517578, 131.0500030517578, 128.10000610351562, 126.9000015258789, 129.3000030517578, 129.5, 126.9000015258789, 126.4000015258789, 126.0999984741211, 128.10000610351562, 126.44999694824219, 126.5, 125.1500015258789, 125.5999984741211, 125.0999984741211, 126.0, 124.9000015258789, 124.9000015258789, 126.0, 129.5, 128.39999389648438, 127.19999694824219, 128.75, 127.6500015258789, 131.0, 129.6999969482422, 125.5999984741211, 125.0, 124.94999694824219, 124.44999694824219, 124.25, 125.44999694824219, 125.05000305175781, 125.0, 124.55000305175781, 126.0, 123.05000305175781, 122.0, 117.25, 118.0, 118.3499984741211, 117.55000305175781, 117.3499984741211, 117.55000305175781, 115.80000305175781, 104.3499984741211, 104.0999984741211, 106.4000015258789, 106.80000305175781, 107.30000305175781, 109.1500015258789, 107.1500015258789, 107.0, 107.0, 105.55000305175781, 101.5, 99.94999694824219, 100.30000305175781, 102.0, 102.5, 98.69999694824219, 97.4000015258789, 95.69999694824219, 90.30000305175781, 94.0, 90.9000015258789, 94.19999694824219, 93.3499984741211, 89.5, 82.1500015258789, 76.75, 66.5999984741211, 59.400001525878906, 63.5, 61.099998474121094, 61.599998474121094, 62.150001525878906, 63.04999923706055, 60.650001525878906, 63.5, 61.20000076293945, 61.849998474121094, 66.0, 61.04999923706055, 65.0, 67.0, 69.0, 71.19999694824219, 72.4000015258789, 76.0, 78.80000305175781, 75.0999984741211, 74.0, 77.80000305175781, 75.3499984741211, 69.69999694824219, 65.9000015258789, 67.5, 67.1500015258789, 68.0, 67.80000305175781, 69.4000015258789, 72.5999984741211, 76.0, 78.9000015258789, 81.3499984741211, 78.30000305175781, 76.4000015258789, 76.0, 76.94999694824219, 79.0, 77.0, 77.25, 76.94999694824219, 74.4000015258789, 77.19999694824219, 78.75, 76.94999694824219, 77.0, 77.19999694824219, 77.5, 78.05000305175781, 85.0, 84.44999694824219, 86.05000305175781, 85.0, 85.0999984741211, 90.0, 91.94999694824219, 90.5999984741211, 88.30000305175781, 81.55000305175781, 84.0, 85.5, 83.5999984741211, 83.0, 85.5, 86.0999984741211, 85.5, 86.25, 82.25, 83.0999984741211, 83.94999694824219, 83.5, 79.0, 81.3499984741211, 82.44999694824219, 83.05000305175781, 83.75, 82.0, 81.5999984741211, 80.0, 79.19999694824219, 78.5, 78.0, 76.4000015258789, 76.30000305175781, 81.0, 80.5999984741211, 82.75, 82.5, 82.0999984741211, 81.4000015258789, 81.0, 79.80000305175781, 80.0, 77.94999694824219, 78.3499984741211, 76.30000305175781, 76.5, 77.8499984741211, 78.1500015258789, 79.5, 79.55000305175781, 78.44999694824219, 78.55000305175781, 78.69999694824219, 77.5, 79.75, 80.8499984741211, 79.0, 82.94999694824219, 81.30000305175781, 81.4000015258789, 81.19999694824219, 81.80000305175781, 80.3499984741211, 81.0, 82.75, 81.1500015258789, 80.5, 78.55000305175781, 78.19999694824219, 76.55000305175781, 73.94999694824219, 73.5, 73.44999694824219, 73.8499984741211, 73.75, 74.25, 73.05000305175781, 73.80000305175781, 74.0, 71.0, 69.69999694824219, 66.05000305175781, 68.0, 69.44999694824219, 72.5, 69.4000015258789, 67.5, 69.0, 70.0, 69.30000305175781, 70.9000015258789, 69.0, 70.30000305175781, 69.0999984741211, 68.5, 67.0, 66.6500015258789, 67.3499984741211, 69.5, 68.0, 67.4000015258789, 69.19999694824219, 69.5, 68.0999984741211, 67.25, 66.05000305175781, 65.0, 65.4000015258789, 66.0, 66.30000305175781, 66.9000015258789, 67.69999694824219, 68.6500015258789, 69.05000305175781, 71.44999694824219, 72.5, 71.19999694824219, 72.69999694824219, 71.6500015258789, 71.5, 72.44999694824219, 72.0, 76.75, 77.19999694824219, 81.1500015258789, 80.80000305175781, 78.5, 81.5, 85.4000015258789, 89.75, 91.1500015258789, 93.0, 91.5, 91.0, 92.5999984741211, 98.25, 102.5, 101.9000015258789, 105.0999984741211, 101.5, 98.9000015258789, 89.05000305175781, 90.5, 92.5, 94.0, 94.19999694824219, 93.5, 93.30000305175781, 93.75, 94.05000305175781, 96.5, 98.9000015258789, 98.0, 98.94999694824219, 101.5, 102.0, 104.94999694824219, 107.0, 105.25, 101.4000015258789, 97.75, 99.0, 99.05000305175781, 94.5999984741211, 93.05000305175781, 91.4000015258789, 89.0, 90.75, 89.0, 92.5, 94.5999984741211, 94.25, 98.94999694824219, 99.5, 99.80000305175781, 102.0, 100.0, 98.9000015258789, 97.0, 99.25, 102.0, 103.69999694824219, 110.69999694824219, 105.9000015258789]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Box Plot for `Open` column\", \"x\": 0.5}},\n",
              "                        {\"displaylogo\": false, \"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('82d3dd79-b22e-4a5d-9b0c-49313c5657d8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0css1S8YywM"
      },
      "source": [
        "### Using Percentile \n",
        "\n",
        "In this method, the user will enter a __percentile value__ in the _range of 0 to 0.25 having a step size of 0.001_ , based on which the __upper and lower limit of value for a specific column__ will be obtained. Then the values residing __below the lower__ and __above the upper limit__ are considered as __outliers__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E55LMByu_xLM"
      },
      "source": [
        "# function to calculate the percentile score\r\n",
        "def calc_percentile(data, rqrd_col, low):\r\n",
        "    # obtain the outlier threshold values\r\n",
        "    high = 1 - low\r\n",
        "    percentile_outlier = data[rqrd_col].quantile([low, high])\r\n",
        "\r\n",
        "    # obtain the values\r\n",
        "    lower_val = percentile_outlier[low]\r\n",
        "    upper_val = percentile_outlier[high]\r\n",
        "\r\n",
        "    # obtain the outliers and inliers\r\n",
        "    perc_outliers = data[rqrd_col][(data[rqrd_col]<=lower_val) | (data[rqrd_col]>=upper_val)]\r\n",
        "    perc_inliers = data[rqrd_col][(data[rqrd_col]>lower_val) & (data[rqrd_col]<upper_val)]\r\n",
        "\r\n",
        "    # put the outliers and inliers in a dataframe\r\n",
        "    df_perc_outlier = pd.DataFrame({'Normal':perc_inliers, 'Outlier':perc_outliers})\r\n",
        "    \r\n",
        "    return df_perc_outlier, lower_val, upper_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gONDuKXn_yYb",
        "outputId": "5198e1ca-15b8-46a3-8f87-54a629cfce0c"
      },
      "source": [
        "calc_percentile(df, 'Open', low = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                Normal  Outlier\n",
              " Date                           \n",
              " 2019-02-12  138.500000      NaN\n",
              " 2019-02-14  133.500000      NaN\n",
              " 2019-02-15  138.000000      NaN\n",
              " 2019-02-18  136.800003      NaN\n",
              " 2019-02-19  137.199997      NaN\n",
              " ...                ...      ...\n",
              " 2021-02-16   99.250000      NaN\n",
              " 2021-02-17  102.000000      NaN\n",
              " 2021-02-18  103.699997      NaN\n",
              " 2021-02-19  110.699997      NaN\n",
              " 2021-02-22  105.900002      NaN\n",
              " \n",
              " [498 rows x 2 columns], 69.5, 161.61499786376953)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:33:13.573011Z",
          "start_time": "2020-12-14T14:33:10.289313Z"
        },
        "code_folding": [
          5
        ],
        "id": "v0xljzYBYywM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "eadf505d-f555-4f15-db64-4a61cc381c6a"
      },
      "source": [
        "df_perc_outlier, lower_val, upper_val = calc_percentile(df, 'Open', 0.1)\n",
        "\n",
        "# generate the histogram plot\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=df_perc_outlier['Normal'], name=\"Normal\",\n",
        "                            marker_color='green', nbinsx = 100))\n",
        "fig.add_trace(go.Histogram(x=df_perc_outlier['Outlier'], name=\"Outlier\", \n",
        "                            marker_color='red', nbinsx = 100))\n",
        "fig.update_layout(height=600, width=900,\n",
        "                  title=\"Histogram distribution for `{}` column\".format(enter_col),\n",
        "                  title_x=0.5)\n",
        "fig.update_xaxes(title_text=enter_col)\n",
        "fig.update_yaxes(title_text='count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"cec4cddf-4cc6-4c37-b3d6-164635cb7884\" class=\"plotly-graph-div\" style=\"height:600px; width:900px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"cec4cddf-4cc6-4c37-b3d6-164635cb7884\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'cec4cddf-4cc6-4c37-b3d6-164635cb7884',\n",
              "                        [{\"marker\": {\"color\": \"green\"}, \"name\": \"Normal\", \"nbinsx\": 100, \"type\": \"histogram\", \"x\": [138.5, 133.5, 138.0, 136.8000030517578, 137.1999969482422, 141.60000610351562, 145.0, 148.60000610351562, 149.0, 147.5, 148.75, 144.39999389648438, 149.89999389648438, 148.75, 155.10000610351562, 154.8000030517578, 152.10000610351562, 151.5500030517578, 154.0, 151.25, 150.0, 151.89999389648438, 155.0, 155.9499969482422, 156.64999389648438, 152.9499969482422, 152.0, 159.0, 160.14999389648438, 160.0, 160.5, 159.1999969482422, 158.5500030517578, 155.25, 156.0, 157.8000030517578, 158.60000610351562, 156.9499969482422, 158.0, 159.0, 158.3000030517578, 156.5, 160.85000610351562, 160.6999969482422, 158.0, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 161.4499969482422, 152.39999389648438, 153.0500030517578, 153.0, 153.0, 150.0, 151.0, 152.39999389648438, 149.9499969482422, 144.5, 144.0, 146.5, 145.39999389648438, 144.3000030517578, 144.39999389648438, 142.0, 141.39999389648438, 137.6999969482422, 138.5, 136.0, 132.8000030517578, 130.85000610351562, 131.4499969482422, 130.5, 131.64999389648438, 130.9499969482422, 129.0, 127.69999694824219, 128.0, 126.5999984741211, 125.0, 121.19999694824219, 116.3499984741211, 124.0, 124.0999984741211, 125.05000305175781, 120.0, 123.5, 120.5, 117.5, 119.44999694824219, 126.4000015258789, 127.30000305175781, 129.1999969482422, 126.25, 126.25, 130.9499969482422, 130.75, 129.6999969482422, 127.0, 124.4000015258789, 137.4499969482422, 136.25, 132.0, 131.8000030517578, 135.89999389648438, 131.10000610351562, 131.5, 128.1999969482422, 129.9499969482422, 129.5500030517578, 126.44999694824219, 125.8499984741211, 126.30000305175781, 129.4499969482422, 137.0, 140.0, 141.10000610351562, 140.10000610351562, 143.5, 144.25, 141.89999389648438, 141.60000610351562, 140.5500030517578, 144.0, 140.60000610351562, 140.5500030517578, 144.14999389648438, 148.1999969482422, 146.35000610351562, 144.9499969482422, 141.9499969482422, 138.39999389648438, 139.10000610351562, 138.1999969482422, 137.3000030517578, 135.4499969482422, 135.0, 132.85000610351562, 133.6999969482422, 131.0, 133.85000610351562, 131.85000610351562, 130.39999389648438, 132.39999389648438, 133.0500030517578, 131.0500030517578, 128.10000610351562, 126.9000015258789, 129.3000030517578, 129.5, 126.9000015258789, 126.4000015258789, 126.0999984741211, 128.10000610351562, 126.44999694824219, 126.5, 125.1500015258789, 125.5999984741211, 125.0999984741211, 126.0, 124.9000015258789, 124.9000015258789, 126.0, 129.5, 128.39999389648438, 127.19999694824219, 128.75, 127.6500015258789, 131.0, 129.6999969482422, 125.5999984741211, 125.0, 124.94999694824219, 124.44999694824219, 124.25, 125.44999694824219, 125.05000305175781, 125.0, 124.55000305175781, 126.0, 123.05000305175781, 122.0, 117.25, 118.0, 118.3499984741211, 117.55000305175781, 117.3499984741211, 117.55000305175781, 115.80000305175781, 104.3499984741211, 104.0999984741211, 106.4000015258789, 106.80000305175781, 107.30000305175781, 109.1500015258789, 107.1500015258789, 107.0, 107.0, 105.55000305175781, 101.5, 99.94999694824219, 100.30000305175781, 102.0, 102.5, 98.69999694824219, 97.4000015258789, 95.69999694824219, 90.30000305175781, 94.0, 90.9000015258789, 94.19999694824219, 93.3499984741211, 89.5, 82.1500015258789, 76.75, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 71.19999694824219, 72.4000015258789, 76.0, 78.80000305175781, 75.0999984741211, 74.0, 77.80000305175781, 75.3499984741211, 69.69999694824219, null, null, null, null, null, null, 72.5999984741211, 76.0, 78.9000015258789, 81.3499984741211, 78.30000305175781, 76.4000015258789, 76.0, 76.94999694824219, 79.0, 77.0, 77.25, 76.94999694824219, 74.4000015258789, 77.19999694824219, 78.75, 76.94999694824219, 77.0, 77.19999694824219, 77.5, 78.05000305175781, 85.0, 84.44999694824219, 86.05000305175781, 85.0, 85.0999984741211, 90.0, 91.94999694824219, 90.5999984741211, 88.30000305175781, 81.55000305175781, 84.0, 85.5, 83.5999984741211, 83.0, 85.5, 86.0999984741211, 85.5, 86.25, 82.25, 83.0999984741211, 83.94999694824219, 83.5, 79.0, 81.3499984741211, 82.44999694824219, 83.05000305175781, 83.75, 82.0, 81.5999984741211, 80.0, 79.19999694824219, 78.5, 78.0, 76.4000015258789, 76.30000305175781, 81.0, 80.5999984741211, 82.75, 82.5, 82.0999984741211, 81.4000015258789, 81.0, 79.80000305175781, 80.0, 77.94999694824219, 78.3499984741211, 76.30000305175781, 76.5, 77.8499984741211, 78.1500015258789, 79.5, 79.55000305175781, 78.44999694824219, 78.55000305175781, 78.69999694824219, 77.5, 79.75, 80.8499984741211, 79.0, 82.94999694824219, 81.30000305175781, 81.4000015258789, 81.19999694824219, 81.80000305175781, 80.3499984741211, 81.0, 82.75, 81.1500015258789, 80.5, 78.55000305175781, 78.19999694824219, 76.55000305175781, 73.94999694824219, 73.5, 73.44999694824219, 73.8499984741211, 73.75, 74.25, 73.05000305175781, 73.80000305175781, 74.0, 71.0, 69.69999694824219, null, null, null, 72.5, null, null, null, 70.0, null, 70.9000015258789, null, 70.30000305175781, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 71.44999694824219, 72.5, 71.19999694824219, 72.69999694824219, 71.6500015258789, 71.5, 72.44999694824219, 72.0, 76.75, 77.19999694824219, 81.1500015258789, 80.80000305175781, 78.5, 81.5, 85.4000015258789, 89.75, 91.1500015258789, 93.0, 91.5, 91.0, 92.5999984741211, 98.25, 102.5, 101.9000015258789, 105.0999984741211, 101.5, 98.9000015258789, 89.05000305175781, 90.5, 92.5, 94.0, 94.19999694824219, 93.5, 93.30000305175781, 93.75, 94.05000305175781, 96.5, 98.9000015258789, 98.0, 98.94999694824219, 101.5, 102.0, 104.94999694824219, 107.0, 105.25, 101.4000015258789, 97.75, 99.0, 99.05000305175781, 94.5999984741211, 93.05000305175781, 91.4000015258789, 89.0, 90.75, 89.0, 92.5, 94.5999984741211, 94.25, 98.94999694824219, 99.5, 99.80000305175781, 102.0, 100.0, 98.9000015258789, 97.0, 99.25, 102.0, 103.69999694824219, 110.69999694824219, 105.9000015258789]}, {\"marker\": {\"color\": \"red\"}, \"name\": \"Outlier\", \"nbinsx\": 100, \"type\": \"histogram\", \"x\": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 165.6999969482422, 168.35000610351562, 169.5, 168.0, 169.3000030517578, 168.85000610351562, 170.0, 170.39999389648438, 171.39999389648438, 167.39999389648438, 168.39999389648438, 167.1999969482422, 162.0, 164.60000610351562, 163.39999389648438, 166.5, 170.0, 175.10000610351562, 174.85000610351562, 178.5, 174.0, 174.0, 173.14999389648438, 173.89999389648438, 172.0, 168.0, 173.0, 172.89999389648438, 170.5, 169.60000610351562, 168.10000610351562, 164.85000610351562, 168.0500030517578, 168.8000030517578, 168.9499969482422, 169.5, 164.0, 166.8000030517578, 166.3000030517578, 171.75, 171.14999389648438, 165.5, 166.10000610351562, 167.5500030517578, 170.3000030517578, 168.1999969482422, 163.0, 165.8000030517578, 166.39999389648438, 167.10000610351562, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 66.5999984741211, 59.400001525878906, 63.5, 61.099998474121094, 61.599998474121094, 62.150001525878906, 63.04999923706055, 60.650001525878906, 63.5, 61.20000076293945, 61.849998474121094, 66.0, 61.04999923706055, 65.0, 67.0, 69.0, null, null, null, null, null, null, null, null, null, 65.9000015258789, 67.5, 67.1500015258789, 68.0, 67.80000305175781, 69.4000015258789, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 66.05000305175781, 68.0, 69.44999694824219, null, 69.4000015258789, 67.5, 69.0, null, 69.30000305175781, null, 69.0, null, 69.0999984741211, 68.5, 67.0, 66.6500015258789, 67.3499984741211, 69.5, 68.0, 67.4000015258789, 69.19999694824219, 69.5, 68.0999984741211, 67.25, 66.05000305175781, 65.0, 65.4000015258789, 66.0, 66.30000305175781, 66.9000015258789, 67.69999694824219, 68.6500015258789, 69.05000305175781, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]}],\n",
              "                        {\"height\": 600, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Histogram distribution for `Open` column\", \"x\": 0.5}, \"width\": 900, \"xaxis\": {\"title\": {\"text\": \"Open\"}}, \"yaxis\": {\"title\": {\"text\": \"count\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cec4cddf-4cc6-4c37-b3d6-164635cb7884');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTjQcAxQYywM"
      },
      "source": [
        "## Multivariate Outlier Detection\n",
        "\n",
        "### Using Bonferroni Correction\n",
        "\n",
        "The Bonferroni correction is a multiple-comparison correction used when several dependent or independent statistical tests are being performed simultaneously. If multiple hypotheses are tested, the __chance of observing a rare event increases__, and therefore, the likelihood of incorrectly failing to accept a null hypothesis (i.e., making a __Type I error__) increases.\n",
        "\n",
        "The Bonferroni correction compensates for that increase by testing each hypothesis at a significance level of $\\alpha/m$, where $\\alpha$ is the desired overall __alpha level__ and $m$ is the number of hypotheses.\n",
        "\n",
        "In this process, the user can choose independent variable(s) and dependent variables to build a __linear regression__ model, and consider a data point as an outlier if its corresponding __Bonferroni corrected p-value__ is less than the threshold provided by the user.\n",
        "\n",
        "To obtain a visual understanding of the outlier(s), the user can select any variable from the chosen independent variable(s). This will be plotted against the selected dependent variable. Also, the outliers in this plot are colored as red and _the size of the points plotted depends on the third variable selected by the user_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueAOVb2_EObe"
      },
      "source": [
        "num_cols = df._get_numeric_data().columns.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1KMtrYPEQRb",
        "outputId": "f0f6d51e-e202-4026-a870-063d6a6366bc"
      },
      "source": [
        "from statsmodels.formula.api import ols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:34:19.723464Z",
          "start_time": "2020-12-14T14:33:22.060510Z"
        },
        "id": "a4F_SwMtYywM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d262b1-f04e-42bf-e689-d5d7ddb3fc31"
      },
      "source": [
        "\n",
        "while True:\n",
        "    dep_col_in = input('\\nEnter the dependent column name: ')\n",
        "    if dep_col_in in num_cols:\n",
        "        break\n",
        "    else:\n",
        "        print(colored('\\nPlease enter column name properly.','red',attrs=['bold']))\n",
        "        continue\n",
        "\n",
        "num_cols_available = num_cols.copy()\n",
        "num_cols_available.remove(dep_col_in)\n",
        "\n",
        "while True:\n",
        "    indep_cols_in = input('\\nEnter the independent column name(s) (comma `,` separated for multiple): ')\n",
        "    # if user enters nothing, then it will ask for some input\n",
        "    if indep_cols_in == '':\n",
        "        print(colored(\"\\nPlease enter atleast one column name!\",\"red\",attrs=['bold']))\n",
        "        continue\n",
        "    else:\n",
        "        # if it is a single column entry, keep on asking for input unless correctly entered\n",
        "        if ',' not in indep_cols_in:\n",
        "            if ' ' in indep_cols_in:\n",
        "                print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
        "                continue\n",
        "            else:\n",
        "                if indep_cols_in in num_cols_available:\n",
        "                    indep_cols = [indep_cols_in]\n",
        "                    break\n",
        "                else:\n",
        "                    print(colored(\"\\nIncorrect/dependent column name entered. Please try again!\",\"red\",attrs=['bold']))\n",
        "                    continue\n",
        "        # if it is multiple column entry and properly entered, then it is stored in list\n",
        "        else:\n",
        "            # split the string with ',' and convert it into list\n",
        "            # check if any element is empty, then eliminate it\n",
        "            # remove extra white spaces from column name using `strip`\n",
        "\n",
        "            indep_cols = [i.strip() for i in indep_cols_in.split(',') if i]\n",
        "            indep_cols_check =  all(elem in num_cols_available for elem in indep_cols)\n",
        "            if indep_cols_check:\n",
        "                break\n",
        "            else:\n",
        "                print(colored(\"\\nIncorrect/dependent column name entered. Please try again!\",\"red\",attrs=['bold']))\n",
        "                continue\n",
        "\n",
        "while True:\n",
        "    # enter the throshold value for z-score\n",
        "    bonferroni_thresh = float(input('\\nEnter the threshold value (0.0 to 1.0 having step size of 0.01): '))\n",
        "\n",
        "    # check if the value entered lies in the range\n",
        "    bonferroni_thresh_range = [round(i,2) for i in list(np.arange(0.0, 1.01, 0.01))]\n",
        "\n",
        "    if bonferroni_thresh not in bonferroni_thresh_range:\n",
        "        # if not, keep on asking to enter\n",
        "        print(colored(\"\\nIncorrect range! Please enter properly.\",\"red\",attrs=['bold']))\n",
        "        continue\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "if len(indep_cols) == 1:\n",
        "    reg_indep_expression = indep_cols[0]\n",
        "else:\n",
        "    reg_indep_expression = ' + '.join(indep_cols)\n",
        "\n",
        "# perform OLS regression method\n",
        "ols_model = ols('{} ~ {}'.format(dep_col_in, reg_indep_expression), data=df).fit()\n",
        "\n",
        "# obtain the results of the outliers\n",
        "outlier_result = ols_model.outlier_test()\n",
        "\n",
        "# merge the outlier result with the original dataframe\n",
        "df_ols_merged = pd.concat([outlier_result, df], axis=1)\n",
        "\n",
        "# tag the outliers based on the threshold\n",
        "df_ols_merged['outlier_flag'] = ['Outlier' if i < bonferroni_thresh else 'Normal' for i in df_ols_merged['bonf(p)']]\n",
        "\n",
        "# filter the outliers based on the threshold mentioned and display the result\n",
        "df_ols_outlier_merged = df_ols_merged[df_ols_merged['bonf(p)'] < bonferroni_thresh]\n",
        "df_ols_outlier_merged.insert(loc=0, column='index',value=df_ols_outlier_merged.index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Enter the dependent column name: Open\n",
            "\n",
            "Enter the independent column name(s) (comma `,` separated for multiple): Close, High\n",
            "\n",
            "Enter the threshold value (0.0 to 1.0 having step size of 0.01): 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "FoLWpRPWFFXR",
        "outputId": "0d305d0c-eccb-4fdd-caa3-2ef3a9540fee"
      },
      "source": [
        "outlier_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student_resid</th>\n",
              "      <th>unadj_p</th>\n",
              "      <th>bonf(p)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-02-12</th>\n",
              "      <td>-0.110812</td>\n",
              "      <td>0.911811</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-14</th>\n",
              "      <td>0.560637</td>\n",
              "      <td>0.575299</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-15</th>\n",
              "      <td>-0.047104</td>\n",
              "      <td>0.962449</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-18</th>\n",
              "      <td>-0.145825</td>\n",
              "      <td>0.884119</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-19</th>\n",
              "      <td>-0.730532</td>\n",
              "      <td>0.465412</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-16</th>\n",
              "      <td>-2.043048</td>\n",
              "      <td>0.041578</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-17</th>\n",
              "      <td>-0.004546</td>\n",
              "      <td>0.996375</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-18</th>\n",
              "      <td>-4.731802</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.001449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-19</th>\n",
              "      <td>0.719516</td>\n",
              "      <td>0.472163</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-22</th>\n",
              "      <td>-0.438403</td>\n",
              "      <td>0.661285</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>498 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            student_resid   unadj_p   bonf(p)\n",
              "2019-02-12      -0.110812  0.911811  1.000000\n",
              "2019-02-14       0.560637  0.575299  1.000000\n",
              "2019-02-15      -0.047104  0.962449  1.000000\n",
              "2019-02-18      -0.145825  0.884119  1.000000\n",
              "2019-02-19      -0.730532  0.465412  1.000000\n",
              "...                   ...       ...       ...\n",
              "2021-02-16      -2.043048  0.041578  1.000000\n",
              "2021-02-17      -0.004546  0.996375  1.000000\n",
              "2021-02-18      -4.731802  0.000003  0.001449\n",
              "2021-02-19       0.719516  0.472163  1.000000\n",
              "2021-02-22      -0.438403  0.661285  1.000000\n",
              "\n",
              "[498 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8LuOFNME9Xz",
        "outputId": "88ea6a00-df51-4892-c167-19f71f8241a1"
      },
      "source": [
        "indep_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Close', 'High']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:34:45.660717Z",
          "start_time": "2020-12-14T14:34:22.333322Z"
        },
        "id": "_6bKpj0oYywN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "7355f1f0-70cb-4465-908d-9489cfeb3fbc"
      },
      "source": [
        "# selecting one of the selected independent column names\n",
        "print('_'*75)\n",
        "\n",
        "while True:\n",
        "    indep_col_in = input('\\nEnter any one column name from the set of independent variable(s) you entered: ')\n",
        "    if indep_col_in in indep_cols:\n",
        "        break\n",
        "    else:\n",
        "        print(colored('\\nPlease enter column name properly.','red',attrs=['bold']))\n",
        "        continue\n",
        "\n",
        "while True:\n",
        "    ref_col_in = input('\\nEnter the reference column name from the above shown list: ')\n",
        "    if ref_col_in in num_cols:\n",
        "        break\n",
        "    else:\n",
        "        print(colored('\\nPlease enter column name properly.','red',attrs=['bold']))\n",
        "        continue\n",
        "\n",
        "fig = px.scatter(df_ols_merged, x=indep_col_in, y=dep_col_in, size=ref_col_in,\n",
        "                  color='outlier_flag', color_discrete_sequence=['green','red'],\n",
        "                  height=600, width=900, \n",
        "                  title=\"Scatter Plot of `{}` VS `{}` with marker size w.r.t. the value of `{}`\".format(dep_col_in, indep_col_in, ref_col_in)\n",
        "                  )\n",
        "fig.update_layout(legend=dict(\n",
        "    x=0,y=1,traceorder=\"normal\"))\n",
        "fig.show(config={'displaylogo': False})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "___________________________________________________________________________\n",
            "\n",
            "Enter any one column name from the set of independent variable(s) you entered: Close\n",
            "\n",
            "Enter the reference column name from the above shown list: Open\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4efe0b0c-bddd-4855-b684-ea8835d6b992\" class=\"plotly-graph-div\" style=\"height:600px; width:900px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4efe0b0c-bddd-4855-b684-ea8835d6b992\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4efe0b0c-bddd-4855-b684-ea8835d6b992',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"outlier_flag=Normal<br>Close=%{x}<br>Open=%{marker.size}\", \"legendgroup\": \"outlier_flag=Normal\", \"marker\": {\"color\": \"green\", \"size\": [138.5, 133.5, 138.0, 136.8000030517578, 137.1999969482422, 141.60000610351562, 145.0, 148.60000610351562, 149.0, 147.5, 148.75, 144.39999389648438, 149.89999389648438, 148.75, 155.10000610351562, 154.8000030517578, 152.10000610351562, 151.5500030517578, 154.0, 151.25, 150.0, 151.89999389648438, 155.0, 155.9499969482422, 156.64999389648438, 152.9499969482422, 152.0, 159.0, 160.14999389648438, 160.0, 160.5, 159.1999969482422, 158.5500030517578, 155.25, 156.0, 157.8000030517578, 158.60000610351562, 156.9499969482422, 158.0, 159.0, 158.3000030517578, 156.5, 160.85000610351562, 160.6999969482422, 158.0, 165.6999969482422, 168.35000610351562, 169.5, 168.0, 169.3000030517578, 168.85000610351562, 170.0, 170.39999389648438, 171.39999389648438, 167.39999389648438, 168.39999389648438, 167.1999969482422, 162.0, 164.60000610351562, 163.39999389648438, 166.5, 170.0, 175.10000610351562, 174.85000610351562, 178.5, 174.0, 174.0, 173.14999389648438, 173.89999389648438, 172.0, 168.0, 173.0, 172.89999389648438, 170.5, 169.60000610351562, 168.10000610351562, 164.85000610351562, 168.0500030517578, 168.8000030517578, 168.9499969482422, 169.5, 164.0, 166.8000030517578, 166.3000030517578, 171.75, 171.14999389648438, 165.5, 166.10000610351562, 167.5500030517578, 170.3000030517578, 168.1999969482422, 163.0, 165.8000030517578, 166.39999389648438, 167.10000610351562, 161.4499969482422, 152.39999389648438, 153.0500030517578, 153.0, 153.0, 150.0, 151.0, 152.39999389648438, 149.9499969482422, 144.5, 144.0, 146.5, 145.39999389648438, 144.3000030517578, 144.39999389648438, 142.0, 141.39999389648438, 137.6999969482422, 138.5, 136.0, 132.8000030517578, 130.85000610351562, 131.4499969482422, 130.5, 131.64999389648438, 130.9499969482422, 129.0, 127.69999694824219, 128.0, 126.5999984741211, 125.0, 121.19999694824219, 116.3499984741211, 124.0, 124.0999984741211, 125.05000305175781, 120.0, 123.5, 120.5, 117.5, 126.4000015258789, 127.30000305175781, 129.1999969482422, 126.25, 126.25, 130.9499969482422, 130.75, 129.6999969482422, 127.0, 137.4499969482422, 136.25, 132.0, 131.8000030517578, 135.89999389648438, 131.10000610351562, 131.5, 128.1999969482422, 129.9499969482422, 129.5500030517578, 126.44999694824219, 125.8499984741211, 126.30000305175781, 129.4499969482422, 137.0, 140.0, 141.10000610351562, 140.10000610351562, 143.5, 144.25, 141.89999389648438, 141.60000610351562, 140.5500030517578, 144.0, 140.60000610351562, 140.5500030517578, 144.14999389648438, 148.1999969482422, 146.35000610351562, 144.9499969482422, 141.9499969482422, 138.39999389648438, 139.10000610351562, 138.1999969482422, 137.3000030517578, 135.4499969482422, 135.0, 132.85000610351562, 133.6999969482422, 131.0, 133.85000610351562, 131.85000610351562, 130.39999389648438, 132.39999389648438, 133.0500030517578, 131.0500030517578, 128.10000610351562, 126.9000015258789, 129.3000030517578, 129.5, 126.9000015258789, 126.4000015258789, 126.0999984741211, 128.10000610351562, 126.44999694824219, 126.5, 125.1500015258789, 125.5999984741211, 125.0999984741211, 126.0, 124.9000015258789, 124.9000015258789, 126.0, 129.5, 128.39999389648438, 127.19999694824219, 128.75, 127.6500015258789, 131.0, 129.6999969482422, 125.5999984741211, 125.0, 124.94999694824219, 124.44999694824219, 124.25, 125.44999694824219, 125.05000305175781, 125.0, 124.55000305175781, 126.0, 123.05000305175781, 122.0, 117.25, 118.0, 118.3499984741211, 117.55000305175781, 117.3499984741211, 117.55000305175781, 115.80000305175781, 104.3499984741211, 104.0999984741211, 106.4000015258789, 106.80000305175781, 107.30000305175781, 109.1500015258789, 107.1500015258789, 107.0, 107.0, 105.55000305175781, 101.5, 99.94999694824219, 100.30000305175781, 102.0, 102.5, 98.69999694824219, 97.4000015258789, 95.69999694824219, 90.30000305175781, 94.0, 90.9000015258789, 94.19999694824219, 93.3499984741211, 89.5, 82.1500015258789, 76.75, 66.5999984741211, 59.400001525878906, 63.5, 61.099998474121094, 62.150001525878906, 60.650001525878906, 63.5, 61.20000076293945, 61.849998474121094, 66.0, 61.04999923706055, 65.0, 67.0, 69.0, 71.19999694824219, 72.4000015258789, 76.0, 78.80000305175781, 75.0999984741211, 74.0, 77.80000305175781, 75.3499984741211, 69.69999694824219, 65.9000015258789, 67.5, 67.1500015258789, 68.0, 67.80000305175781, 69.4000015258789, 76.0, 78.9000015258789, 81.3499984741211, 78.30000305175781, 76.4000015258789, 76.0, 76.94999694824219, 79.0, 77.0, 77.25, 76.94999694824219, 74.4000015258789, 77.19999694824219, 78.75, 76.94999694824219, 77.0, 77.19999694824219, 77.5, 78.05000305175781, 85.0, 84.44999694824219, 86.05000305175781, 85.0, 85.0999984741211, 90.0, 91.94999694824219, 90.5999984741211, 88.30000305175781, 81.55000305175781, 84.0, 85.5, 83.5999984741211, 83.0, 85.5, 86.0999984741211, 85.5, 86.25, 82.25, 83.0999984741211, 83.94999694824219, 83.5, 79.0, 81.3499984741211, 82.44999694824219, 83.05000305175781, 83.75, 82.0, 81.5999984741211, 80.0, 79.19999694824219, 78.5, 78.0, 76.4000015258789, 76.30000305175781, 81.0, 80.5999984741211, 82.75, 82.5, 82.0999984741211, 81.4000015258789, 81.0, 79.80000305175781, 80.0, 77.94999694824219, 78.3499984741211, 76.30000305175781, 76.5, 77.8499984741211, 78.1500015258789, 79.5, 79.55000305175781, 78.44999694824219, 78.55000305175781, 78.69999694824219, 77.5, 79.75, 80.8499984741211, 79.0, 82.94999694824219, 81.30000305175781, 81.4000015258789, 81.19999694824219, 81.80000305175781, 80.3499984741211, 81.0, 82.75, 81.1500015258789, 80.5, 78.55000305175781, 78.19999694824219, 76.55000305175781, 73.94999694824219, 73.5, 73.44999694824219, 73.8499984741211, 73.75, 74.25, 73.05000305175781, 73.80000305175781, 74.0, 71.0, 69.69999694824219, 66.05000305175781, 68.0, 69.44999694824219, 72.5, 69.4000015258789, 67.5, 69.0, 70.0, 69.30000305175781, 70.9000015258789, 69.0, 70.30000305175781, 69.0999984741211, 68.5, 67.0, 66.6500015258789, 67.3499984741211, 69.5, 68.0, 67.4000015258789, 69.19999694824219, 69.5, 68.0999984741211, 67.25, 66.05000305175781, 65.0, 65.4000015258789, 66.0, 66.30000305175781, 66.9000015258789, 67.69999694824219, 68.6500015258789, 69.05000305175781, 71.44999694824219, 72.5, 71.19999694824219, 72.69999694824219, 71.6500015258789, 71.5, 72.44999694824219, 72.0, 76.75, 77.19999694824219, 81.1500015258789, 80.80000305175781, 78.5, 81.5, 85.4000015258789, 89.75, 91.1500015258789, 93.0, 91.5, 91.0, 98.25, 102.5, 101.9000015258789, 105.0999984741211, 101.5, 98.9000015258789, 89.05000305175781, 90.5, 92.5, 94.0, 94.19999694824219, 93.5, 93.30000305175781, 93.75, 94.05000305175781, 96.5, 98.9000015258789, 98.0, 98.94999694824219, 101.5, 102.0, 104.94999694824219, 107.0, 105.25, 101.4000015258789, 97.75, 99.0, 99.05000305175781, 94.5999984741211, 93.05000305175781, 91.4000015258789, 89.0, 90.75, 89.0, 92.5, 94.5999984741211, 94.25, 98.94999694824219, 99.5, 99.80000305175781, 102.0, 100.0, 98.9000015258789, 97.0, 99.25, 102.0, 110.69999694824219, 105.9000015258789], \"sizemode\": \"area\", \"sizeref\": 0.44625, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"outlier_flag=Normal\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [137.4499969482422, 132.14999389648438, 135.0, 137.10000610351562, 139.14999389648438, 144.0500030517578, 146.9499969482422, 148.60000610351562, 147.75, 147.5, 148.0, 148.64999389648438, 149.10000610351562, 155.0, 154.8000030517578, 152.0500030517578, 150.39999389648438, 153.0, 151.85000610351562, 149.75, 151.0500030517578, 156.0, 154.64999389648438, 157.0, 151.85000610351562, 152.35000610351562, 158.64999389648438, 160.0500030517578, 160.10000610351562, 155.89999389648438, 157.25, 157.5, 155.4499969482422, 155.25, 156.5500030517578, 158.10000610351562, 157.5500030517578, 158.0, 158.60000610351562, 157.60000610351562, 156.4499969482422, 160.4499969482422, 160.60000610351562, 157.89999389648438, 163.75, 168.64999389648438, 168.85000610351562, 168.39999389648438, 169.1999969482422, 168.89999389648438, 170.25, 170.14999389648438, 171.6999969482422, 168.89999389648438, 169.39999389648438, 166.3000030517578, 164.10000610351562, 164.14999389648438, 162.75, 165.9499969482422, 167.35000610351562, 176.0, 174.75, 177.0, 174.14999389648438, 174.39999389648438, 173.64999389648438, 175.3000030517578, 171.5500030517578, 169.4499969482422, 171.9499969482422, 172.14999389648438, 170.60000610351562, 169.39999389648438, 167.5, 164.64999389648438, 169.25, 170.85000610351562, 168.9499969482422, 169.3000030517578, 164.39999389648438, 166.14999389648438, 167.35000610351562, 172.0, 170.9499969482422, 165.1999969482422, 166.1999969482422, 167.5500030517578, 170.60000610351562, 167.75, 161.0, 165.64999389648438, 166.3000030517578, 167.10000610351562, 161.5, 152.39999389648438, 153.39999389648438, 151.64999389648438, 153.10000610351562, 149.6999969482422, 150.6999969482422, 152.25, 149.89999389648438, 143.39999389648438, 143.89999389648438, 146.64999389648438, 144.60000610351562, 143.60000610351562, 143.85000610351562, 141.89999389648438, 140.35000610351562, 138.75, 138.85000610351562, 136.89999389648438, 134.35000610351562, 131.3000030517578, 131.4499969482422, 130.5, 130.85000610351562, 130.9499969482422, 127.75, 126.8499984741211, 127.9000015258789, 126.30000305175781, 125.05000305175781, 121.19999694824219, 116.94999694824219, 122.25, 123.75, 125.5999984741211, 121.19999694824219, 123.25, 121.19999694824219, 117.1500015258789, 119.0, 127.94999694824219, 129.39999389648438, 125.75, 125.9000015258789, 128.85000610351562, 130.6999969482422, 129.6999969482422, 127.0, 124.30000305175781, 136.3000030517578, 133.5, 131.4499969482422, 137.0500030517578, 131.5, 131.8000030517578, 128.1999969482422, 128.10000610351562, 129.5500030517578, 126.55000305175781, 125.8499984741211, 125.5, 129.0, 135.25, 138.64999389648438, 141.64999389648438, 141.39999389648438, 143.5, 144.25, 141.89999389648438, 140.6999969482422, 139.5500030517578, 142.14999389648438, 140.5500030517578, 141.6999969482422, 144.14999389648438, 147.39999389648438, 146.35000610351562, 144.64999389648438, 142.0500030517578, 138.3000030517578, 139.10000610351562, 138.1999969482422, 136.1999969482422, 135.5500030517578, 133.89999389648438, 133.25, 133.6999969482422, 131.14999389648438, 133.85000610351562, 130.9499969482422, 130.25, 132.64999389648438, 133.14999389648438, 131.75, 128.10000610351562, 127.6500015258789, 130.0500030517578, 128.89999389648438, 126.55000305175781, 126.30000305175781, 125.19999694824219, 128.0, 125.9000015258789, 126.5, 125.5999984741211, 125.30000305175781, 125.0999984741211, 125.69999694824219, 125.1500015258789, 124.6500015258789, 126.0, 128.0, 128.39999389648438, 127.94999694824219, 128.8000030517578, 127.44999694824219, 128.0500030517578, 128.4499969482422, 126.25, 125.75, 123.44999694824219, 123.69999694824219, 124.1500015258789, 125.44999694824219, 125.05000305175781, 124.6500015258789, 124.5, 125.4000015258789, 122.9000015258789, 122.75, 116.4000015258789, 117.80000305175781, 118.3499984741211, 117.25, 116.5999984741211, 117.6500015258789, 115.5999984741211, 108.94999694824219, 103.44999694824219, 106.1500015258789, 106.80000305175781, 107.25, 109.25, 106.19999694824219, 106.80000305175781, 106.69999694824219, 105.3499984741211, 103.30000305175781, 99.94999694824219, 99.30000305175781, 101.69999694824219, 102.80000305175781, 98.05000305175781, 97.55000305175781, 95.80000305175781, 93.3499984741211, 91.94999694824219, 89.0999984741211, 93.3499984741211, 92.80000305175781, 92.5999984741211, 88.5, 74.55000305175781, 71.69999694824219, 62.5, 65.9000015258789, 60.150001525878906, 60.0, 61.04999923706055, 60.400001525878906, 62.5, 61.54999923706055, 64.75, 64.44999694824219, 63.349998474121094, 68.30000305175781, 65.75, 69.8499984741211, 72.9000015258789, 74.44999694824219, 77.30000305175781, 75.05000305175781, 74.3499984741211, 75.44999694824219, 76.30000305175781, 74.0, 69.3499984741211, 65.3499984741211, 67.3499984741211, 67.5999984741211, 68.44999694824219, 69.19999694824219, 70.5, 76.55000305175781, 78.44999694824219, 79.25, 75.5999984741211, 76.0, 77.05000305175781, 76.30000305175781, 78.19999694824219, 76.1500015258789, 76.1500015258789, 72.9000015258789, 77.1500015258789, 78.0, 77.0, 76.1500015258789, 77.0, 77.94999694824219, 78.8499984741211, 83.19999694824219, 83.94999694824219, 84.30000305175781, 86.30000305175781, 84.9000015258789, 86.94999694824219, 90.94999694824219, 90.0, 88.25, 86.9000015258789, 83.9000015258789, 84.0999984741211, 83.9000015258789, 84.19999694824219, 83.5999984741211, 85.75, 84.8499984741211, 85.6500015258789, 83.0999984741211, 81.5999984741211, 84.1500015258789, 82.4000015258789, 81.3499984741211, 80.44999694824219, 81.8499984741211, 82.4000015258789, 83.75, 81.94999694824219, 81.30000305175781, 80.19999694824219, 78.55000305175781, 78.9000015258789, 77.3499984741211, 76.25, 76.1500015258789, 80.3499984741211, 80.30000305175781, 82.44999694824219, 82.5999984741211, 82.6500015258789, 81.0, 80.80000305175781, 79.9000015258789, 79.5, 77.69999694824219, 78.30000305175781, 75.9000015258789, 76.0999984741211, 76.94999694824219, 78.05000305175781, 78.6500015258789, 79.05000305175781, 78.44999694824219, 78.0999984741211, 78.5999984741211, 77.0999984741211, 79.30000305175781, 80.6500015258789, 79.55000305175781, 82.1500015258789, 81.05000305175781, 81.25, 80.5, 81.4000015258789, 80.1500015258789, 80.3499984741211, 81.94999694824219, 79.3499984741211, 80.30000305175781, 79.05000305175781, 77.9000015258789, 76.44999694824219, 74.44999694824219, 72.69999694824219, 73.6500015258789, 73.30000305175781, 73.6500015258789, 73.80000305175781, 73.05000305175781, 73.1500015258789, 74.30000305175781, 71.1500015258789, 69.5, 67.6500015258789, 66.6500015258789, 68.9000015258789, 71.8499984741211, 69.1500015258789, 69.25, 69.1500015258789, 69.19999694824219, 69.3499984741211, 70.5, 68.44999694824219, 70.30000305175781, 69.1500015258789, 69.0999984741211, 67.0, 66.05000305175781, 67.25, 69.30000305175781, 67.4000015258789, 67.80000305175781, 68.8499984741211, 69.0, 68.19999694824219, 66.8499984741211, 66.3499984741211, 64.4000015258789, 64.9000015258789, 65.69999694824219, 66.30000305175781, 66.44999694824219, 67.5999984741211, 68.0, 68.44999694824219, 70.75, 72.3499984741211, 71.55000305175781, 72.1500015258789, 71.30000305175781, 71.94999694824219, 72.1500015258789, 71.6500015258789, 76.5, 76.0999984741211, 80.80000305175781, 80.19999694824219, 78.5, 81.55000305175781, 84.8499984741211, 88.69999694824219, 89.8499984741211, 91.55000305175781, 90.75, 91.19999694824219, 91.6500015258789, 101.5, 100.44999694824219, 102.9000015258789, 101.5, 99.0, 89.8499984741211, 90.55000305175781, 90.80000305175781, 93.1500015258789, 93.80000305175781, 93.1500015258789, 93.25, 93.05000305175781, 93.19999694824219, 96.94999694824219, 94.94999694824219, 96.94999694824219, 97.9000015258789, 100.6500015258789, 102.55000305175781, 103.44999694824219, 105.25, 105.05000305175781, 101.4000015258789, 96.6500015258789, 98.0999984741211, 98.8499984741211, 94.69999694824219, 92.75, 91.3499984741211, 89.69999694824219, 90.6500015258789, 88.30000305175781, 90.8499984741211, 92.8499984741211, 93.3499984741211, 97.6500015258789, 97.6500015258789, 99.6500015258789, 101.0, 100.0, 99.44999694824219, 97.0, 98.44999694824219, 103.75, 102.25, 105.0999984741211, 106.30000305175781], \"xaxis\": \"x\", \"y\": [138.5, 133.5, 138.0, 136.8000030517578, 137.1999969482422, 141.60000610351562, 145.0, 148.60000610351562, 149.0, 147.5, 148.75, 144.39999389648438, 149.89999389648438, 148.75, 155.10000610351562, 154.8000030517578, 152.10000610351562, 151.5500030517578, 154.0, 151.25, 150.0, 151.89999389648438, 155.0, 155.9499969482422, 156.64999389648438, 152.9499969482422, 152.0, 159.0, 160.14999389648438, 160.0, 160.5, 159.1999969482422, 158.5500030517578, 155.25, 156.0, 157.8000030517578, 158.60000610351562, 156.9499969482422, 158.0, 159.0, 158.3000030517578, 156.5, 160.85000610351562, 160.6999969482422, 158.0, 165.6999969482422, 168.35000610351562, 169.5, 168.0, 169.3000030517578, 168.85000610351562, 170.0, 170.39999389648438, 171.39999389648438, 167.39999389648438, 168.39999389648438, 167.1999969482422, 162.0, 164.60000610351562, 163.39999389648438, 166.5, 170.0, 175.10000610351562, 174.85000610351562, 178.5, 174.0, 174.0, 173.14999389648438, 173.89999389648438, 172.0, 168.0, 173.0, 172.89999389648438, 170.5, 169.60000610351562, 168.10000610351562, 164.85000610351562, 168.0500030517578, 168.8000030517578, 168.9499969482422, 169.5, 164.0, 166.8000030517578, 166.3000030517578, 171.75, 171.14999389648438, 165.5, 166.10000610351562, 167.5500030517578, 170.3000030517578, 168.1999969482422, 163.0, 165.8000030517578, 166.39999389648438, 167.10000610351562, 161.4499969482422, 152.39999389648438, 153.0500030517578, 153.0, 153.0, 150.0, 151.0, 152.39999389648438, 149.9499969482422, 144.5, 144.0, 146.5, 145.39999389648438, 144.3000030517578, 144.39999389648438, 142.0, 141.39999389648438, 137.6999969482422, 138.5, 136.0, 132.8000030517578, 130.85000610351562, 131.4499969482422, 130.5, 131.64999389648438, 130.9499969482422, 129.0, 127.69999694824219, 128.0, 126.5999984741211, 125.0, 121.19999694824219, 116.3499984741211, 124.0, 124.0999984741211, 125.05000305175781, 120.0, 123.5, 120.5, 117.5, 126.4000015258789, 127.30000305175781, 129.1999969482422, 126.25, 126.25, 130.9499969482422, 130.75, 129.6999969482422, 127.0, 137.4499969482422, 136.25, 132.0, 131.8000030517578, 135.89999389648438, 131.10000610351562, 131.5, 128.1999969482422, 129.9499969482422, 129.5500030517578, 126.44999694824219, 125.8499984741211, 126.30000305175781, 129.4499969482422, 137.0, 140.0, 141.10000610351562, 140.10000610351562, 143.5, 144.25, 141.89999389648438, 141.60000610351562, 140.5500030517578, 144.0, 140.60000610351562, 140.5500030517578, 144.14999389648438, 148.1999969482422, 146.35000610351562, 144.9499969482422, 141.9499969482422, 138.39999389648438, 139.10000610351562, 138.1999969482422, 137.3000030517578, 135.4499969482422, 135.0, 132.85000610351562, 133.6999969482422, 131.0, 133.85000610351562, 131.85000610351562, 130.39999389648438, 132.39999389648438, 133.0500030517578, 131.0500030517578, 128.10000610351562, 126.9000015258789, 129.3000030517578, 129.5, 126.9000015258789, 126.4000015258789, 126.0999984741211, 128.10000610351562, 126.44999694824219, 126.5, 125.1500015258789, 125.5999984741211, 125.0999984741211, 126.0, 124.9000015258789, 124.9000015258789, 126.0, 129.5, 128.39999389648438, 127.19999694824219, 128.75, 127.6500015258789, 131.0, 129.6999969482422, 125.5999984741211, 125.0, 124.94999694824219, 124.44999694824219, 124.25, 125.44999694824219, 125.05000305175781, 125.0, 124.55000305175781, 126.0, 123.05000305175781, 122.0, 117.25, 118.0, 118.3499984741211, 117.55000305175781, 117.3499984741211, 117.55000305175781, 115.80000305175781, 104.3499984741211, 104.0999984741211, 106.4000015258789, 106.80000305175781, 107.30000305175781, 109.1500015258789, 107.1500015258789, 107.0, 107.0, 105.55000305175781, 101.5, 99.94999694824219, 100.30000305175781, 102.0, 102.5, 98.69999694824219, 97.4000015258789, 95.69999694824219, 90.30000305175781, 94.0, 90.9000015258789, 94.19999694824219, 93.3499984741211, 89.5, 82.1500015258789, 76.75, 66.5999984741211, 59.400001525878906, 63.5, 61.099998474121094, 62.150001525878906, 60.650001525878906, 63.5, 61.20000076293945, 61.849998474121094, 66.0, 61.04999923706055, 65.0, 67.0, 69.0, 71.19999694824219, 72.4000015258789, 76.0, 78.80000305175781, 75.0999984741211, 74.0, 77.80000305175781, 75.3499984741211, 69.69999694824219, 65.9000015258789, 67.5, 67.1500015258789, 68.0, 67.80000305175781, 69.4000015258789, 76.0, 78.9000015258789, 81.3499984741211, 78.30000305175781, 76.4000015258789, 76.0, 76.94999694824219, 79.0, 77.0, 77.25, 76.94999694824219, 74.4000015258789, 77.19999694824219, 78.75, 76.94999694824219, 77.0, 77.19999694824219, 77.5, 78.05000305175781, 85.0, 84.44999694824219, 86.05000305175781, 85.0, 85.0999984741211, 90.0, 91.94999694824219, 90.5999984741211, 88.30000305175781, 81.55000305175781, 84.0, 85.5, 83.5999984741211, 83.0, 85.5, 86.0999984741211, 85.5, 86.25, 82.25, 83.0999984741211, 83.94999694824219, 83.5, 79.0, 81.3499984741211, 82.44999694824219, 83.05000305175781, 83.75, 82.0, 81.5999984741211, 80.0, 79.19999694824219, 78.5, 78.0, 76.4000015258789, 76.30000305175781, 81.0, 80.5999984741211, 82.75, 82.5, 82.0999984741211, 81.4000015258789, 81.0, 79.80000305175781, 80.0, 77.94999694824219, 78.3499984741211, 76.30000305175781, 76.5, 77.8499984741211, 78.1500015258789, 79.5, 79.55000305175781, 78.44999694824219, 78.55000305175781, 78.69999694824219, 77.5, 79.75, 80.8499984741211, 79.0, 82.94999694824219, 81.30000305175781, 81.4000015258789, 81.19999694824219, 81.80000305175781, 80.3499984741211, 81.0, 82.75, 81.1500015258789, 80.5, 78.55000305175781, 78.19999694824219, 76.55000305175781, 73.94999694824219, 73.5, 73.44999694824219, 73.8499984741211, 73.75, 74.25, 73.05000305175781, 73.80000305175781, 74.0, 71.0, 69.69999694824219, 66.05000305175781, 68.0, 69.44999694824219, 72.5, 69.4000015258789, 67.5, 69.0, 70.0, 69.30000305175781, 70.9000015258789, 69.0, 70.30000305175781, 69.0999984741211, 68.5, 67.0, 66.6500015258789, 67.3499984741211, 69.5, 68.0, 67.4000015258789, 69.19999694824219, 69.5, 68.0999984741211, 67.25, 66.05000305175781, 65.0, 65.4000015258789, 66.0, 66.30000305175781, 66.9000015258789, 67.69999694824219, 68.6500015258789, 69.05000305175781, 71.44999694824219, 72.5, 71.19999694824219, 72.69999694824219, 71.6500015258789, 71.5, 72.44999694824219, 72.0, 76.75, 77.19999694824219, 81.1500015258789, 80.80000305175781, 78.5, 81.5, 85.4000015258789, 89.75, 91.1500015258789, 93.0, 91.5, 91.0, 98.25, 102.5, 101.9000015258789, 105.0999984741211, 101.5, 98.9000015258789, 89.05000305175781, 90.5, 92.5, 94.0, 94.19999694824219, 93.5, 93.30000305175781, 93.75, 94.05000305175781, 96.5, 98.9000015258789, 98.0, 98.94999694824219, 101.5, 102.0, 104.94999694824219, 107.0, 105.25, 101.4000015258789, 97.75, 99.0, 99.05000305175781, 94.5999984741211, 93.05000305175781, 91.4000015258789, 89.0, 90.75, 89.0, 92.5, 94.5999984741211, 94.25, 98.94999694824219, 99.5, 99.80000305175781, 102.0, 100.0, 98.9000015258789, 97.0, 99.25, 102.0, 110.69999694824219, 105.9000015258789], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"outlier_flag=Outlier<br>Close=%{x}<br>Open=%{marker.size}\", \"legendgroup\": \"outlier_flag=Outlier\", \"marker\": {\"color\": \"red\", \"size\": [119.44999694824219, 124.4000015258789, 61.599998474121094, 63.04999923706055, 72.5999984741211, 92.5999984741211, 103.69999694824219], \"sizemode\": \"area\", \"sizeref\": 0.44625, \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"outlier_flag=Outlier\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [125.1500015258789, 133.60000610351562, 68.1500015258789, 72.3499984741211, 79.9000015258789, 96.80000305175781, 110.69999694824219], \"xaxis\": \"x\", \"y\": [119.44999694824219, 124.4000015258789, 61.599998474121094, 63.04999923706055, 72.5999984741211, 92.5999984741211, 103.69999694824219], \"yaxis\": \"y\"}],\n",
              "                        {\"height\": 600, \"legend\": {\"itemsizing\": \"constant\", \"tracegroupgap\": 0, \"traceorder\": \"normal\", \"x\": 0, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Scatter Plot of `Open` VS `Close` with marker size w.r.t. the value of `Open`\"}, \"width\": 900, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Close\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Open\"}}},\n",
              "                        {\"displaylogo\": false, \"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4efe0b0c-bddd-4855-b684-ea8835d6b992');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0yoI8lYywN"
      },
      "source": [
        "### Using Mahalanobis Distance\n",
        "\n",
        "__Mahalanobis distance__ ($D^2$) is an effective multivariate distance metric that measures the distance between a point (vector) and a distribution.\n",
        "\n",
        "_Euclidean distance_ is the commonly used straight line distance between two points. If the two points, say (p1, q1) and (p2, q2), are in a 2D plane, then the Euclidean distance is:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "D_e = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\n",
        "\\end{gather*}$$\n",
        "\n",
        "This formula can be extended to many dimensions, as long as the dimensions are _equally weighted and are independent of each other._ It does not consider how the rest of the points in the dataset vary. So, it cannot really be used to judge how close a point actually is to the distribution of points. Hence, we need Mahalanobis distance for multivariate outlier detection.\n",
        "\n",
        "The formula to compute Mahalanobis distance is as follows:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "D^2 = \\frac{(x-m)^T.(x-m)}{C}\n",
        "\\end{gather*}$$\n",
        "\n",
        "where\n",
        " - $D^2$ is the square of the Mahalanobis distance\n",
        " - $x$ is the vector of the observation (row in a dataset)\n",
        " - $m$ is the vector of mean values of independent variables (mean of each column)\n",
        " - $C$ is the covariance matrix of independent variables (_Refresher_ : Covariance represents the direction of the relationship between two variables [+ or - or 0], so it shows the strength of how one variable is related to changes of the other)\n",
        " \n",
        "$(x – m)$ is essentially the distance of the vector from the mean. We then divide this by the covariance matrix $C$. This is essentially a multivariate equivalent of the regular standardization ($z = \\frac{(x – \\mu)}{\\sigma}$). If the variables in your dataset are strongly correlated, then the covariance will be high, which will effectively reduce the distance. So effectively, it addresses both the problems of scale as well as the correlation of the variables.\n",
        "\n",
        "In order to detect the outliers, we should specify the threshold; we do so by multiplying the mean of the Mahalanobis distance results by the extremeness degree `k` in which __k = 2.0 * std__ for _extreme values_ and __3.0 * std__ for the _very extreme values_ , according to the __68–95–99.7 rule__.\n",
        "\n",
        "![](https://github.com/ml-nest/EDA/blob/master/Source/68-95-99.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8u557xNYywN"
      },
      "source": [
        "Before calculating the Mahalanobis distance, we need to rescale (numerical column) and one-hot encode (categorical column) the data. Then, we need to check if data is singular (i.e. determinant is zero). This can be checked and handled using __Variance Inflation Factor (VIF)__ which will help us to drop the columns causing singularity.\n",
        "\n",
        "VIF estimates how much the variance of a coefficient is _\"inflated\"_ because of __linear dependence with other predictors__. Thus, a __VIF of 1.4__ tells us that the variance of a particular coefficient is __40% larger__ than it would be if that predictor was completely uncorrelated with all the other predictors.\n",
        "\n",
        "The VIF has a lower bound of 1 but the upper bound varies depending on the problem statement. In order to learn more about the threshold, check this [link](https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/).\n",
        "\n",
        "In order to know at what scenarios a high VIF is not a problem and can be safely ignored, check this [link](https://statisticalhorizons.com/multicollinearity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:34:54.501889Z",
          "start_time": "2020-12-14T14:34:52.677250Z"
        },
        "code_folding": [
          4,
          45
        ],
        "id": "T1jeDeHWYywN"
      },
      "source": [
        "# displaying the outlier rows from the data frame\n",
        "value=\"mahalanobis outlier\"\n",
        "\n",
        "# function to calculate VIF score for multi-colinearity\n",
        "def get_vif(vif_df, truncate_value):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    vif_df         : dataframe, (nobs, k_vars)\n",
        "                     design matrix with all explanatory variables, as for example used in\n",
        "                     regression.\n",
        "    truncate_value : int/float number\n",
        "                     Every value greater than this will be capped by this value\n",
        "    Returns\n",
        "    -------\n",
        "    vif : Series\n",
        "          variance inflation factors\n",
        "    '''\n",
        "\n",
        "    vif_df = add_constant(vif_df, has_constant='add')\n",
        "    collinearity_check = True\n",
        "\n",
        "    vif_drop_col = []\n",
        "    while collinearity_check:\n",
        "        vifs = pd.Series(\n",
        "            [1 / (1. - OLS(vif_df[col].values, \n",
        "                           vif_df.loc[:, vif_df.columns != col].values).fit().rsquared) for col in vif_df],\n",
        "            index=vif_df.columns, name='VIF')\n",
        "\n",
        "        vifs = pd.DataFrame(vifs)\n",
        "        vifs.drop('const', axis=0, inplace=True)\n",
        "        vif_drop = vifs['VIF'].idxmax()\n",
        "        if vifs['VIF'].max() > truncate_value:\n",
        "            vif_drop_col.append(vif_drop)\n",
        "            vif_df.drop(vif_drop,axis=1,inplace=True)\n",
        "        else:\n",
        "            collinearity_check=False\n",
        "\n",
        "    vif_drop_df = pd.DataFrame(truncate_value, columns = ['VIF'], index=vif_drop_col)\n",
        "    vif_collated = pd.concat([vifs,vif_drop_df])\n",
        "    vif_collated = vif_collated.sort_values(by=['VIF'],ascending=True)\n",
        "\n",
        "    return vif_collated\n",
        "\n",
        "# function to calculate the Mahalanobis distance\n",
        "def calculate_mahalanobis(x=None, data=None, cov=None):\n",
        "    '''\n",
        "    Compute the Mahalanobis Distance between each row of x and the data.\n",
        "    \n",
        "    input:\n",
        "        x    : vector or matrix of data with, say, p columns.\n",
        "        data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
        "        cov  : covariance matrix (p x p) of the distribution. If 'None', will be computed from data.\n",
        "    return:\n",
        "        mahal.diagonal() : calculated Mahalanobis Distance\n",
        "    '''\n",
        "    \n",
        "    # distance of the vector from the mean (x - m)\n",
        "    dist_vec = x - np.mean(data)\n",
        "    \n",
        "    # estimate the covariance matrix of the data\n",
        "    if not cov:\n",
        "        cov = np.cov(data.T)\n",
        "        \n",
        "    # inverse covariance matrix of independent variables (C^(-1))\n",
        "    inv_covmat = np.linalg.inv(cov)\n",
        "    \n",
        "    # dot product of the distance vector with C^(-1)\n",
        "    dot_transp_inv_cov = np.dot(dist_vec, inv_covmat)\n",
        "    \n",
        "    # multiplying the above result with the transpose of the distance vector\n",
        "    mahalanobis = np.dot(dot_transp_inv_cov, dist_vec.T)\n",
        "    \n",
        "    return mahalanobis.diagonal()\n",
        "\n",
        "try:\n",
        "    # create a copy of the dataframe and rescale it\n",
        "    df_maha_matrix = df.copy()\n",
        "    \n",
        "    # removing categorical columns having too many categories to avoid memory crashing/ singularity\n",
        "    df_desc_cat = df.describe(include=[np.object]).T\n",
        "    df_desc_cat['columns'] = [x for x in df_desc_cat.index]\n",
        "    df_desc_cat = df_desc_cat.reset_index()\n",
        "    \n",
        "    cat_cols_to_drop = []\n",
        "    num_of_catgrs = []\n",
        "    for idx,each_uniq in enumerate(df_desc_cat['unique'].to_list()):\n",
        "        if each_uniq > 30:\n",
        "            num_of_catgrs.append(each_uniq)\n",
        "            cat_cols_to_drop.append(df_desc_cat.loc[idx,'columns'])\n",
        "    \n",
        "    if cat_cols_to_drop == []:\n",
        "        final_cat_cols = cat_cols.copy()\n",
        "    else:\n",
        "        if __name__ == '__main__':\n",
        "            display(Markdown('You data has __`{}`__ categorical columns having more than 30 unique categories.'.format(cat_cols_to_drop)))\n",
        "            display(Markdown('If you don\\'t drop them, then you will end up with __{} more columns__ due to one-hot encoding process.'.format(sum(num_of_catgrs))))\n",
        "            display(Markdown('__NOTE__: This might cause `MemoryError` or will take a longer time to process. Also, this will affect the result of this operation.'))\n",
        "\n",
        "            while True:\n",
        "                select_ = input('\\nDo you want to DROP these columns and proceed? (Y/N): ')\n",
        "                if select_.lower() == 'y':\n",
        "                    final_cat_cols = [x for x in cat_cols if x not in cat_cols_to_drop]\n",
        "                    break\n",
        "                else:\n",
        "                    final_cat_cols = cat_cols.copy()\n",
        "                    break\n",
        "        else:\n",
        "            final_cat_cols = cat_cols.copy()\n",
        "\n",
        "    cols_to_be_used = num_cols.copy()\n",
        "    cols_to_be_used.extend(final_cat_cols)\n",
        "\n",
        "    df_maha_matrix = df_maha_matrix[cols_to_be_used]\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
        "    \n",
        "    if cat_cols == []:\n",
        "        pass\n",
        "    else:\n",
        "        df_maha_matrix = pd.get_dummies(df_maha_matrix, prefix=final_cat_cols)\n",
        "\n",
        "    if num_cols == []:\n",
        "        pass\n",
        "    else:\n",
        "        df_maha_matrix[num_cols] = StandardScaler().fit_transform(df_maha_matrix[num_cols])\n",
        "\n",
        "    col_to_drop_for_md = []\n",
        "\n",
        "    vif_scores = get_vif(df_maha_matrix,5)\n",
        "\n",
        "    linear_dependent = vif_scores[vif_scores['VIF']>=5]\n",
        "\n",
        "    if len(linear_dependent)>0:\n",
        "        col_to_drop_for_md.extend(linear_dependent.index)\n",
        "        if __name__ == '__main__':\n",
        "            clear_output()\n",
        "        display(Markdown('__Linearly Dependent Columns dropped are:__'))\n",
        "        print('{}'.format(' || '.join(col_to_drop_for_md)))\n",
        "\n",
        "        df_maha_cols = df_maha_matrix.columns\n",
        "\n",
        "        # columns on which factor analysis wiil be performed\n",
        "        col_to_retain_for_fa = set(df_maha_cols) - set(col_to_drop_for_md)\n",
        "\n",
        "        # display updated list of column names\n",
        "        df_maha_matrix.drop(col_to_drop_for_md, axis=1, inplace=True)\n",
        "        updated_df_maha_cols_vis = ' || '.join(col_to_retain_for_fa)\n",
        "        print(colored(\"\\nColumns Retained:\",'blue',attrs=['bold']),\"\\nCount: {}\\n{}\".format(len(col_to_retain_for_fa), updated_df_maha_cols_vis))\n",
        "    else:\n",
        "        display(Markdown(\"<span style='color:red'>No Linearly Dependent Columns in the dataframe!\"))\n",
        "\n",
        "    # calculate the mahalanobis distance\n",
        "    mahalanobis_dist = calculate_mahalanobis(x=df_maha_matrix, data=df_maha_matrix)\n",
        "    mahalanobis_dist = np.around(mahalanobis_dist, decimals=4)\n",
        "    \n",
        "    multivar_outlier_df = df.copy()\n",
        "    # storing the mahalanobis distance under the column 'mahalanobis_dist'\n",
        "    multivar_outlier_df['mahalanobis_dist'] = mahalanobis_dist\n",
        "    multivar_outlier_df.insert(loc=0, column='index', value=multivar_outlier_df.index)\n",
        "\n",
        "    # calculate extremeness degree with 99.7% threshold value\n",
        "    mahalanobis_dist_std = np.std(mahalanobis_dist)\n",
        "\n",
        "    # obtain the mean of the 'mahalanobis_dist' column\n",
        "    mahalanobis_dist_mean = np.mean(mahalanobis_dist)\n",
        "    \n",
        "    # degree of extremeness\n",
        "    k = (3*mahalanobis_dist_std)\n",
        "\n",
        "    # calculate the upper and lower threshold value\n",
        "    upper_thres = mahalanobis_dist_mean + k\n",
        "    lower_thres = mahalanobis_dist_mean - k\n",
        "    \n",
        "    multivar_outlier_df_final = multivar_outlier_df[(multivar_outlier_df['mahalanobis_dist'] < lower_thres) | (multivar_outlier_df['mahalanobis_dist'] > upper_thres)]\n",
        "\n",
        "    display(Markdown('__Total rows with outliers:__ {}'.format(multivar_outlier_df_final.shape[0])))\n",
        "    display(Markdown('Displaying the dataframe having outliers with Mahalanobis distance __< `{}` or > `{}`__:'.format(lower_thres.round(4),upper_thres.round(4))))\n",
        "    display_data(multivar_outlier_df_final.round(4))\n",
        "    \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    clear_output()\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLE9qsBAYywO"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8XrLPGzYywO"
      },
      "source": [
        "***\n",
        "\n",
        "# Variable Analysis\n",
        "\n",
        "## Univariate Analysis\n",
        "\n",
        "_Univariate analysis looks at one feature at a time._ When we analyze a feature (a.k.a. column) independently, we are usually most interested in the distribution of its values and ignore other features in the data-set.\n",
        "\n",
        "In this section we have explored, the various descriptive and quantile statistics associated with each variable, for both numerical and categorical. After performing data imputation, we can obtain univariate analysis as follow:\n",
        "\n",
        "### Numerical Features \n",
        "\n",
        "First, we will obtain the descriptive and numerical statistics, and then take a look at the plot(s).\n",
        "\n",
        "__Histogram Plot__ : A histogram groups values into bins of equal value range. The shape of the histogram may contain clues about the underlying distribution type: gaussian, exponential, etc. You can also spot skewness in its shape when the distribution is nearly regular but has some anomalies.\n",
        "\n",
        "__Density Plot__ : There is also another, often clearer, way to grasp the distribution - density plots or, more formally, _Density Plots_. They can be considered as smoothed version of the histogram. Their main advantage over the histogram is that they do not depend on the size of the bins.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-26T13:40:50.038524Z",
          "start_time": "2020-02-26T13:40:50.035301Z"
        },
        "id": "HjdgpV5gYywO"
      },
      "source": [
        "#### Numerical Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:35:04.858643Z",
          "start_time": "2020-12-14T14:35:04.726281Z"
        },
        "id": "PAf4sJEmYywP"
      },
      "source": [
        "value = \"Statistical Summary of numerical columns\"\n",
        "\n",
        "try:\n",
        "    # descriptive and numerical statistics of numerical column(s)\n",
        "\n",
        "    if num_cols == []:\n",
        "        display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
        "    else:\n",
        "        num_desc = df[num_cols].describe().T\n",
        "        num_desc.insert(loc=5, column='IQR', value=(num_desc['75%']-num_desc['25%']))\n",
        "        num_desc.drop(['25%','50%','75%'], axis=1 ,inplace = True)\n",
        "\n",
        "        num_desc['skewness'] = df[num_cols].skew()\n",
        "        num_desc['kurtosis'] = df[num_cols].kurt()\n",
        "        num_desc.insert(loc=0, column='columns', value=num_desc.index)\n",
        "        num_desc.insert(loc=3, column='median', value=df[num_cols].median())    \n",
        "        display_data(num_desc.round(4))\n",
        "    \n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    clear_output()\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyXKGVWTYywP"
      },
      "source": [
        " #### Histogram plots for numerical columns\n",
        "A __Histogram__ groups values into bins of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential, etc. It can help us to spot any skewness in its shape when the distribution is nearly regular but has some anomalies.\n",
        "\n",
        "There is also another, often clearer, way to grasp the distribution - __Density Plots__ or, more formally, __Kernel Density Plots__. They can be considered a smoothed version of the histogram. Their main advantage over the histogram is that they do not depend on the size of the bins."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:35:07.859158Z",
          "start_time": "2020-12-14T14:35:06.772048Z"
        },
        "id": "TE42oudpYywP"
      },
      "source": [
        "# histogram plots on the numerical column(s)\n",
        "\n",
        "value=\"Plotting Numeric Cols\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        if num_cols == []:\n",
        "            display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
        "\n",
        "        # for multiple numerical columns\n",
        "        else:\n",
        "            # initializing the tabs\n",
        "            fig = go.Figure()\n",
        "            selected_cols = num_cols\n",
        "            \n",
        "            for each_col in selected_cols:\n",
        "                vis = True if each_col == num_cols[0] else False\n",
        "                fig.add_trace(go.Histogram(x=df[each_col], opacity=0.5, histnorm='probability density',\n",
        "                                          visible=vis))\n",
        "\n",
        "            tab_dict_list = []\n",
        "            for each_col in selected_cols:\n",
        "                vis_check = [[True] if i==each_col else [False] for i in selected_cols]\n",
        "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
        "                tab_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
        "                                                  {\"title\": \"Histogram plots for: {}\".format(each_col)}],\n",
        "                                          label=each_col, method=\"update\"))\n",
        "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
        "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
        "                                  showlegend=False, title_x=0.5)\n",
        "\n",
        "            clear_output()\n",
        "            fig.update_yaxes(title_text= 'KDE')\n",
        "            display(Markdown('__Histogram Plots for the numerical columns are generated!__'))\n",
        "            fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        # display the error\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3cOvkApYywP"
      },
      "source": [
        "### Categorical Features\n",
        "\n",
        "#### Categorical Statistics\n",
        "\n",
        "First, we will obtain descriptive statistics, and then take a look at the plot(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:35:09.238647Z",
          "start_time": "2020-12-14T14:35:09.201629Z"
        },
        "code_folding": [
          4
        ],
        "id": "NZWy8lqRYywQ"
      },
      "source": [
        "# descriptive statistics of categorical column(s)\n",
        "value = \"Statistical Summary of categorical columns\"\n",
        "\n",
        "# function to calculate data summary on categorical columns\n",
        "def categorical_summary(inp_df,categorical_columns):\n",
        "    \"\"\"\n",
        "    Function for returning statistical summary of categorical columns\n",
        "    \n",
        "    input:\n",
        "    inp_df : Input Dataframe\n",
        "    categorical_columns : List of categorical columns in the dataframe\n",
        "    \n",
        "    return:\n",
        "    Statistical summary of categorical columns in dataframe\n",
        "    \"\"\"\n",
        "    cat_summ = pd.DataFrame()\n",
        "    for col in categorical_columns:\n",
        "        cat_col_summary = pd.DataFrame(inp_df[col].value_counts(dropna=False)).reset_index()\n",
        "        cat_col_summary.columns = ['Category','Frequency']\n",
        "        cat_col_summary['Percentage(%)'] = (cat_col_summary[\"Frequency\"]/cat_col_summary[\"Frequency\"].sum())*100\n",
        "        cat_col_summary['Variable'] = col\n",
        "        cat_summ = cat_summ.append(cat_col_summary)\n",
        "        \n",
        "    return cat_summ\n",
        "\n",
        "try:\n",
        "    cat_summ = categorical_summary(df,cat_cols)\n",
        "    display_data(cat_summ[['Variable','Category','Frequency','Percentage(%)']].round(4))\n",
        "    \n",
        "except Exception as err:\n",
        "    # display the error\n",
        "    clear_output()\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nZ2orpGYywQ"
      },
      "source": [
        "#### Frequency plots for categorical columns\n",
        "Here, we will get a __frequency table__, which shows how frequent each value of the categorical variable is, and using a __bar plot__, we can visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:35:11.144461Z",
          "start_time": "2020-12-14T14:35:10.679273Z"
        },
        "code_folding": [
          4
        ],
        "id": "gqzdnQ8OYywQ"
      },
      "source": [
        "# generating the plots (if any)\n",
        "value=\"Plotting Cat Cols\"\n",
        "\n",
        "# frequency bar plot of categories in categorical column(s)\n",
        "def cat_feature_viz(data, categorical_column):\n",
        "    '''\n",
        "    Function to visualize the counts of categories in categorical variables. \n",
        "    If there are more than 20 categories in a variable, we calculate the \n",
        "    mean of value count. Then, any categories whose value count is less than \n",
        "    the mean value is kept in a category 'Other'.\n",
        "    input:\n",
        "        data               : data frame\n",
        "        categotical_column : list of categorical columns\n",
        "    return:\n",
        "        None. It displays a table of value count of categories and a horizontal \n",
        "        bar plot in the console output for each categorical column(s).\n",
        "    '''\n",
        "    \n",
        "    # getting the value count against each category (key) and storing it as key:value pair\n",
        "    cat_value_pair = dict(data[categorical_column].value_counts().items())\n",
        "    \n",
        "    # list of category names and their corresponding values\n",
        "    cat_value_pair_keys = list(cat_value_pair.keys())\n",
        "    cat_value_pair_values = list(cat_value_pair.values())\n",
        "    \n",
        "    # map the keys under the `col` column and value count under `count` column\n",
        "    cat_dict = {categorical_column:cat_value_pair_keys, 'count':cat_value_pair_values}\n",
        "    \n",
        "    # create the dataframe\n",
        "    cat_value_df = pd.DataFrame(cat_dict).set_index(categorical_column)\n",
        "    cat_value_df.reset_index(inplace=True)\n",
        "    cat_value_df.columns = [categorical_column,'count']\n",
        "    print(cat_value_df)\n",
        "    return cat_value_df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        if cat_cols == []:\n",
        "            display(Markdown('__NO CATEGORICAL COLUMNS AVAILABLE!__'))\n",
        "            script, div = None, None\n",
        "\n",
        "        # for multiple categorical columns\n",
        "        else:\n",
        "            selected_cols = cat_cols\n",
        "            \n",
        "            fig = go.Figure()\n",
        "\n",
        "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
        "            \n",
        "            ignored_cols = []\n",
        "            for c in selected_cols:\n",
        "                vis = True if c == selected_cols[0] else False\n",
        "                if df[c].nunique()>30:\n",
        "                    ignored_cols.append(c)\n",
        "                    vis = False\n",
        "                else:\n",
        "                    cat_value_df = cat_feature_viz(df, c)\n",
        "                    fig.add_trace(go.Bar(x=cat_value_df[c], y=cat_value_df['count'],width=0.5, visible=vis))\n",
        "            \n",
        "            tab_dict_list = []\n",
        "            for each_col in selected_cols:\n",
        "                vis_check = [[True] if i==each_col else [False] for i in selected_cols]\n",
        "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
        "                tab_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
        "                                                  {\"title\": \"Frequency plots for: {}\".format(each_col)}],\n",
        "                                          label=each_col, method=\"update\"))\n",
        "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
        "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
        "                                  showlegend=False, title_x=0.5)\n",
        "            \n",
        "            fig.update_xaxes(tickangle=45)\n",
        "            clear_output()\n",
        "            display(Markdown('__Plots generated!__'))\n",
        "            if ignored_cols == []:\n",
        "                pass\n",
        "            else:\n",
        "                display(Markdown('Plots of the following columns are NOT generated for having __too many categories__!'))\n",
        "                display(Markown('__Columns__ : {}'.format(ignored_cols)))\n",
        "            \n",
        "            fig.update_yaxes(title='Frequency')\n",
        "            fig.show(config={'displaylogo':False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        # display the error\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "            \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMALUdkYywQ"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pMslxRQYywR"
      },
      "source": [
        "## Bivariate Analysis <a name='bivar'></a>\n",
        "\n",
        "The bivariate analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a _pre-defined significance level_. There are 3 possible types of bivariate analysis:\n",
        "\n",
        "* __Continuous & Continuous__   : both are continuous numerical variables\n",
        "* __Categorical & Categorical__ : both are categorical variables\n",
        "* __Continuous & Categorical__  : one is a continuous numerical variable and other is a categorical variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_grMXrYYywR"
      },
      "source": [
        "### Continuous & Continuous <a name='bivar_cont-cont'></a>\n",
        "\n",
        "To analyze the relationship __strength__ between 2 continuous variables, we need to look at its __correlation__ $r$:\n",
        "* +1 -> perfect positive linear correlation\n",
        "* -1 -> perfect negative linear correlation\n",
        "* 0  -> no correlation\n",
        "\n",
        "$$\\begin{gather*}\n",
        "r = \\frac {cov(x,y)} {\\sqrt{var(x).var(y)}}\\\\\n",
        "where \\ \\ cov(x,y)=\\frac {\\sum{(x-\\bar{x})}{(y-\\bar{y})}} {n} \\ , \\ var(x)=\\frac {\\sum{(x-\\bar{x})^2}}{n} \n",
        "\\end{gather*}$$\n",
        "\n",
        "First, we are plotting the __correlation matrix__ as well as the __correlation network__ to understand which variables are highly correlated.\n",
        "\n",
        "Then, based on the degree of correlation, we can narrow down to our required set of highly correlated numerical columns and generate a __scatter plot__, whose pattern indicates the __type of relationship__ (linear or non-linear) between variables. \n",
        "\n",
        "#### Correlation Matrix <a name='corr_matrix'></a>\n",
        "\n",
        "We will plot a correlation matrix by masking the upper triangle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:35:14.711788Z",
          "start_time": "2020-12-14T14:35:13.753298Z"
        },
        "id": "yUYyWzkcYywR"
      },
      "source": [
        "# generating the correlation matrix\n",
        "value=\"Correlation Matrix\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        if num_cols == []:\n",
        "            display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
        "\n",
        "        # if only numerical column is available\n",
        "        elif len(num_cols) == 1:\n",
        "            print(colored('CAN\\'T generate Correlation Matrix','red'), \n",
        "                  colored('because there is only one numeric column.','magenta'))\n",
        "\n",
        "        # if multiple numerical columns are available\n",
        "        else:\n",
        "\n",
        "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plot</h2></div>'))\n",
        "\n",
        "            # compute the correlation matrix\n",
        "            df_corr = df[num_cols].corr().round(3)\n",
        "\n",
        "            # generate a mask for the upper triangle\n",
        "            mask_corr = np.array(df_corr)\n",
        "            mask_corr[np.triu_indices_from(mask_corr)] = None\n",
        "\n",
        "            heat = go.Heatmap(z=mask_corr, x=num_cols, y=num_cols, \n",
        "                              xgap=1, ygap=1, colorscale='RdYlGn')\n",
        "\n",
        "            clear_output()\n",
        "            display(Markdown('__Plot generated!__'))\n",
        "            layout = go.Layout(title_text='Correlation Matrix', title_x=0.5, \n",
        "                               width=900, height=900,\n",
        "                               template='none',\n",
        "                               xaxis_showgrid=False, yaxis_showgrid=False, yaxis_autorange='reversed')\n",
        "   \n",
        "            fig=go.Figure(data=[heat], layout=layout)\n",
        "            fig.update_xaxes(tickangle=45, tickfont=dict(size=8))\n",
        "            fig.update_yaxes(tickangle=45, tickfont=dict(size=8))\n",
        "    \n",
        "            fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        # display the error\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err) \n",
        "        \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDHIlVn_YywS"
      },
      "source": [
        "#### Correlation Network\n",
        "\n",
        "This plot helps us to find the positively or negatively correlated variables via edges and its __strength based on the thickness__. In this plot, the user needs to pass 4 arguments in the function `create_corr_network`:\n",
        "* __`data`__ : The dataframe in which the correlation has to be performed\n",
        "* __`corr_type`__ : It will be either `pos` or `neg`\n",
        "* __`corr_thresh`__ : The threshold value of correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:36:53.791400Z",
          "start_time": "2020-12-14T14:36:49.761855Z"
        },
        "code_folding": [
          4
        ],
        "scrolled": false,
        "id": "TJDQ-KbqYywS"
      },
      "source": [
        "# generating the correlation network\n",
        "value=\"Correlation Network\"\n",
        "\n",
        "# function to create correlation network\n",
        "def corr_network(data, corr_type, corr_thresh=None):\n",
        "    '''\n",
        "    Function to create a correlation network that highlights which variables are highly correlated \n",
        "    and degree of the node i.e. how many edges are linked to the node.\n",
        "    \n",
        "    input: \n",
        "        data          : dataframe\n",
        "        corr_type     : correlation type (positive or negative)\n",
        "        corr_thresh   : The threshold value of correlation. If None, then no filtering will be done while \n",
        "                        dispalying the plot, else it will display connections above the threshold value\n",
        "    return: \n",
        "        None. It displays correlation network plot in the console output.\n",
        "    '''\n",
        "    # create the correlation matrix\n",
        "    data_corr = data.corr()\n",
        "\n",
        "    # extracts the indices from the correlation matrix, which are the stocks\n",
        "    stocks = data_corr.index.values\n",
        "\n",
        "    # changes from dataframe to matrix, so it is easier to create a graph with networkx\n",
        "    data_corr = np.asmatrix(data_corr)\n",
        "\n",
        "    # creates graph using the data of the correlation matrix\n",
        "    G = nx.from_numpy_matrix(data_corr)\n",
        "\n",
        "    # relabels the nodes to match the stocks names\n",
        "    G = nx.relabel_nodes(G,lambda x: stocks[x])\n",
        "    H = G.copy()\n",
        "\n",
        "    # default edge color and node edge color if corr_type is positive\n",
        "    edge_colors = plt.cm.Greys\n",
        "    node_edge_color = 'b'\n",
        "\n",
        "    # checks all the edges and removes some based on corr_direction\n",
        "    for stock1, stock2, weight in G.edges(data=True):\n",
        "        # if we only want to see the positive correlations we then delete the edges with weight smaller than 0\n",
        "        if corr_type == 'pos':\n",
        "            edge_colors = plt.cm.Greens\n",
        "            node_edge_color = 'g'\n",
        "            if corr_thresh == None:\n",
        "                if weight[\"weight\"] < 0:\n",
        "                    H.remove_edge(stock1, stock2)\n",
        "            else:\n",
        "                if weight[\"weight\"] < 0 or weight[\"weight\"] < corr_thresh:\n",
        "                    H.remove_edge(stock1, stock2)\n",
        "\n",
        "        # this part runs if the corr_type is negative and removes edges with weights equal or largen than 0\n",
        "        else:\n",
        "            edge_colors = plt.cm.Reds\n",
        "            node_edge_color = 'r'\n",
        "            if corr_thresh == None:\n",
        "                if weight[\"weight\"] >= 0:\n",
        "                    H.remove_edge(stock1, stock2)\n",
        "            else:\n",
        "                if weight[\"weight\"] >= 0 or weight[\"weight\"] > corr_thresh:\n",
        "                    H.remove_edge(stock1, stock2)\n",
        "\n",
        "    # crates a list for edges and for the weights\n",
        "    edges, weights = zip(*nx.get_edge_attributes(H,'weight').items())\n",
        "\n",
        "    # increases the value of weights, so that they are more visible in the graph\n",
        "    weights = tuple([(1+abs(x))**3 for x in weights])\n",
        "\n",
        "    # calculates the degree of each node\n",
        "    d = list(nx.degree(H))\n",
        "\n",
        "    # creates list of nodes and a list their degrees that will be used later for their sizes\n",
        "    node_list = [key for (key,val) in d]\n",
        "\n",
        "    # positions\n",
        "#     positions = nx.fruchterman_reingold_layout(H)\n",
        "    positions = nx.spring_layout(H)\n",
        "\n",
        "    # figure size\n",
        "    plt.figure(figsize=(25,25))\n",
        "\n",
        "    # draws nodes\n",
        "    nx.draw_networkx_nodes(H, positions, node_color='#d3d3d3', nodelist=node_list,\n",
        "                           alpha=0.5).set_edgecolor(node_edge_color)\n",
        "    # draws the edges\n",
        "    nx.draw_networkx_edges(H, positions, edge_list=edges, style='solid', \n",
        "                           width=weights, edge_color = weights,\n",
        "                           edge_cmap = edge_colors,\n",
        "                           edge_vmin = min(weights), edge_vmax=max(weights))\n",
        "    # styling for labels\n",
        "    label_desc = nx.draw_networkx_labels(H, positions, font_size = 12, font_family='sans-serif')\n",
        "\n",
        "    angles = {}\n",
        "    origin_ = cmath.phase(complex(0,0))\n",
        "    for each_label, each_coord in positions.items():\n",
        "        x_ = each_coord.tolist()[0]\n",
        "        y_ = each_coord.tolist()[1]\n",
        "        label_loc = cmath.phase(complex(x_,y_))\n",
        "        label_angle = (label_loc - origin_) * 180 / cmath.pi\n",
        "        angles[each_label] = label_angle\n",
        "\n",
        "    for node, t in label_desc.items():\n",
        "        t.set_rotation(angles[node])\n",
        "\n",
        "    # displays the graph without axis\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    if num_cols == []:\n",
        "        display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
        "\n",
        "    elif len(num_cols) == 1:\n",
        "        print(colored('CAN\\'T generate Correlation Network','red'), \n",
        "              colored('because there is only one numeric column.','magenta'))\n",
        "    else:\n",
        "        try:\n",
        "            # ask the user for the correlation theshold\n",
        "            display(Markdown('Do you want to filter based on a __threshold value of correlation?__ '))\n",
        "            allow_thresh = input('Write Y or N: ')\n",
        "            if allow_thresh.lower() == 'y':\n",
        "                df_corr = df.corr().stack().reset_index()\n",
        "                df_corr.columns = ['col_1','col_2','corr_val']\n",
        "\n",
        "                df_corr.where(df_corr[\"col_1\"] != df_corr[\"col_2\"], inplace = True) \n",
        "                df_corr.dropna(inplace=True)\n",
        "\n",
        "                while True:\n",
        "                    display(Markdown('Please enter a value between **0** and __{}__ for `pos` correlation plot'.format(df_corr['corr_val'].max())))\n",
        "                    corr_thresh_p = float(input('Enter the threshold: '))\n",
        "\n",
        "                    if corr_thresh_p > df_corr['corr_val'].max():\n",
        "                        print(colored('INCORRECT RANGE! Please try again.','red',attrs=['bold']))\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "                        \n",
        "                while True:\n",
        "                    display(Markdown('Please enter a value between **0** and __{}__ for `neg` correlation plot'.format(df_corr['corr_val'].min())))\n",
        "                    corr_thresh_n = float(input('Enter the threshold: '))\n",
        "                    \n",
        "                    if corr_thresh_n < df_corr['corr_val'].min():\n",
        "                        print(colored('INCORRECT RANGE! Please try again.','red',attrs=['bold']))\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            else:\n",
        "                corr_thresh_p, corr_thresh_n = None, None\n",
        "\n",
        "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plot</h2></div>'))\n",
        "\n",
        "            clear_output()\n",
        "            display(Markdown('__NOTE__ : _Please re-run the code snippet in case you get overlapping nodes._'))\n",
        "            \n",
        "            display(Markdown('__Positive Correlation Netowrk__'))\n",
        "            print('_'*100)\n",
        "            corr_network(df[num_cols], 'pos', corr_thresh_p)\n",
        "            \n",
        "            display(Markdown('__Negative Correlation Netowrk__'))\n",
        "            print('_'*100)\n",
        "            corr_network(df[num_cols], 'neg', corr_thresh_n)\n",
        "            \n",
        "            track_cell(value, flag)\n",
        "        except Exception as err:\n",
        "            print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "            flag = 0\n",
        "            err = str(err)\n",
        "            track_cell(value, flag, err)\n",
        "            \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ams46jvYywS"
      },
      "source": [
        "Now that the correlation matrix and correlation network are obtained, we can display __highly correlated columns__ which will be paired to create the scatter plots.\n",
        "\n",
        "#### Scatter Plot\n",
        "\n",
        "Generates a scatter plot of highly correlated variable pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:37:06.092229Z",
          "start_time": "2020-12-14T14:37:02.315247Z"
        },
        "id": "s08wsa-1YywS"
      },
      "source": [
        "# generate pairs with the best correlation, both positive and negative\n",
        "value=\"Scatter Plots with best correlation\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # user enters the threshold\n",
        "    display(Markdown('Provide a __threshold to select the range of correlation value__.'))\n",
        "    display(Markdown('We will take this threshold percentage of the maximum and minimum correlated values for the specific data.'))\n",
        "    display(Markdown('For example, if _threshold_ is __0.9__ and _max_ and _min_ of correlation matrix are __0.9__ and __-0.8__ respectively,\\\n",
        "     then the range of correlation value obtained will be __=> (0.9*0.9=) 0.81__ and __<= (-0.8*0.9=) -0.72__.'))\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            corr_thresh = float(input('Threshold: '))\n",
        "\n",
        "            # check if the value entered lies in the range\n",
        "            corr_thresh_range = [round(i,2) for i in list(np.arange(0.1, 1.01, 0.01))]\n",
        "\n",
        "            if corr_thresh not in corr_thresh_range:\n",
        "                # if not, keep on asking to enter\n",
        "                print(colored(\"\\nIncorrect range! Please enter properly.\",\"red\",attrs=['bold']))\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # correlation matrix\n",
        "        df_corr = df.corr().stack().reset_index()\n",
        "        df_corr.columns = ['col_1','col_2','correlation_val']\n",
        "\n",
        "        # dropping identical columns pairs and missing values\n",
        "        df_corr.where(df_corr[\"col_1\"] != df_corr[\"col_2\"], inplace = True) \n",
        "        df_corr.dropna(inplace=True)\n",
        "\n",
        "        # filtering values greater than the threshold\n",
        "        pos_limit = round(corr_thresh * df_corr['correlation_val'].max(), 3)\n",
        "        neg_limit = round(corr_thresh * df_corr['correlation_val'].min(), 3)\n",
        "        pos_range = df_corr['correlation_val'] >= pos_limit\n",
        "        neg_range = df_corr['correlation_val'] <= neg_limit\n",
        "        df_selected = df_corr[pos_range | neg_range]\n",
        "\n",
        "        # filtering out lower/upper triangular duplicates \n",
        "        df_selected['ordered_cols'] = df_selected.apply(lambda x: '-'.join(sorted([x['col_1'],x['col_2']]))\n",
        "                                                        ,axis=1)\n",
        "\n",
        "        df_selected.drop_duplicates(['ordered_cols'], inplace=True)\n",
        "        df_selected.drop(['ordered_cols'], axis=1, inplace=True)\n",
        "\n",
        "        clear_output()\n",
        "        display(Markdown('The correlation values __greater than equal to {}__ and\\\n",
        "        __less than equal to {}__ are selected.'.format(pos_limit, neg_limit)))\n",
        "\n",
        "        # generating the unqiue pairs in a list\n",
        "        selected_pairs = []\n",
        "        for i in range(len(df_selected)):\n",
        "            selected_pairs.append(df_selected.iloc[i,0:2].tolist())\n",
        "\n",
        "        display(Markdown('_The pairs selected are as follows:_'))\n",
        "        print(selected_pairs)\n",
        "\n",
        "        track_cell(value, flag)\n",
        "\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "\n",
        "    display(Markdown('<span style=\"color:darkgreen; font-size: 15px\">Please proceed for generating the <b>scatter plots</b>.</span>'))\n",
        "\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:37:12.951369Z",
          "start_time": "2020-12-14T14:37:11.738670Z"
        },
        "code_folding": [
          5
        ],
        "id": "ONz3sNqHYywT"
      },
      "source": [
        "# generating the scatter plots\n",
        "value=\"Generating Scatter Plots\"\n",
        "\n",
        "\n",
        "# function to generate \n",
        "def gen_scatter_plot(x_col, y_col,visible):\n",
        "    '''\n",
        "    Function to generate the scatter plot along with the estimator.\n",
        "    '''\n",
        "    global fig\n",
        "    \n",
        "    # determine non-parametric smoothed estimates of x and y\n",
        "    smoothed_estimate = lowess_method(df[y_col], df[x_col])\n",
        "    smoothed_estimated_x = smoothed_estimate[...,0]\n",
        "    smoothed_estimated_y = smoothed_estimate[...,1]\n",
        "\n",
        "    df_scatter = df.copy()\n",
        "\n",
        "    x = df[x_col].to_numpy()\n",
        "    y = df[y_col].to_numpy()\n",
        "    \n",
        "    # parameters and covariance from of the fit of 1-D polynom\n",
        "    p, cov = np.polyfit(x, y, 1, cov=True)\n",
        "    # model using the fit parameters; NOTE: parameters here are coefficients\n",
        "    y_model = np.polyval(p, x)\n",
        "\n",
        "    # number of observations\n",
        "    n = df_scatter.shape[0]\n",
        "    # number of parameters\n",
        "    m = p.size\n",
        "    dof = n - m\n",
        "    t_stat = stats.t.ppf(confidence, dof)\n",
        "\n",
        "    # estimates of Error in Data/Model\n",
        "    resid = y - y_model\n",
        "    # standard deviation of the error\n",
        "    s_err = np.sqrt(np.sum(resid**2) / dof)\n",
        "\n",
        "    x_ = smoothed_estimated_x\n",
        "    y_ = smoothed_estimated_y\n",
        "\n",
        "    # obtaining the confidence interval\n",
        "    ci = t_stat * s_err * np.sqrt(1/n + (x_ - np.mean(x))**2 / np.sum((x - np.mean(x))**2))\n",
        "    \n",
        "    # calculating the upper and lower limit\n",
        "    df_scatter['lower'] = y_ - ci\n",
        "    df_scatter['upper'] = y_ + ci\n",
        "    df_scatter['x_'] = x_\n",
        "    df_scatter['y_'] = y_\n",
        "\n",
        "    # scatter plot between x and y\n",
        "    fig.add_trace(go.Scatter(x=df[x_col], y=df[y_col],\n",
        "                             mode = 'markers',marker=dict(color='teal',size = 10,opacity  = 0.6),\n",
        "                             showlegend = False, visible=visible))\n",
        "    \n",
        "    # plotting the upper confidence interval\n",
        "    fig.add_trace(go.Scatter(x=df_scatter['x_'], y=df_scatter['upper'], line_color='yellow',\n",
        "                             opacity=0.9, showlegend=False, visible=visible))\n",
        "    \n",
        "    # plotting the lower confidence interval\n",
        "    fig.add_trace(go.Scatter(x=df_scatter['x_'], y=df_scatter['lower'],\n",
        "                             fill='tonexty', fillcolor='rgba(255,255,0,0.7)', line_color='yellow',\n",
        "                             opacity = 0.9, showlegend=False, visible = visible))\n",
        "    \n",
        "    # plotting the estimator\n",
        "    fig.add_trace(go.Scatter(x=df_scatter['x_'], y=df_scatter['y_'], mode='lines',\n",
        "                             name='Estimator', marker=dict(color='purple'),\n",
        "                             showlegend = False, visible = visible))\n",
        "    \n",
        "    return fig\n",
        "\n",
        "confidence = 0.95\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        lowess_method = sm.nonparametric.lowess\n",
        "\n",
        "        if num_cols == []:\n",
        "            display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
        "            script, div = None, None\n",
        "\n",
        "        elif len(num_cols) == 1:\n",
        "            print(colored('CAN\\'T perform scatter plotting','red'), \n",
        "                  colored('because there is only one numeric column.','magenta'))\n",
        "            script, div = None, None\n",
        "        else:\n",
        "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plot</h2></div>'))\n",
        "\n",
        "            # if there is only one numerical column, then no need to use tab\n",
        "            fig = go.Figure()\n",
        "            if len(selected_pairs) == 1:\n",
        "                x_col = selected_pairs[0][0]\n",
        "                y_col = selected_pairs[0][1]\n",
        "\n",
        "                fig = gen_scatter_plot(x_col, y_col,True)\n",
        "                fig.update_layout(title = \"Scatter plot of {} vs {}\".format(x_col,y_col), title_x=0.5)\n",
        "                clear_output()\n",
        "            else:\n",
        "                for i,each_tab in enumerate(selected_pairs):\n",
        "                    visible =  True if each_tab == selected_pairs[0] else False  \n",
        "                    x_col = selected_pairs[i][0]\n",
        "                    y_col = selected_pairs[i][1]\n",
        "\n",
        "                    fig = gen_scatter_plot(x_col, y_col,visible)\n",
        "\n",
        "                clear_output()\n",
        "                # performing the operation on panel level\n",
        "                panel_dict_list = []\n",
        "\n",
        "                selected_pairs= [\"{} vs {}\".format(i[0],i[1]) for i in selected_pairs]\n",
        "                # creating the drop-down option\n",
        "\n",
        "                for i, each_panel in enumerate(selected_pairs):\n",
        "                    vis_check = [[True]*4 if j==each_panel else [False]*4 for j in selected_pairs]\n",
        "                    vis_check_flat = [item for sublist in vis_check for item in sublist]\n",
        "\n",
        "                    x_name = each_panel.split(' vs ')[0]\n",
        "                    y_name = each_panel.split(' vs ')[1]\n",
        "                    title_final = 'X: {}, Y: {}'.format(x_name, y_name)\n",
        "                    panel_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
        "                                                        {\"title\":\"Scatter plot of {}\".format(title_final),\n",
        "                                                         \"xaxis\": {\"title\":\"{}\".format(x_name)},\n",
        "                                                         \"yaxis\": {\"title\":\"{}\".format(y_name)}}],\n",
        "                                                label=each_panel, method=\"update\"))\n",
        "\n",
        "                    fig.update_layout(updatemenus=[dict(buttons=list(panel_dict_list),\n",
        "                                                        direction=\"right\",\n",
        "                                                        x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")], title_x=0.5)\n",
        "\n",
        "            fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        # display the error\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tczMlujQYywT"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2rHlxznYywT"
      },
      "source": [
        "### Categorical & Categorical <a name='bivar_cat-cat'></a>\n",
        "\n",
        "Here, we will perform the __Chi-Squared test__. Its Null Hypothesis ($H_0$) states that 2 categorical variables are __independent__, which can be measured from its __p-value__. Then we will plot the __Stacked Bar Chart__ to compare the proportion of each category for one categorical variable across the categories of the second categorical variable.\n",
        "\n",
        "If the __p-value is significantly high, then we fail to accept $H_{0}$__ and claim that the __variables are dependent__. Then we need to perform a __post-hoc__ test, __Tchouproff Contingency Coefficient__ (better known as __Cramer's V__), which measures the amount of dependency between two dependent categorical variables using the formula:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "\\rho_{\\tau} = \\sqrt {\\frac {\\chi^{2}} {n \\sqrt{df}}}\\\\\n",
        "where \\ \\ \\chi^2=\\sum_{i=1}^r\\sum_{j=1}^c \\frac {(o_{ij}-e_{ij})^2} {e_{ij}} \\ , \\ e_{ij}=\\frac {o_{i}.o_{j}}{o} \\ , \\ df=(r-1)(c-1)\n",
        "\\end{gather*}$$\n",
        "\n",
        "Here, $o_{i}$ is the sum of the observed value in the $i^{th}$ row, $df$ is the degree of freedom and, $r$ and $c$ represents the number of rows and columns respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pht8SZknYywU"
      },
      "source": [
        "#### Chi Square Test with Stacked Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:37:16.214035Z",
          "start_time": "2020-12-14T14:37:15.687560Z"
        },
        "code_folding": [
          4
        ],
        "id": "8DoJzIsoYywU"
      },
      "source": [
        "# generate the result of the chi-square test\n",
        "value=\"Chi Square Stacked Bar Charts\"\n",
        "\n",
        "# function to create chi-square test and display the stacked bar plot\n",
        "def chi_sqr_analysis(data, col_1, col_2,visible):\n",
        "    '''\n",
        "    Function to perform Chi-Square Test and plot stacked bar chart indicating the \n",
        "    how the categories of one variable are distributed across the categories of another variable.\n",
        "    \n",
        "    input:\n",
        "        data  : data frame\n",
        "        col_1 : name of the 1st column\n",
        "        col_2 : name of the 2nd column\n",
        "    return:\n",
        "        None : stacked bar plot generated\n",
        "    '''\n",
        "    global fig\n",
        "    # create the Contingency table\n",
        "    contigency_table = pd.crosstab(index = data[col_1], columns = data[col_2])\n",
        "\n",
        "    # perform the chi-square test\n",
        "    stat, p, dof, expected = stats.chi2_contingency(contigency_table)\n",
        "\n",
        "    p_stacked_bar = None\n",
        "\n",
        "    # check if Null Hypothesis is getting accepted\n",
        "    if p < 0.05:\n",
        "        # plot the chart\n",
        "        cat_prop = df.groupby(col_1)[col_2].value_counts(normalize=True).unstack()\n",
        "        cat_prop.fillna(0, inplace=True)\n",
        "        \n",
        "        chi_sqr_df = pd.DataFrame(columns=['Test Statistic','Degree of Freedom','p-value','Variables are Independent?','$H_0$'],\n",
        "                                  data = [[stat,dof,p,'YES','Accept']])\n",
        "    \n",
        "        cat_prop = df.groupby(col_2)[col_1]\n",
        "        y_col_vals = df[col_2].unique().tolist()\n",
        "        x_col_vals = df[col_1].unique().tolist()\n",
        "        cat_prop.fillna(0, inplace=True)\n",
        "\n",
        "        for i in range(0,len(cat_prop)):\n",
        "            fig.add_trace(go.Bar(name= y_col_vals[i],  \n",
        "                 x=df.groupby(col_2)[col_1].get_group(y_col_vals[i]).value_counts(normalize=True).index,\n",
        "                 y=df.groupby(col_2)[col_1].get_group(y_col_vals[i]).value_counts(normalize=True).tolist(),\n",
        "                                visible=visible))\n",
        "\n",
        "        # Change the bar mode\n",
        "        fig.update_layout(barmode='stack',barnorm = \"percent\")\n",
        "\n",
        "        chi_sqr_table = DataTable(columns=[TableColumn(field=Ci, title=Ci) for Ci in chi_sqr_df.columns],\n",
        "                                 source=ColumnDataSource(chi_sqr_df),\n",
        "                                 height=100)\n",
        "        return chi_sqr_df, fig\n",
        "\n",
        "    else:\n",
        "        # interpret the Cramer's V to analyse the degree of dependency (dod)\n",
        "        dependent_cat_cols.append([col_1,col_2])\n",
        "        \n",
        "        chi_sqr_df = pd.DataFrame(columns=['Test Statistic','Degree of Freedom','p-value','Variables are Independent?','$H_0$'], \n",
        "                                  data = [[stat,dof,p,'NO','Fail to Accept']])\n",
        "        chi_sqr_table = DataTable(columns=[TableColumn(field=Ci, title=Ci) for Ci in chi_sqr_df.columns],\n",
        "                                 source=ColumnDataSource(chi_sqr_df),\n",
        "                                 height=100)\n",
        "        \n",
        "        source = ColumnDataSource(dict(x=[0], y=[0], text=['The variables are DEPENDENT since p-value > 0.05 !']))\n",
        "        display(Markdown('<span style=\"color:darkgreen; font-size: 15px\">The variables are DEPENDENT since p-value > 0.05 !</span>'))\n",
        "        \n",
        "        return chi_sqr_df, fig\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        if cat_cols == []:\n",
        "            display(Markdown('__NO CATEGORICAL COLUMNS AVAILABLE!__'))\n",
        "\n",
        "        elif len(cat_cols) == 1:\n",
        "            print(colored('CAN\\'T generate Grouped Column Chart','red'), \n",
        "                  colored('because there is only one categorical column.','magenta'))\n",
        "        else:\n",
        "            dependent_cat_cols = []\n",
        "            selected_cat_col_pair = list(combinations(cat_cols,2))\n",
        "            fig = go.Figure()\n",
        "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
        "            \n",
        "            # display the result\n",
        "            if len(selected_cat_col_pair) == 1:\n",
        "                \n",
        "                cat_col_1 = selected_cat_col_pair[0][0]\n",
        "                cat_col_2 = selected_cat_col_pair[0][1]\n",
        "                chi_sqr_table, fig =  chi_sqr_analysis(df, cat_col_1, cat_col_2,True)\n",
        "                fig.update_layout(title = \"Stacked bar chart of categories of `{}` across categories of `{}`\".format(cat_col_2,cat_col_1),\n",
        "                                 title_x=0.5, legend_orientation=\"h\", height = 800)\n",
        "                fig.update_xaxes(title_text=cat_col_1)\n",
        "                fig.update_yaxes(title_text= cat_col_2) \n",
        "                clear_output()\n",
        "                display_data(chi_sqr_table.round(4))\n",
        "            \n",
        "            else:\n",
        "                # initializing the tabs for the graphs\n",
        "                stckd_bar_plt_pairs = [Panel() for i in range(len(selected_cat_col_pair))]\n",
        "                \n",
        "                # for each cat col pair, start processing in a tab\n",
        "                for i in range(len(selected_cat_col_pair)):\n",
        "                    cat_col_1 = selected_cat_col_pair[i][0]\n",
        "                    cat_col_2 = selected_cat_col_pair[i][1]\n",
        "                    visible = True if i==0 else False\n",
        "                    chi_sqr_table, fig = chi_sqr_analysis(df, cat_col_1, cat_col_2,visible)\n",
        "                \n",
        "                panel_dict_list = []\n",
        "                \n",
        "                clear_output()\n",
        "                \n",
        "                selected_panels_list = selected_cat_col_pair.copy()\n",
        "\n",
        "                selected_panels= [\"{} vs {}\".format(i[0],i[1]) for i in selected_panels_list]\n",
        "                # creating the drop-down option\n",
        "                for i, each_panel in enumerate(selected_panels):\n",
        "                    vis_check = [[True]*len(df[selected_panels_list[i][1]].unique()) if each_of_panel==each_panel else [False]*len(df[selected_panels_list[i][1]].unique()) for i,each_of_panel in enumerate(selected_panels)]\n",
        "                    vis_check_flat = [item for sublist in vis_check for item in sublist]\n",
        "\n",
        "                    panel_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
        "                                                        {\"title\": \"Stacked bar chart of categories of `{}` across categories of `{}`\".format(selected_panels_list[i][1],selected_panels_list[i][0])}],\n",
        "                                           label = each_panel, method=\"update\"))\n",
        "\n",
        "                    fig.update_layout(updatemenus=[dict(buttons=list(panel_dict_list),                                              \n",
        "                                                direction=\"right\",\n",
        "                                                x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
        "                                     title_x=0.5,legend_orientation=\"h\")\n",
        "            \n",
        "            fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        # display the error\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJFHrT-YywU"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msv00CbYywU"
      },
      "source": [
        "### Continuous & Categorical <a name='bivar_cont-cat'></a> \n",
        "\n",
        "#### ANOVA Test\n",
        "\n",
        "__The One-Way (ANalysis Of VAriance) ANOVA test__ assesses whether the means of a numerical variable for two or more categories of a categorical variable are statistically different from each other. The _Null Hypothesis_ ($H_0$) in ANOVA is that the __means are equal__.\n",
        "\n",
        "The test statistic for ANOVA is __F-Score__ which is calculated as follows:\n",
        "* First, we need to calculate __sums of squares (SS)__ which is a measure of how much variance is explained. The _between treatment_ SS ($SS_B$) is computed by summing the squared differences __between each observation (or category) mean__ ($\\bar{X_j}$) and the __overall mean__ ($\\bar{X}$). The squared differences are weighted by the sample sizes per group ($n_j$). The _error_ or _residual_ SS ($SS_E$ or $SS_R$) is computed by summing the squared differences __between each observation__ ($X$) and __its group mean__ ($\\bar{X_j}$). The double summation ($\\sum\\sum$) indicates the summation of the squared differences within each treatment and then the summation of the totals across treatments to produce a single value. The formula is:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "SS_B = \\sum {n_j} {(\\bar{X_j} - \\bar{X})}^2\n",
        "\\end{gather*}$$\n",
        "\n",
        "$$\\begin{gather*}\n",
        "SS_E = \\sum\\sum {(X - \\bar{X_j})}^2\n",
        "\\end{gather*}$$\n",
        "\n",
        "* Then, we calculate the __degrees of freedom (DOF)__. The _between treatment_ DOF is $dof_B$ = `k-1`. The _error_ DOF is $dof_E$ = `N-k`. Here, `k` is the number of treatments or independent comparison groups, and `N` is the total number of observations.\n",
        "* Then, using the above two measures, we need to calculate the __mean square (MS)__, which is computed by dividing SS by DOF. So, we get:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "MSB = \\frac {SS_B} {dof_B}\n",
        "\\end{gather*}$$\n",
        "\n",
        "$$\\begin{gather*}\n",
        "MSE = \\frac {SS_E} {dof_E}\n",
        "\\end{gather*}$$\n",
        "\n",
        "* Finally, we can calculate F-Score by dividing `MSB` by `MSE`, and hence we get:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "F = \\frac {MSB} {MSE}\n",
        "\\end{gather*}$$\n",
        "\n",
        "If __F-Score is significantly higher than the critical value__ (obtained from a table of probability values for the F-distribution with DOF $dof_B$ = `k-1`, $dof_E$ = `N-k`), then there is a __significant difference in the group means__, and hence we can __fail to accept $H_0$__.\n",
        "\n",
        "There are 3 assumptions that need to be met for the results of an ANOVA test to be considered accurate and trustworthy. It’s important to note the assumptions apply to the residuals and not the variables themselves. The ANOVA assumptions are the same as for linear regression and are:\n",
        "* Normality\n",
        "* Homogeneity of variance\n",
        "* Independent observations\n",
        "\n",
        "If these assumptions are not met, and one does not want to transform the data, an alternative test that could be used is the __Kruskal-Wallis H-test__ or __Welch’s ANOVA__.\n",
        "\n",
        "Here, we need to look at the __scatter plot with error bars__. Each data point is the average of the numerical data for the corresponding categories of a categorical variable with an error bar showing standard error. The __box plot__ allows us to understand the distribution of the data across the categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:38:06.348257Z",
          "start_time": "2020-12-14T14:37:18.512700Z"
        },
        "code_folding": [
          4
        ],
        "scrolled": false,
        "id": "Sa_bMfmaYywV"
      },
      "source": [
        "# performing the ANOVA test\n",
        "value=\"Anova Test\"\n",
        "\n",
        "# function to modify the in-built output of ANOVA table\n",
        "def anova_table(aov):\n",
        "    '''\n",
        "    Function to add mean square error and Model Effect Size in the existing ANOVA table\n",
        "\n",
        "    input:\n",
        "        aov : Generated ANOVA table\n",
        "    return:\n",
        "        aov_mod : Modified ANOVA table\n",
        "    '''\n",
        "    # calculate the mean square\n",
        "    aov['mean_sq'] = aov[:]['sum_sq']/aov[:]['df']\n",
        "\n",
        "    # calculate the model effect size\n",
        "    aov['$\\eta^2$'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
        "\n",
        "    # calculate the unbiased model effect size\n",
        "    aov['$\\omega^2$'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*aov['mean_sq'][-1]))/(sum(aov['sum_sq'])+aov['mean_sq'][-1])\n",
        "\n",
        "    col_names = ['sum_sq', 'df', 'mean_sq', 'F', 'PR(>F)', '$\\eta^2$', '$\\omega^2$']\n",
        "    aov_mod = aov[col_names]\n",
        "\n",
        "    return aov_mod\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # displaying the numerical and categorical columns\n",
        "        print(colored(\"\\nNumerical Columns:\",'magenta',attrs=['bold']),\"\\n{}\".format(num_cols_vis))\n",
        "        print(colored(\"\\nCategorical Columns:\",'blue',attrs=['bold']),\"\\n{}\".format(cat_cols_vis))\n",
        "\n",
        "        if cat_cols == [] or num_cols == []:\n",
        "            display(Markdown('__CANNOT PERFORM THIS OPERATION! No numerical or categorical columns.__'))\n",
        "        else:\n",
        "            while True:\n",
        "                num_col_in = input('\\nEnter your numerical column: ')\n",
        "                if num_col_in in num_cols:\n",
        "                    break\n",
        "                else:\n",
        "                    print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "                    continue\n",
        "\n",
        "            while True:\n",
        "                cat_col_in = input('\\nEnter your categorical column: ')\n",
        "                if cat_col_in in cat_cols:\n",
        "                    break\n",
        "                else:\n",
        "                    print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "                    continue\n",
        "\n",
        "            # to generate ANOVA table, we need to perform ordinary least squares (ols) method \n",
        "            # and then the anova_lm method\n",
        "            \n",
        "            df_temp = df.copy()\n",
        "            if cat_col_in == 'class':\n",
        "                df_temp = df_temp.rename(columns={'class': 'class_'})\n",
        "                cat_col_in = 'class_'\n",
        "            \n",
        "            mod = ols('{} ~ {}'.format(num_col_in, cat_col_in), data=df_temp).fit()\n",
        "            aov_table = anova_lm(mod, typ=2)\n",
        "            # displaying the modified ANOVA table\n",
        "            clear_output()\n",
        "            display(Markdown('#### ANOVA Table'))\n",
        "            display_data(anova_table(aov_table).round(4))\n",
        "                \n",
        "            err_mean = []\n",
        "            err_std = []\n",
        "            for cat in list(df_temp[cat_col_in].unique()):\n",
        "                df_sub = df_temp[df_temp[cat_col_in] == cat][num_col_in]\n",
        "                n = len(df_sub)\n",
        "                err_mean.append(np.mean(df_sub))\n",
        "                err_std.append(np.std(df_sub)/pow(n,0.5))\n",
        "            \n",
        "            fig = go.Figure(data=go.Scatter(x=df_temp[cat_col_in].unique(),\n",
        "                                            y=err_mean, mode='markers', \n",
        "#                                             marker=dict(color=list(range(len(df_temp[cat_col_in].unique())))),\n",
        "                                            error_y=dict(type='data', array=err_std, visible=True)))\n",
        "            fig.update_layout(title='Error Bar Plot of `{}` and `{}`'.format(cat_col_in, num_col_in),\n",
        "                              title_x=0.5)\n",
        "            fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag=0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "            \n",
        "else:\n",
        "    num_col_in = 'occupancy'\n",
        "    cat_col_in = 'location_type'\n",
        "    mod = ols('{} ~ {}'.format(num_col_in, cat_col_in), data=df).fit()\n",
        "    aov_table = anova_lm(mod, typ=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o9U2trSYywV"
      },
      "source": [
        "Let’s break down this ANOVA table.\n",
        "* _Rows_\n",
        "    * The __model__ ($1^{st}$) row is the between groups (categories) effect which is the overall experimental effect\n",
        "    * The `residual` row is the unsystematic variation in the data\n",
        "* _Columns_\n",
        "    * `sum_sq` is the sum of squares ($SS_B$ and $SS_R$ - explained above).\n",
        "    * `df` is the degree of freedom.\n",
        "    * `mean_sq` is the mean squares ($MS_B$ and $MS_R$ - explained above), which is also used to calculate the $\\omega^2$ squared using `df`.\n",
        "    * `F` is the F-Score (explained above).\n",
        "    * `PR(>F)` is the p-value associated with the F statistic of a given effect and test statistic.\n",
        "    * $\\eta^2$ and $\\omega^2$ are the __model effect size__, which tells us how much of an impact the experiment will have in the real world. $\\omega^2$ is considered a better measure because of its unbiased calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MHnDe8uYywV"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYGHwZvIYywW"
      },
      "source": [
        "### Trellis Plot - Bivariate plot with a conditional variable <a name='bivar_any'></a>\n",
        "\n",
        "__Trellis plot__ displays the relationship between two variables, conditioned on the values taken by one variable. \n",
        "* For __categorical conditioning variable__, this means carrying out the same plot for the data subsets corresponding to __each category__ of that variable. \n",
        "* For __numeric conditioning variable__, this means carrying out the same plot for the data subsets corresponding to __quantiles__ of that variable.\n",
        "\n",
        "The table below will give a better understanding of the possible combinations and the expected outcome:\n",
        "\n",
        "| X | Y | Conditional | Components in subplot | Type of plot |\n",
        "|:-:|:-:|:----:|:---------------------:|:------------:|\n",
        "| continuous | continuous | continuous | 4 quantiles | scatter plot\n",
        "| continuous | continuous | categorical | categories | scatter plot\n",
        "| continuous | categorical | continuous | 4 quantiles | scatter plot\n",
        "| continuous | categorical | categorical | categories | scatter plot\n",
        "| categorical | continuous | continuous | 4 quantiles | scatter plot\n",
        "| categorical | continuous | categorical | categories | scatter plot\n",
        "| categorical | categorical | continuous | 4 quantiles | stacked bar plot\n",
        "| categorical | categorical | categorical | categories | stacked bar plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:38:54.904913Z",
          "start_time": "2020-12-14T14:38:13.441113Z"
        },
        "id": "CpDHLaSIYywW"
      },
      "source": [
        "# performing data preprocessing for trellis plot\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # display list of numerical column names for ease\n",
        "    num_cols_vis = ' || '.join(num_cols)\n",
        "    print(colored(\"Numerical Columns:\",'magenta',attrs=['bold']),\"\\n{}\".format(num_cols_vis))\n",
        "\n",
        "    # display list of categorical column names for ease\n",
        "    cat_cols_vis = ' || '.join(cat_cols)\n",
        "    print(colored(\"\\nCategorical Columns:\",'blue',attrs=['bold']),\"\\n{}\".format(cat_cols_vis))\n",
        "\n",
        "    while True:\n",
        "        x_var_col = input('\\nEnter X variable: ')\n",
        "        if x_var_col in num_cols or x_var_col in cat_cols:\n",
        "            break\n",
        "        else:\n",
        "            print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "            continue\n",
        "\n",
        "    while True:\n",
        "        y_var_col = input('\\nEnter Y variable: ')\n",
        "        if y_var_col in num_cols or y_var_col in cat_cols:\n",
        "            break\n",
        "        else:\n",
        "            print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "            continue\n",
        "\n",
        "    while True:\n",
        "        cond_var_col = input('\\nEnter conditional column: ')\n",
        "        if cond_var_col in num_cols or cond_var_col in cat_cols:\n",
        "            break\n",
        "        else:\n",
        "            print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "            continue\n",
        "\n",
        "    df_trellis = df.copy()\n",
        "    le_mapping_dict = {}\n",
        "\n",
        "    if cat_cols == [] or (x_var_col in cat_cols and y_var_col in cat_cols):\n",
        "        pass\n",
        "    else:\n",
        "        # perform label encoding to the categorical columns\n",
        "        label_encoder = LabelEncoder()\n",
        "\n",
        "        for i in cat_cols:\n",
        "            label_encoder.fit(df_trellis[i])\n",
        "            df_trellis[i] = label_encoder.transform(df_trellis[i])\n",
        "            # mapping the categories with the label encoded value\n",
        "            le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),\n",
        "                                       label_encoder.classes_))\n",
        "            le_mapping_dict[i] = le_name_mapping\n",
        "\n",
        "    display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for generating <b>Trellis Plot</b>.</span>'))\n",
        "    \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:38:57.653706Z",
          "start_time": "2020-12-14T14:38:57.014895Z"
        },
        "code_folding": [
          6,
          68,
          148
        ],
        "scrolled": false,
        "id": "7F1Hak-uYywW"
      },
      "source": [
        "# generating the trellis plot\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "value=\"Trellis Plot Generation\"\n",
        "\n",
        "#Function to generate scatter or stacked bar chart for categorical conditional variables\n",
        "def trellis_cat_cond():\n",
        "    '''\n",
        "    Function to generate scatter or stacked bar chart for categorical conditional variables.\n",
        "    \n",
        "    input:\n",
        "        NA\n",
        "    return:\n",
        "        Grid plots of scatter or stacked bar chart\n",
        "    '''\n",
        "    #### cont,cont,cat/#### cat, cat, cat\n",
        "    trellis_plots = []\n",
        "    cond_var_categories = df_trellis[cond_var_col].unique()\n",
        "    titles = []\n",
        "    for each_cat in cond_var_categories:\n",
        "\n",
        "        titles.append('{}'.format(le_mapping_dict[cond_var_col][each_cat]))   \n",
        "    nrows = len(titles)//2+1 if len(titles)%2!=0 else int(len(titles)/2)\n",
        "    fig = make_subplots(rows=nrows, cols=2, subplot_titles=titles)\n",
        "\n",
        "    r, c = list(range(1,(len(titles)//2+2))), list(range(1,3))\n",
        "    rc_pair = list(product(r, c))\n",
        "    x_col_vals = df_trellis[x_var_col].unique().tolist()\n",
        "    \n",
        "    if x_var_col in cat_cols and y_var_col in cat_cols:\n",
        "        for p_id, each_cat in enumerate(cond_var_categories):\n",
        "            fig_r = rc_pair[p_id][0]\n",
        "            fig_c = rc_pair[p_id][1]\n",
        "            df_trellis_filtered = df_trellis[df_trellis[cond_var_col] == each_cat]\n",
        "            df_trellis_filtered = df_trellis_filtered[[x_var_col, y_var_col]]\n",
        "            df_trellis_grpd = df_trellis_filtered.groupby(x_var_col)[y_var_col]\n",
        "            x_col_vals = df_trellis_filtered[x_var_col].unique().tolist()\n",
        "            y_col_vals = df_trellis_filtered[y_var_col].unique().tolist()\n",
        "            df_trellis_grpd.fillna(0, inplace=True)\n",
        "            for i in range(0,len(df_trellis_grpd)):\n",
        "                    fig.add_trace(go.Bar(name= x_col_vals[i],  \n",
        "                     x=df_trellis_filtered.groupby(x_var_col)[y_var_col].get_group(x_col_vals[i]).value_counts(normalize=True).index,\n",
        "                     y=df_trellis_filtered.groupby(x_var_col)[y_var_col].get_group(x_col_vals[i]).value_counts(normalize=True).tolist()\n",
        "                                        ,showlegend=False)\n",
        "                                 ,row=fig_r, col=fig_c)\n",
        "\n",
        "        # Change the bar mode\n",
        "            fig.update_layout(barmode='stack',barnorm = \"percent\")\n",
        "        fig.update_xaxes(title = x_var_col)     \n",
        "        fig.update_yaxes(title = y_var_col)  \n",
        "        fig.update_layout(width=1000, height=1000*len(titles)//2+1, showlegend=False)\n",
        "        fig.show(config={'displayLogo':False})\n",
        "    else:\n",
        "        # generating the plots for every numerical columns\n",
        "        for p_id, each_cat in enumerate(cond_var_categories):\n",
        "            df_trellis_filtered = df_trellis[df_trellis[cond_var_col] == each_cat]\n",
        "            fig_r = rc_pair[p_id][0]\n",
        "            fig_c = rc_pair[p_id][1]\n",
        "            fig.add_trace(go.Scatter(x=df_trellis_filtered[x_var_col], y=df_trellis_filtered[y_var_col],\n",
        "                             mode = 'markers',   marker=dict(color='teal',size = 10,opacity  = 0.6),\n",
        "                                    showlegend  = False), \n",
        "                          row=fig_r, col=fig_c)\n",
        "        fig.update_xaxes(title = x_var_col)     \n",
        "        fig.update_yaxes(title = y_var_col) \n",
        "        fig.update_layout(width=1000, height=500*len(titles)//2+1, showlegend=False)\n",
        "        fig.show(config={'displayLogo':False})\n",
        "\n",
        "#Function to generate scatter or stacked bar chart for continuous conditional variables\n",
        "def trellis_quan_cond():\n",
        "    '''\n",
        "    Function to generate scatter or stacked bar chart for continuous conditional variables.\n",
        "    \n",
        "    input:\n",
        "        NA\n",
        "    return:\n",
        "        Grid plots of scatter or stacked bar chart\n",
        "    '''\n",
        "    #### cont,cont,cont/ #### cont-cat, cont / #### cat,cat,cont\n",
        "    q1 = df_trellis[cond_var_col].quantile(0.25)\n",
        "    q2 = df_trellis[cond_var_col].quantile(0.5)\n",
        "    q3 = df_trellis[cond_var_col].quantile(0.75)\n",
        "    q4 = df_trellis[cond_var_col].quantile(1)\n",
        "\n",
        "    # tag the values falling the respective quantiles by creating a new column\n",
        "    quantile_col = []\n",
        "    for i in df_trellis.index.values.tolist():\n",
        "        if df_trellis.loc[i,cond_var_col] <= q1:\n",
        "            quantile_col.append('Q1')\n",
        "        elif df_trellis.loc[i,cond_var_col] > q1 and df.loc[i,cond_var_col] <= q2:\n",
        "            quantile_col.append('Q2')\n",
        "        elif df_trellis.loc[i,cond_var_col] > q2 and df.loc[i,cond_var_col] <= q3:\n",
        "            quantile_col.append('Q3')\n",
        "        elif df_trellis.loc[i,cond_var_col] > q3 and df.loc[i,cond_var_col] <= q4:\n",
        "            quantile_col.append('Q4')\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # add the values in the new column\n",
        "    df_for_quantile = df_trellis.copy()\n",
        "    df_for_quantile['quantile'] = quantile_col\n",
        "\n",
        "    cond_var_categories = ['Q1','Q2','Q3','Q4']\n",
        "    trellis_plots = []\n",
        "    titles = []\n",
        "    for each_cat in cond_var_categories:\n",
        "\n",
        "        titles.append('{}'.format(each_cat))   \n",
        "\n",
        "    fig = make_subplots(rows=2, cols=2, subplot_titles=titles)\n",
        "\n",
        "    r, c = list(range(1,3)), list(range(1,3))\n",
        "    rc_pair = list(product(r, c))\n",
        "    x_col_vals = df_trellis[x_var_col].unique().tolist()\n",
        "    trellis_plots = []\n",
        "\n",
        "    for p_id, each_cat in enumerate(cond_var_categories):\n",
        "        fig_r = rc_pair[p_id][0]\n",
        "        fig_c = rc_pair[p_id][1]\n",
        "        df_trellis_filtered = df_for_quantile[df_for_quantile['quantile'] == each_cat]\n",
        "        \n",
        "        if x_var_col in cat_cols and y_var_col in cat_cols:\n",
        "            df_trellis_filtered = df_trellis_filtered[[x_var_col, y_var_col]]\n",
        "            df_trellis_grpd = df_trellis_filtered.groupby(y_var_col)[x_var_col]\n",
        "            x_col_vals = df_trellis_filtered[x_var_col].unique().tolist()\n",
        "            y_col_vals = df_trellis_filtered[y_var_col].unique().tolist()\n",
        "            df_trellis_grpd.fillna(0, inplace=True)\n",
        "\n",
        "            for i in range(0,len(df_trellis_grpd)):\n",
        "                    fig.add_trace(go.Bar(name= y_col_vals[i],  \n",
        "                     x=df_trellis_filtered.groupby(y_var_col)[x_var_col].get_group(y_col_vals[i]).value_counts(normalize=True).index,\n",
        "                     y=df_trellis_filtered.groupby(y_var_col)[x_var_col].get_group(y_col_vals[i]).value_counts(normalize=True).tolist()\n",
        "                                        ,showlegend=False)\n",
        "                                 ,row=fig_r, col=fig_c)\n",
        "\n",
        "        # Change the bar mode\n",
        "            fig.update_layout(barmode='stack')\n",
        "        else:\n",
        "            fig.add_trace(go.Scatter(x=df_trellis_filtered[x_var_col], y=df_trellis_filtered[y_var_col],\n",
        "                             mode = 'markers',   marker=dict(color='teal',size = 10,opacity  = 0.6),\n",
        "                                    showlegend  = False), \n",
        "                          row=fig_r, col=fig_c)\n",
        "\n",
        "    fig.update_xaxes(title = x_var_col)     \n",
        "    fig.update_yaxes(title = y_var_col)  \n",
        "    fig.update_layout(width=1000, height=500*len(titles)//2+1, showlegend=False)          \n",
        "    fig.show(config={'displayLogo':False})\n",
        "\n",
        "#Function to generate the Trellis plot based on the type of conditional variable selected\n",
        "def trellis_plot(x_var_col, y_var_col, cond_var, cat_cols, many_categories):\n",
        "    '''\n",
        "    Function to generate the Trellis plot based on the type of conditonal variable selected.\n",
        "    \n",
        "    input:\n",
        "        x_var_col       : the x variable column that the user selected\n",
        "        y_var_col       : the y variable column that the user selected\n",
        "        cond_var        : the conditional variable \n",
        "        cat_cols        : the set of categorical columns obtained from the dataset\n",
        "        many_categories : boolean check if there are too many categories to visualize and then we don't plot\n",
        "    return:\n",
        "        Output of either trellis_cat_cond() or trellis_quan_cond() function\n",
        "    '''\n",
        "    \n",
        "    if le_mapping_dict == {}:\n",
        "        many_categories = False\n",
        "    else:\n",
        "        if x_var_col in cat_cols:\n",
        "            if len(set(df_trellis[x_var_col])) > 20:\n",
        "                print(colored('Too many categories in X variable!','red'))\n",
        "                many_categories = True\n",
        "            else:\n",
        "                display(Markdown('The name-label pairs for _X axis_ of __{}__ : {}'.format(x_var_col, \n",
        "                                                                       le_mapping_dict[x_var_col])))\n",
        "        elif y_var_col in cat_cols:\n",
        "            if len(set(df_trellis[y_var_col])) > 20:\n",
        "                print(colored('Too many categories in Y variable!','red'))\n",
        "                many_categories = True\n",
        "            else:\n",
        "                display(Markdown('The name-label pairs for _Y axis_ of __{}__ : {}'.format(y_var_col, \n",
        "                                                                       le_mapping_dict[y_var_col])))\n",
        "        else:\n",
        "            pass\n",
        "    \n",
        "    if many_categories:\n",
        "        display(Markdown('__Too many categories to display the result!__ Please select an alternative categorical column.'))\n",
        "    else:\n",
        "        if cond_var in cat_cols:\n",
        "            trellis_cat_cond()\n",
        "        else:\n",
        "            trellis_quan_cond()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # if categorical condition has been provided\n",
        "        if cond_var_col in cat_cols:\n",
        "            many_categories = False\n",
        "            if len(set(df_trellis[cond_var_col])) > 20:\n",
        "                print(colored('Too many categories in conditional variable!','red'))\n",
        "                many_categories = True\n",
        "            elif x_var_col in cat_cols and y_var_col in cat_cols:\n",
        "                pass\n",
        "            else:\n",
        "                display(Markdown('The name-label pairs for _conditional column_ of __{}__ : {}'.format(cond_var_col, \n",
        "                                                                           le_mapping_dict[cond_var_col])))\n",
        "\n",
        "            tp=trellis_plot(x_var_col, y_var_col, cond_var_col, cat_cols, many_categories)\n",
        "\n",
        "        # if numerical condtion has been provided\n",
        "        else:\n",
        "            many_categories = False\n",
        "            tp=trellis_plot(x_var_col, y_var_col, cond_var_col, cat_cols, many_categories)\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "        \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve_tSgwSYywW"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIx0tTjYywX"
      },
      "source": [
        "## Multivariate Analysis <a name='multivar'></a>\n",
        "\n",
        "Multivariate Analysis, the statistical principle of multivariate statistics, is an ever-expanding set of techniques to analyze more than two features in data at a time. Covering all the possible techniques is beyond the scope of this notebook - so we will be focusing on __Interdependence Techniques__ which doesn't treat any variable as dependent or independent but focuses on understanding the underlying structure of the entire set of the variables. These techniques include:\n",
        "\n",
        "* __Factor Analysis__ - If the structure of the variables is to be analyzed\n",
        "* __Cluster Analysis__ - If the variables are to be grouped to represent the structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIUjDNbjYywX"
      },
      "source": [
        "### Factor Analysis <a name='factor'></a>\n",
        "\n",
        "Factor Analysis provides the tool for analyzing the structure of the interrelationships among a large number of variables by defining a set of highly interrelated variables, known as __factors__ or __components__. It helps in data interpretations by reducing the number of variables. Take a look at this [hypothesis example](#fact_eg) to better understand what are we doing using factor analysis.\n",
        "\n",
        "There are 2 types of factor analysis:\n",
        "* __Exploratory Factor Analysis (EFA)__: It is the most popular factor analysis approach among social and management researchers. Its basic assumption is that any observed variable is directly associated with any factor.\n",
        "* __Confirmatory Factor Analysis (CFA)__: Its basic assumption is that each factor is associated with a particular set of observed variables. CFA confirms what is expected on the basis.\n",
        "\n",
        "_In this notebook, we will be covering EFA._\n",
        "\n",
        "##### How does factor analysis work?\n",
        "As mentioned, the primary objective of factor analysis is to reduce the number of observed variables and find unobservable variables, and it can be achieved in 3 steps:\n",
        "* __Step 1 - Adequacy Test__: We need to evaluate the “factorability” of our dataset. Factorability means \"can we found the factors in the dataset?\" If yes, then only we can proceed for the remaining steps, else ignore.\n",
        "* __Step 2 - Factor Extraction__: In this step, the number of factors and factor loading is performed.\n",
        "* __Step 3 - Factor Rotation__: In this step, rotation tries to convert factors into __uncorrelated factors__ i.e., making the factors independent of each other. There are lots of rotation methods available, which are performed [here](#fact_rot), such as:\n",
        "    <a name='rot_types'></a>\n",
        "    * __Orthogonal__ rotation: _varimax, oblimax, quartimax, equamax_\n",
        "    * __Oblique__ rotation: _promax, oblimin, quartimin_\n",
        "\n",
        "\n",
        "##### Step 1. : Is Factor Analysis necessary?\n",
        "\n",
        "1. _If a visual inspection reveals no substantial number of correlations greater than .30, then factor analysis is __probably inappropriate__._ The correlations among variables can also be analyzed by computing the __partial correlations__ among variables. A partial correlation is a correlation that is unexplained when the effects of other variables are taken into account. _If “true” factors exist in the data, the partial correlation should be small, because the variable can be explained by the variables loading on the factors. __If the partial correlations are high, indicating no underlying factors, then factor analysis is inappropriate__._\n",
        "2. __Bartlett’s Test of Sphericity__ _(a measure of how closely the shape of an object resembles that of a perfect sphere)_ checks whether or not an observed matrix is significantly different from an identity matrix. Small values __(less than 0.05)__ of the significance level indicate that __factor analysis may be useful with the data__. In order to execute this test, you must ensure the following 2 conditions are satisfied:\n",
        "    * Eliminate any 1 column from the pair of columns that are __perfectly correlated__, whether it is positive or negative as obtained from the [correlation matrix](#corr_matrix).\n",
        "    * Eliminate columns with very __less variance__ (close to 0).\n",
        "\n",
        "\n",
        "3. __Measure of Sampling Adequacy (MSA)__ checks if it is possible to factorize the main variables efficiently. It ranges from 0 to 1, reaching 1 when each variable is _perfectly predicted without error by the other variables_. The measure can be interpreted with the following guidelines: \n",
        "    * => 0.80 -> meritorious\n",
        "    * => 0.70 -> middling\n",
        "    * => 0.60 -> mediocre\n",
        "    * => 0.50 -> miserable\n",
        "    * < 0.50  -> unacceptable\n",
        "    \n",
        "   The MSA increases as \n",
        "   * the sample size increases, OR\n",
        "   * the average correlations increase, OR\n",
        "   * the number of variables increases, OR\n",
        "   * the number of factors decreases\n",
        "       \n",
        "   __MSA values must exceed .50__ for both the overall test and each variable - variables with values less than .50 should be omitted from the factor analysis one at a time, with the smallest one being omitted each time. __Kaiser-Meyer-Olkin (KMO) Test__ is a measure of the adequacy of sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-04T13:48:13.701531Z",
          "start_time": "2020-03-04T13:48:13.695892Z"
        },
        "id": "CB1LZLo9YywX"
      },
      "source": [
        "#### Data Preprocessing for factor analysis\n",
        "\n",
        "In this section the categorical columns are one-hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:02.265270Z",
          "start_time": "2020-12-14T14:39:01.652130Z"
        },
        "code_folding": [
          4
        ],
        "id": "oHBIhK7OYywY"
      },
      "source": [
        "# finding the variances of the columns\n",
        "value=\"FA data preprocess\"\n",
        "\n",
        "# function to preprocess the data for factor and cluster analysis\n",
        "def data_preprocess(data):\n",
        "    # removing categorical columns having too many categories to avoid memory crashing/ singularity\n",
        "    df_desc_cat = data.describe(include=[np.object]).T\n",
        "    df_desc_cat['columns'] = [x for x in df_desc_cat.index]\n",
        "    df_desc_cat = df_desc_cat.reset_index()\n",
        "\n",
        "    cat_cols_to_drop = []\n",
        "    num_of_catgrs = []\n",
        "    for idx,each_uniq in enumerate(df_desc_cat['unique'].to_list()):\n",
        "        if each_uniq > 30:\n",
        "            num_of_catgrs.append(each_uniq)\n",
        "            cat_cols_to_drop.append(df_desc_cat.loc[idx,'columns'])\n",
        "    \n",
        "    final_cat_cols = cat_cols.copy()\n",
        "    if cat_cols_to_drop == []:\n",
        "        pass\n",
        "    else:\n",
        "        display(Markdown('You data has __`{}`__ categorical columns having more than 30 unique categories.'.format(cat_cols_to_drop)))\n",
        "        display(Markdown('If you don\\'t drop them, then you will end up with __{} more columns__ due to one-hot encoding process.'.format(sum(num_of_catgrs))))\n",
        "        display(Markdown('__NOTE__: This might cause `MemoryError` or will take a longer time to process. Also, this will affect the result of this operation.'))\n",
        "\n",
        "        while True:\n",
        "            select_ = input('\\nDo you want to DROP these columns and proceed? (Y/N): ')\n",
        "            if select_.lower() == 'y':\n",
        "                final_cat_cols = [x for x in cat_cols if x not in cat_cols_to_drop]\n",
        "                break\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    cols_to_be_used = num_cols.copy()\n",
        "    cols_to_be_used.extend(final_cat_cols)\n",
        "    \n",
        "\n",
        "    data = data[cols_to_be_used]\n",
        "\n",
        "    if cat_cols == []:\n",
        "        pass\n",
        "    else:\n",
        "        # perform one hot encoding to the categorical columns\n",
        "        data = pd.get_dummies(data, columns=final_cat_cols)\n",
        "        data = data.rename(columns = lambda x: (x.replace(' ','_')))\n",
        "\n",
        "    if num_cols == []:\n",
        "        pass\n",
        "    else:\n",
        "        for each_col in num_cols:\n",
        "            data[each_col] = (df[each_col] - df[each_col].mean())/df[each_col].std()\n",
        "\n",
        "    # in-case, there is further inf available in the processed data\n",
        "    data = data.replace([np.inf, -np.inf], np.nan)\n",
        "    data.dropna(axis=0, inplace=True)\n",
        "\n",
        "    # lower-case and replace any white-space/dot/comma seperator with '_' for every column name\n",
        "    data = data.rename(columns = lambda x: x.replace(' ','_').replace('.','_').replace(',','_').replace(\"'\",\"\"))\n",
        "    \n",
        "    return data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # create a copy of the dataframe\n",
        "        df_fact_anal = df.copy()\n",
        "        \n",
        "        df_fact_anal = data_preprocess(df_fact_anal)\n",
        "\n",
        "        display(Markdown('Displaying the __preprocessed dataframe__ of __{}__ rows and __{}__ columns:'.format(df_fact_anal.shape[0], df_fact_anal.shape[1])))\n",
        "        display_data(df_fact_anal.head(100).round(4))\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94B_sHr1YywY"
      },
      "source": [
        "#### Singularity check for variables\n",
        "\n",
        "Here, we are using the same technique as mentioned in Mahalanobis distance to remove columns causing singularity, post which we will perform the Adequacy test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:05.548563Z",
          "start_time": "2020-12-14T14:39:04.263320Z"
        },
        "id": "M2tRsP7mYywY"
      },
      "source": [
        "value = \"Absolute Linear Dependence - FA\"\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
        "    \n",
        "        col_to_drop_for_fa = []\n",
        "\n",
        "        vif_scores = get_vif(df_fact_anal,5)\n",
        "\n",
        "        linear_dependent = vif_scores[vif_scores['VIF']>=5]\n",
        "\n",
        "        if len(linear_dependent)>0:\n",
        "            col_to_drop_for_fa.extend(linear_dependent.index)\n",
        "\n",
        "            clear_output()\n",
        "            display(Markdown('__Linearly Dependent Columns dropped are:__'))\n",
        "            print('{}'.format(' || '.join(col_to_drop_for_fa)))\n",
        "\n",
        "            fact_anal_cols = df_fact_anal.columns\n",
        "\n",
        "            # columns on which factor analysis wiil be performed\n",
        "            col_to_retain_for_fa = set(fact_anal_cols) - set(col_to_drop_for_fa)\n",
        "\n",
        "            # display updated list of column names\n",
        "            df_fact_anal.drop(col_to_drop_for_fa, axis=1, inplace=True)\n",
        "            updated_fact_anal_cols_vis = ' || '.join(col_to_retain_for_fa)\n",
        "            print(colored(\"\\nColumns Retained:\",'blue',attrs=['bold']),\"\\nCount: {}\\n{}\".format(len(col_to_retain_for_fa), updated_fact_anal_cols_vis))\n",
        "        else:\n",
        "            clear_output()\n",
        "            display(Markdown(\"<span style='color:red'>No Linearly Dependent Columns in the dataframe!\"))\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    clear_output()\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbO4YldMYywY"
      },
      "source": [
        "#### Adequacy Test\n",
        "\n",
        "The adequacy test is done to understand if the dataset has enough factors to go ahead with factor analysis.\n",
        "Henry Kaiser (1970) introduced a Measure of Sampling Adequacy (MSA) of factor analytic data matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:06.858505Z",
          "start_time": "2020-12-14T14:39:06.361313Z"
        },
        "id": "W2IP_I2HYywZ"
      },
      "source": [
        "# performing Bartlett’s Test of Sphericity and  Kaiser-Meyer-Olkin (KMO) Test\n",
        "value=\"Bartlett's and KMO\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # performing Bartlett's test\n",
        "        chi_square_value, p_value = calculate_bartlett_sphericity(df_fact_anal)\n",
        "        display(Markdown('__Bartlett’s test of Sphericity__'))\n",
        "        display(Markdown('_Chi-Squared Value_ : __{}__'.format(chi_square_value.round(3))))\n",
        "        display(Markdown('_p-value_ : __{}__'.format(p_value)))\n",
        "\n",
        "        # performing Kaiser-Meyer-Olkin (KMO) Test\n",
        "        kmo_per_variable, kmo_total = calculate_kmo(df_fact_anal)\n",
        "        display(Markdown('__Kaiser-Meyer-Olkin (KMO) Test__'))\n",
        "        display(Markdown('_The KMO score overall (MSA)_  = __{}__'.format(kmo_total.round(3))))\n",
        "\n",
        "        if p_value > 0.05 or kmo_total < 0.5:\n",
        "            print(colored('DON\\'T PROCEED for Factor Analysis!','red',attrs=['bold']))\n",
        "        elif isnan(p_value) or isnan(kmo_total):\n",
        "            print(colored('You have sigular matrix! CAN\\'T PROCEED for Factor Analysis.','red',attrs=['bold']))\n",
        "        else:\n",
        "            print(colored('PROCEED for Factor Analysis!','green',attrs=['bold']))\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "        \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfuxs5EQYywZ"
      },
      "source": [
        "#### Scree plot\n",
        "##### Step 2.1. : How to obtain the right number of factors?\n",
        "\n",
        "Any decision on the number of factors to be retained should be based on several considerations:\n",
        "\n",
        "* __Latent Root Criterion__: This is the most commonly used technique. The rationale for this criterion is that any individual factor should account for the variance of at least a single variable if it is to be retained for interpretation. Thus, only the factors having __eigen-values__ _(represent variance explained each factor from the total variance)_ greater than 1 are considered significant, and rest are not. We can perform this using __Scree Plot__. It is derived by plotting the eigen-values against the number of factors in their order of extraction, and the cut-off would be the point just before where eigen-value becomes less than 1.\n",
        "* __Priori Criterion__: A predetermined number of factors based on research objectives and/or prior research\n",
        "* __Percentage of Variance Criterion__: It is an approach based on achieving a specified cumulative percentage of total variance extracted by successive factors. The purpose is to ensure practical significance for the derived factors by ensuring that they explain at least a specified amount of variance.\n",
        "\n",
        "_Here, we would be performing Latent Root Criterion._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:11.776354Z",
          "start_time": "2020-12-14T14:39:11.150537Z"
        },
        "id": "cCQXoBLmYywZ"
      },
      "source": [
        "# scree plot\n",
        "value=\"Scree Plot\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING. Might take longer to process in case of factors greater than 200. </h2></div>'))\n",
        "        # generate the eigen value matrix\n",
        "        fa = FactorAnalyzer(rotation=None)\n",
        "        fa.fit(df_fact_anal)\n",
        "        ev, v = fa.get_eigenvalues()\n",
        "        \n",
        "        fig = go.Figure()\n",
        "        \n",
        "        # generate the scree plot\n",
        "        fig.add_trace(go.Scatter(x=list(range(1, df_fact_anal.shape[1]+1)), y=ev,\n",
        "                                 mode='markers+lines', marker_color='teal', name='Factor'))\n",
        "        \n",
        "        # highlight the point where latent root criterion is satisfied using green triangle\n",
        "        latent_fact = 0\n",
        "        eigen_val = 0\n",
        "        for i,val in enumerate(list(ev)):\n",
        "            if val < 1:\n",
        "                latent_fact = i\n",
        "                eigen_val = list(ev)[latent_fact-1]\n",
        "                break\n",
        "            else:\n",
        "                pass\n",
        "        \n",
        "        fig.add_trace(go.Scatter(x=[latent_fact], y=[eigen_val], marker_symbol='x', marker_size=15,\n",
        "                                 opacity=0.75, mode='markers', marker_color='tan', name='Latent Root Factor'))\n",
        "        \n",
        "        # highlight the eigen value threshold\n",
        "        fig.add_shape(dict(type=\"line\", x0=0,y0=1, x1=df_fact_anal.shape[1]+1,y1=1,\n",
        "                           line=dict(color=\"maroon\",width=1,dash='dash')))\n",
        "        fig.add_trace(go.Scatter(x=[2.75], y=[0.9], text=[\"Eigen Value Threshold\"], \n",
        "                                 mode=\"text\", showlegend=False))\n",
        "        fig.update_layout(title='Scree Plot',title_x=0.5,width = 900, height = 600)\n",
        "        \n",
        "        fig.update_yaxes(title='Eigen Value')\n",
        "        fig.update_xaxes(title='Number of columns after scaling')\n",
        "        clear_output()\n",
        "        display(Markdown('__Highest number of factors__ that can be taken based on _Latent Root Criterion_ is __{}__'.format(latent_fact)))\n",
        "        fig.show(config={'displayLogo':False})\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)   \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1s8rlOtYywa"
      },
      "source": [
        "#### Factor loading\n",
        "\n",
        "Once we figured the maximum number of factors required, we now need to perform __factor loading__. It is a matrix that shows the relationship of each variable to the underlying factor. It shows the __correlation coefficient__ for observed variables and factors. We will generate a heat map to visualize the distribution of the factors across the columns.\n",
        "\n",
        "While performing this, the user can set the `rotation` parameter as `None`. Here, we have set it as `varimax`. Other options are listed above.\n",
        "\n",
        "Sometimes, factor analysis results in strong correlations of a variable with several factors or a variable has no strong correlations with any of the factors. To tackle this, i.e., in order to make the location of the axes fit the actual data points better, rotation is performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:13.147437Z",
          "start_time": "2020-12-14T14:39:12.603950Z"
        },
        "scrolled": false,
        "id": "TvRzaejwYywa"
      },
      "source": [
        "# performing factor loading\n",
        "value=\"Factor Loading\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
        "\n",
        "        if latent_fact>100 :\n",
        "            clear_output()\n",
        "            print(colored('Cannot display the result due to large number of factors!','red'))\n",
        "        else:\n",
        "            # analyze the factors with the maximum number of factors obtained\n",
        "            fa_threshold_fact = FactorAnalyzer(n_factors = latent_fact, rotation='varimax', bounds=(0.01,1))\n",
        "            fa_threshold_fact.fit(df_fact_anal)\n",
        "\n",
        "            # generate those factor values in a matrix\n",
        "            matrix_fa_threshold_fact = fa_threshold_fact.loadings_\n",
        "\n",
        "            # store the matrix in a dataframe\n",
        "            df_fa_threshold_fact = pd.DataFrame(data = matrix_fa_threshold_fact,\n",
        "                                                columns = ['fact_{}'.format(i+1) for i in range(latent_fact)],\n",
        "                                                index = df_fact_anal.columns)\n",
        "            \n",
        "            heat = go.Heatmap(z=df_fa_threshold_fact, x=df_fa_threshold_fact.columns, \n",
        "                              y=df_fact_anal.columns, xgap=1, ygap=1, colorscale='RdYlGn')\n",
        "            \n",
        "            clear_output()\n",
        "            display(Markdown('__Plot generated!__'))\n",
        "            layout = go.Layout(title_text='Factor Loading Matrix', title_x=0.5, \n",
        "                               width=900, height=900,template='none',\n",
        "                               xaxis_showgrid=False, yaxis_showgrid=False,\n",
        "                               yaxis_autorange='reversed')\n",
        "   \n",
        "            fig=go.Figure(data=[heat], layout=layout)\n",
        "            fig.update_xaxes(tickangle=45, tickfont=dict(size=8))\n",
        "            fig.update_yaxes(tickangle=45, tickfont=dict(size=8))\n",
        "            fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTg8_YmmYywa"
      },
      "source": [
        "#### Variance explained\n",
        "Now, we need to obtain the __variance of each factor__.\n",
        "\n",
        "* The __SS loadings__ row is the sum of squared loadings. This is sometimes used to determine the value of a particular factor. _We say a factor is worth keeping if the __SS loading is greater than 1__._\n",
        "* __Variance__ is simply the proportion of variance explained by each factor.\n",
        "* __Cumulative Variance__ tells us about the total variance explained by all the factors put together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:14.679573Z",
          "start_time": "2020-12-14T14:39:14.403385Z"
        },
        "id": "-KvD4Cn6Yywa"
      },
      "source": [
        "# calculating the variance of the factors\n",
        "value=\"Variance of factors\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
        "        if latent_fact>100:\n",
        "            clear_output()\n",
        "            print(colored('Cannot display the result due to large number of factors!','red'))\n",
        "        else:\n",
        "            # generating the variances across each factors\n",
        "            matrix_fa_var = fa_threshold_fact.get_factor_variance()\n",
        "\n",
        "            # naming the indexes\n",
        "            index_names = ['Sum of Square (SS) Loading', 'Variance', 'Cumulative Variance']\n",
        "\n",
        "            # store the matrix in a dataframe\n",
        "            df_fa_var = pd.DataFrame(data = matrix_fa_var, \n",
        "                                     columns = ['fact_{}'.format(i+1) for i in range(latent_fact)],\n",
        "                                     index = index_names)\n",
        "            clear_output()\n",
        "            # displaying the necessary information\n",
        "            display(df_fa_var.head(10).round(3))\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_h1CYuvYywb"
      },
      "source": [
        "#### Uniqueness and Communality\n",
        "\n",
        "__Uniqueness__ is the variance that is 'unique' to the variable and not shared with other variables. It ranges from 0 to 1. _A high uniqueness value implies that it __doesn't fit neatly__ into our factors_.\n",
        "\n",
        "If we subtract the uniqueness from 1, we get __communality__. The communality of a variable is the proportion of variance of that variable contributed by the common factors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:16.691496Z",
          "start_time": "2020-12-14T14:39:16.432098Z"
        },
        "id": "EmaYo9a_Yywb"
      },
      "source": [
        "# obtaining the factors\n",
        "\n",
        "value = 'uniqueness communality'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        fa_fact = Factor(endog=df_fact_anal, n_factor=latent_fact)\n",
        "        fa_fact.fit()\n",
        "\n",
        "        # obtaining results from factor\n",
        "        fa_fact_result = FactorResults(fa_fact)\n",
        "\n",
        "        # finding uniqueness and communality\n",
        "        df_fa_unq_comm = pd.DataFrame({'columns':df_fact_anal.columns, 'uniqueness':fa_fact_result.uniqueness,\n",
        "                                      'communality':fa_fact_result.communality})\n",
        "\n",
        "        display_data(df_fa_unq_comm.round(4))\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NToGhiYYywb"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld8Wu4K4Yywb"
      },
      "source": [
        "### Cluster Analysis <a name='cluster'></a>\n",
        "\n",
        "Cluster Analysis's primary purpose is to group objects based on the characteristics they possess. The resulting clusters should exhibit __high internal (within-group) homogeneity__ and __high-external (between-groups) heterogeneity__. If the distinction is successful, then objects within clusters will be close together when plotted geometrically, and different clusters will be far apart. Take a look at this [hypothesis example](#clus_eg) to better understand what are we doing using cluster analysis.\n",
        "\n",
        "Cluster Analysis can be done via 2 approaches:\n",
        "* __Centroid Based__: Centroid based clustering algorithms determine the similarity between data points in a cluster _by their closeness with the centroid_. Clusters are formed by assigning each data point to the nearest cluster center, then recalculating the center and reassigning data points to the nearest newly calculated centroid. This process is done iteratively and the points which don’t belong to any cluster are considered outliers. _Example - __K-Means__._\n",
        "* __Density Based__: Density based clustering algorithms look for _varied density regions in data space_. Density regions are separated from each other and the data points in each region are assigned to each cluster. Those records/objects who don’t lie in any density region are classified as an outlier. This method partitions data space. _Example - __DBSCAN (Density-Based Spatial Clustering of Applications with Noise)__._\n",
        "\n",
        "K-Means clustering aims to partition `n` observations into `k` clusters in such a way that the __loss function__ (_sum of the euclidean squared distance of each observation from its assigned cluster's centroid_ ) __is minimized__. Now, K-Means forms almost circular clusters. If outliers are present in data, then the centroid of the cluster shifts which in turn __increases loss function__. Unlike K-Means, DBSCAN can discover arbitrarily shaped clusters and isn’t affected by outliers because clusters formation depends on the density regions of data space, and as a result, the outliers belong to a separate cluster having least density region. Hence, for the _cluster analysis technique_ , we will be focusing on __K-Means__.\n",
        "\n",
        "##### K-Means Clustering Technique\n",
        "\n",
        "As already mentioned, it usually consists of 3 main steps:\n",
        "* __Step 1 - Initialization__ : Choosing the parameters of the model, such as the value of K, distance metric, number of iterations\n",
        "* __Step 2 - Assignment Step__ : The assignment of each observation into one of K clusters, such that the data space is partitioned\n",
        "* __Step 3 - Update Step__ : Based on the previous assignments, a new cluster center is computed\n",
        "* __Step 4__ : Step 2 and 3 are repeated until the __stopping conditions__ are satisfied, which are:\n",
        "    * __Convergence criterion__ : The algorithm is said to converge when the difference between the updated centers and the previous one is less than that small delta value.\n",
        "    * Centroids of newly formed clusters do not change and points remain in the same cluster\n",
        "    * Maximum number of iterations are reached\n",
        "\n",
        "__Silhouette Analysis__ can be used to perform the above-mentioned operations. For each sample, compute the average distance from all data points in the same cluster ($a_i$), from the closest cluster ($b_i$) and finally calculate the __Silhouette Coefficient__ using the formula:\n",
        "\n",
        "$$\\begin{gather*}\n",
        "SC = \\frac {(b_i - a_i)} {max(a_i * b_i)}\n",
        "\\end{gather*}$$\n",
        "\n",
        "Silhouette Score\n",
        "* near +1 indicates that the sample is far away from the neighboring clusters.\n",
        "* near 0 indicates that the sample is very close to the decision boundary between two neighboring clusters.\n",
        "* near -1 indicates that the point is close to its neighboring cluster than to the cluster is assigned.\n",
        "\n",
        "_Hence, a high Silhouette Score is indicative of homogenous clusters._ \n",
        "\n",
        "So, the cell below is first scaling the _numerical columns using __standardization__ as well as data __one-hot encoding__ the categorical columns_. Once we obtain the highest Silhouette Score, we will obtain the optimal value of the number of clusters required to perform K-means clustering, which is indicated by the red-dotted line.\n",
        "\n",
        "#### Data preprocessing for cluster analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:19.497864Z",
          "start_time": "2020-12-14T14:39:19.004361Z"
        },
        "id": "hvd-Dol5Yywc"
      },
      "source": [
        "# finding the variances of the columns\n",
        "value=\"CA data preprocess\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # create a copy of the dataframe\n",
        "        df_silhouette = df.copy()\n",
        "        \n",
        "        df_silhouette = data_preprocess(df_silhouette)\n",
        "        \n",
        "        display(Markdown('Displaying the __preprocessed dataframe__ of __{}__ rows and __{}__ columns:'.format(df_silhouette.shape[0], df_silhouette.shape[1])))\n",
        "        display_data(df_silhouette.head(100).round(4))\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "        \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlH4Sl6kYywc"
      },
      "source": [
        "#### Singularity check for variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:39:22.239157Z",
          "start_time": "2020-12-14T14:39:20.988526Z"
        },
        "code_folding": [],
        "scrolled": false,
        "id": "C_qQgDpMYywc"
      },
      "source": [
        "value = \"Absolute Linear Dependence - CA\"\n",
        "\n",
        "try:\n",
        "    if __name__ == '__main__':\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
        "    \n",
        "        col_to_drop_for_ca = []\n",
        "\n",
        "        vif_scores = get_vif(df_silhouette,5)\n",
        "\n",
        "        linear_dependent = vif_scores[vif_scores['VIF']>=5]\n",
        "\n",
        "        if len(linear_dependent)>0:\n",
        "            col_to_drop_for_ca.extend(linear_dependent.index)\n",
        "\n",
        "            clear_output()\n",
        "            display(Markdown('__Linearly Dependent Columns dropped are:__'))\n",
        "            print('{}'.format(' || '.join(col_to_drop_for_ca)))\n",
        "\n",
        "            clus_anal_cols = df_silhouette.columns\n",
        "\n",
        "            # columns on which cluster analysis wiil be performed\n",
        "            col_to_retain_for_ca = set(clus_anal_cols) - set(col_to_drop_for_ca)\n",
        "\n",
        "            # display updated list of column names\n",
        "            df_silhouette.drop(col_to_drop_for_ca, axis=1, inplace=True)\n",
        "            updated_fact_anal_cols_vis = ' || '.join(col_to_retain_for_ca)\n",
        "            print(colored(\"\\nColumns Retained:\",'blue',attrs=['bold']),\"\\n{}\\n{}\".format(len(col_to_retain_for_ca), updated_fact_anal_cols_vis))\n",
        "        else:\n",
        "            clear_output()\n",
        "            display(Markdown(\"<span style='color:red'>No Linearly Dependent Columns in the dataframe!\"))\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    track_cell(value, flag)\n",
        "except Exception as err:\n",
        "    clear_output()\n",
        "    # display the error\n",
        "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "    flag = 0\n",
        "    err = str(err)\n",
        "    track_cell(value, flag, err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--cQMO7wYywd"
      },
      "source": [
        "#### Identifying optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:17.333491Z",
          "start_time": "2020-12-14T14:40:08.261214Z"
        },
        "id": "6r6OO1ehYywd"
      },
      "source": [
        "##### identify the optimal number of clusters based on Silhouette Score\n",
        "value=\"Silhouette Score Plot\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    display(Markdown('Here, enter the __maximum number of clusters__ against which you will __iterate__ to get the __optimal number of clusters__.'))\n",
        "\n",
        "    while True:\n",
        "        range_upper_limit = int(input('Enter any value > 3: '))\n",
        "        if range_upper_limit <= 3:\n",
        "            print(colored('\\nPlease enter a number greater than 3 to proceed!','red',attrs=['bold']))\n",
        "            continue\n",
        "        else:\n",
        "            if range_upper_limit > 50:\n",
        "                display(Markdown('__WARNING!__ Your number of iteration is very high. This will take time to process!'))\n",
        "                confirm_ask = input('\\nDo you want to continue with {} iterations? (Y/N): '.format(range_upper_limit))\n",
        "                if confirm_ask.lower() == 'y':\n",
        "                    break\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
        "\n",
        "    silhouette_avg_list = []\n",
        "    range_n_clusters = range(3,range_upper_limit+1)\n",
        "\n",
        "    try:\n",
        "        for n_clusters in tqdm(range_n_clusters):\n",
        "            # initialize the clusterer with `n_clusters` value and a random generator seed for reproducibility.\n",
        "            # change the iterations as per your choice\n",
        "            clusterer = KMeans(n_clusters=n_clusters, random_state=111, max_iter=50)\n",
        "            cluster_predict = clusterer.fit_predict(df_silhouette)\n",
        "\n",
        "            # this gives a perspective into the density and separation of the formed clusters \n",
        "            # based on Manhattan Distance. You will get totally different result for Euclidean.\n",
        "            silhouette_avg = silhouette_score(df_silhouette, cluster_predict, metric='manhattan')\n",
        "\n",
        "            # Append to the list\n",
        "            silhouette_avg_list.append(silhouette_avg)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('1. ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "\n",
        "    try:\n",
        "        # pairing the cluster count and Silhouette Score and storing in a dataframe\n",
        "        best_n_clusters_avg_silhouette_pair = zip(list(range_n_clusters), silhouette_avg_list)\n",
        "\n",
        "        df_avg_silhouette = pd.DataFrame()\n",
        "        df_avg_silhouette['clusters'] = pd.Series(list(range_n_clusters))\n",
        "        df_avg_silhouette['silhouette_score'] = pd.Series(silhouette_avg_list)\n",
        "\n",
        "        # obtaining the highest Silhouette Score and hence the best cluster count\n",
        "        highest_avg_silhouette_val = max(silhouette_avg_list)\n",
        "\n",
        "        for clust,avg_silhouette_val in best_n_clusters_avg_silhouette_pair:\n",
        "            if avg_silhouette_val == highest_avg_silhouette_val:\n",
        "                best_n_clusters = clust\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        clear_output()\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('2. ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "\n",
        "    try:\n",
        "        # displaying the result and highlighting the best cluster count with maroon line\n",
        "\n",
        "        fig = go.Figure()\n",
        "        fig.add_shape(dict(type=\"line\", x0=best_n_clusters,y0=0, x1=best_n_clusters,\n",
        "                           y1=max(df_avg_silhouette['silhouette_score']),\n",
        "                           line=dict(color=\"maroon\",width=3,dash='dash')))\n",
        "        \n",
        "        fig.add_trace(go.Line(x=df_avg_silhouette['clusters'], \n",
        "                              y=df_avg_silhouette['silhouette_score'], \n",
        "                              mode=\"lines\", showlegend=False))\n",
        "        \n",
        "        fig.update_layout(title='silhouette scores for various number of clusters',\n",
        "                          title_x=0.5, width = 900, height = 600)\n",
        "        \n",
        "        fig.update_yaxes(title='Silhouette Score')\n",
        "        fig.update_xaxes(title='Number of clusters',range=[2, range_upper_limit+1])\n",
        "        fig.show(config={'displaylogo': False})\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('3. ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "\n",
        "    display(Markdown('<span style=\"color:darkgreen; font-size: 15px\">Continue executing the next cell for showing the <b>frequency distribution plot</b>.</span>'))\n",
        "    \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTvYHlCCYywd"
      },
      "source": [
        "###### Cluster frequency plot\n",
        "\n",
        "Now that we have obtained the optimal number of clusters, we will proceed to visualize the _data space partition_ using a __bar plot__.\n",
        "\n",
        "The bar plots indicate the distribution of observations in each of the clusters formed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:24.334097Z",
          "start_time": "2020-12-14T14:40:23.825597Z"
        },
        "id": "i9pC7xdNYywe"
      },
      "source": [
        "# frequency bar plot to find the distribution of the data points across the clusters\n",
        "value=\"Bar Plot with Optimal Clusters\"\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
        "\n",
        "        # initialize the best_clusterer with best_n_clusters value and a random generator seed of 10 for reproducibility.\n",
        "        best_clusterer = KMeans(n_clusters=best_n_clusters, random_state=10)\n",
        "        best_clusterer_predict = best_clusterer.fit_predict(df_silhouette)\n",
        "\n",
        "        # silhouette_score gives the average value for all the samples.\n",
        "        silhouette_avg = silhouette_score(df_silhouette, best_clusterer_predict)\n",
        "\n",
        "        # compute the silhouette scores for each sample\n",
        "        sample_silhouette_values = silhouette_samples(df_silhouette, best_clusterer_predict)\n",
        "\n",
        "        # sort the cluster IDs and store it in a dataframe\n",
        "        best_clusterer_predict_list_sorted = sorted(best_clusterer_predict.tolist())\n",
        "        best_clusterer_predict_df = pd.DataFrame({'cluster':best_clusterer_predict_list_sorted})\n",
        "\n",
        "        # calculate the count of the clusters\n",
        "        cluster_value_pair = dict(best_clusterer_predict_df['cluster'].value_counts().items())\n",
        "\n",
        "        # list of category names and their corresponding values\n",
        "        cluster_value_pair_keys = list(cluster_value_pair.keys())\n",
        "        cluster_value_pair_values = list(cluster_value_pair.values())\n",
        "\n",
        "        # map the keys under the `cluster` column and value count under `count` column\n",
        "        cluster_dict = {'cluster':cluster_value_pair_keys, 'count':cluster_value_pair_values}\n",
        "        best_clusterer_predict_df_freq = pd.DataFrame(cluster_dict)\n",
        "\n",
        "        # sort the dataframe\n",
        "        best_clusterer_predict_df_freq.sort_values('cluster', inplace=True)\n",
        "        best_clusterer_predict_df_freq['cluster'] = best_clusterer_predict_df_freq.cluster.astype(object)\n",
        "\n",
        "        # display the bar plot\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Bar(x=best_clusterer_predict_df_freq['cluster'], \n",
        "                             y=best_clusterer_predict_df_freq['count'], \n",
        "                             marker_color = 'teal', showlegend=False))\n",
        "        fig.update_layout(title='Silhouette scores for various number of clusters',\n",
        "                          xaxis_tickformat=',d',\n",
        "                          title_x=0.5,width = 900, height = 600)\n",
        "        \n",
        "        fig.update_yaxes(title='Count')\n",
        "        fig.update_xaxes(title='Clusters')\n",
        "        clear_output()\n",
        "        fig.show(config={'displaylogo': False})\n",
        "        \n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "    else:\n",
        "        display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for <b>distribution of centroid across columns</b>.</span>'))\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd_l7h7kYywe"
      },
      "source": [
        "#### Centroid distribution plots\n",
        "The plots below show how the __centroids of each column are distributed across every cluster__. Based on the value of centroids (range of [-2,2]), it can be understood which variable(s) are influencing that cluster more - __higher the value, more is the influence__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:26.810368Z",
          "start_time": "2020-12-14T14:40:26.408738Z"
        },
        "id": "yxTxm5BbYywe"
      },
      "source": [
        "# generating the distribution of centroids for each columns across the clusters\n",
        "value=\"Distribution of Centroids\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
        "\n",
        "    try:\n",
        "        # obtaining the best set of clusters' centroid\n",
        "        best_clusterer_centroids = best_clusterer.cluster_centers_\n",
        "\n",
        "        # creating dataframe with cluster centroids and transposing it\n",
        "        df_best_clusterer_centroids_all = pd.DataFrame(columns = df_silhouette.columns, \n",
        "                                                   data = best_clusterer_centroids)\n",
        "        updated_num_cols = [i for i in num_cols if i not in col_to_drop_for_ca]\n",
        "        \n",
        "        # performing only on numerical columns\n",
        "        df_best_clusterer_centroids = df_best_clusterer_centroids_all[updated_num_cols].T\n",
        "        df_best_clusterer_centroids.reset_index(inplace=True)\n",
        "\n",
        "        # renaming the columns\n",
        "        df_best_clusterer_centroids.rename(columns = lambda x: 'cluster_{}'.format(x), inplace=True)\n",
        "        df_best_clusterer_centroids.rename(columns = {'cluster_index': 'columns'}, inplace=True)\n",
        "\n",
        "        # obtaining the maxima and minima for the plot\n",
        "        max_centroid = df_best_clusterer_centroids.iloc[:,1:].values.max()\n",
        "        min_centroid = df_best_clusterer_centroids.iloc[:,1:].values.min()\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('1. ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "\n",
        "    try:\n",
        "        # initializing the tabs\n",
        "        selected_panels = []\n",
        "        fig = go.Figure()\n",
        "        # generating the plot across the tabs\n",
        "        for i in range(best_clusterer_centroids.shape[0]):\n",
        "            selected_panels.append('cluster_{}'.format(i))\n",
        "            visible = True if i==0 else False\n",
        "            df_best_clusterer_centroids_filtered = df_best_clusterer_centroids[['columns','cluster_{}'.format(i)]]            \n",
        "            fig.add_trace(go.Bar(\n",
        "                        x=df_best_clusterer_centroids_filtered['cluster_{}'.format(i)],\n",
        "                        y=df_best_clusterer_centroids_filtered['columns'],marker_color = 'teal',\n",
        "                        orientation='h',visible = visible))\n",
        "            fig.add_shape(dict(type=\"line\", x0=-1.96, x1=-1.96,\n",
        "                          y0=df_best_clusterer_centroids_filtered['columns'][0]\n",
        "                                 , y1=df_best_clusterer_centroids_filtered['columns'][len(df_best_clusterer_centroids_filtered['columns']) - 1]\n",
        "\n",
        "                           ,line=dict(color=\"maroon\",width=3,dash='dash')))\n",
        "            fig.add_shape(dict(type=\"line\", x0=1.96, x1=1.96,y0=df_best_clusterer_centroids_filtered['columns'][0]\n",
        "                                  ,y1=df_best_clusterer_centroids_filtered['columns'][len(df_best_clusterer_centroids_filtered['columns']) - 1]\n",
        "                           ,line=dict(color=\"maroon\",width=3,dash='dash')))\n",
        "            fig.update_layout(height = 900, width = 900)\n",
        "            fig.update_xaxes(title='Z-Score', range=[-3,3])\n",
        "            fig.update_yaxes(title='Columns')\n",
        "            \n",
        "        panel_dict_list = []\n",
        "        for i, each_panel in enumerate(selected_panels):\n",
        "            vis_check = [[True]*1 if i==each_panel else [False]*1 for i in selected_panels]\n",
        "            vis_check_flat = [item for sublist in vis_check for item in sublist]\n",
        "\n",
        "            panel_dict_list.append(dict(args = [{\"visible\": vis_check_flat}],\n",
        "                                        label=each_panel, method=\"update\"))\n",
        "            \n",
        "            # Add dropdown by opening the option on horizontal direction\n",
        "            fig.update_layout(updatemenus=[dict(buttons=list(panel_dict_list),\n",
        "                                                direction=\"right\", x=0, xanchor=\"left\", y=1.1, yanchor=\"top\")]),\n",
        "\n",
        "        clear_output()\n",
        "        display(Markdown('__Plots generated!__'))\n",
        "        fig.show(config={'displaylogo': False})\n",
        "\n",
        "        track_cell(value, flag) \n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        print(colored('2. ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "    else:\n",
        "        display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for completing the <b>cluster analysis</b>.</span>'))\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0NzoNs9Yywe"
      },
      "source": [
        "#### Sammon's projection\n",
        "\n",
        "Finally, the graph below is __Sammon's projection__. It is an algorithm that maps a high-dimensional space to a space of lower dimensionality while attempting to preserve the structure of inter-point distances in the projection. It is particularly suited for use in exploratory data analysis and is usually considered a non-linear approach since the mapping cannot be represented as a linear combination of the original variables. The centroids are plotted in 2D after performing Sammon’s projection.\n",
        "\n",
        "Denoting the distance between $i^{th}$ and $j^{th}$ objects in the original space by $d_{ij}^*$, and the distance between their projections by $d_{ij}$. Sammon’s mapping aims to minimize the below error function, which is often referred to as Sammon’s stress or Sammon’s error:\n",
        "\n",
        "$$E=\\frac{1}{\\sum_{i<j} d_{ij}^*}\\sum_{i<j}\\frac{(d_{ij}^*-d_{ij})^2}{d_{ij}^*}$$\n",
        "\n",
        "The minimization  of this can be performed either by gradient descent or by other means, usually involving iterative methods. The number of iterations need to be experimentally determined and convergent solutions are not always guaranteed. Many implementations prefer to use the first Principal Components as a starting configuration.\n",
        "\n",
        "Steps to Sammon's projection are:\n",
        "* Extract centroid for individual clusters\n",
        "* Calculate distance matrix between clusters\n",
        "* Generate Sammon's co-ordinates to visualize in 2-D plane\n",
        "* Generate Delaunay Triangulation to show cluster boundaries, the color scale for cluster areas is generated by taking the minimum and maximum of the respective variables across clusters to highlight the value estimate(s) in the clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:28.819018Z",
          "start_time": "2020-12-14T14:40:28.496413Z"
        },
        "code_folding": [
          5
        ],
        "id": "4FuEnz9gYywf"
      },
      "source": [
        "# calculating the voronoi associated with the data\n",
        "\n",
        "value = 'Voronoi Calculation'\n",
        "\n",
        "# creating function to generate the voronoi polygons\n",
        "def voronoi_finite_polygons_2d(vor):\n",
        "    '''\n",
        "    Function to reconstruct infinite voronoi regions in a 2D diagram to finite regions.\n",
        "\n",
        "    input:\n",
        "        vor    : Voronoi input diagram\n",
        "        radius : Distance to 'points at infinity'. Float (optional).\n",
        "\n",
        "    return:\n",
        "        regions  : List of Indices of vertices in each revised Voronoi regions.\n",
        "        vertices : List of coordinates for revised Voronoi vertices. Same as coordinates \n",
        "                   of input vertices, with 'points at infinity' appended to the end.\n",
        "    '''\n",
        "\n",
        "    if vor.points.shape[1] != 2:\n",
        "        return None, None\n",
        "    else:\n",
        "        new_regions = []\n",
        "        line_coord = []\n",
        "        \n",
        "        new_vertices = vor.vertices.tolist()\n",
        "        \n",
        "        center = vor.points.mean(axis=0)\n",
        "        radius = vor.points.ptp().max()\n",
        "        \n",
        "        # Construct a map containing all ridges for a given point\n",
        "        all_ridges = {}\n",
        "        for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
        "            all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
        "            all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
        "\n",
        "        # Reconstruct infinite regions\n",
        "        for p1, region in enumerate(vor.point_region):\n",
        "            vertices = vor.regions[region]\n",
        "\n",
        "            if all(v >= 0 for v in vertices):\n",
        "                # finite region\n",
        "                new_regions.append(vertices)\n",
        "            else:\n",
        "                # reconstruct a non-finite region\n",
        "                ridges = all_ridges[p1]\n",
        "                new_region = [v for v in vertices if v >= 0]\n",
        "\n",
        "                for p2, v1, v2 in ridges:\n",
        "                    if v2 < 0:\n",
        "                        v1, v2 = v2, v1\n",
        "                    if v1 >= 0:\n",
        "                        # finite ridge: already in the region\n",
        "                        continue\n",
        "\n",
        "                    # Compute the missing endpoint of an infinite ridge\n",
        "                    t = vor.points[p2] - vor.points[p1] # tangent\n",
        "                    t /= np.linalg.norm(t)\n",
        "                    n = np.array([-t[1], t[0]])  # normal\n",
        "\n",
        "                    midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
        "                    direction = np.sign(np.dot(midpoint - center, n)) * n\n",
        "                    far_point = vor.vertices[v2] + direction * radius\n",
        "\n",
        "                    new_region.append(len(new_vertices))\n",
        "                    new_vertices.append(far_point.tolist())\n",
        "                    line_coord.append(vor.vertices[v2].tolist())\n",
        "\n",
        "                # sort region counterclockwise\n",
        "                vs = np.asarray([new_vertices[v] for v in new_region])\n",
        "                c = vs.mean(axis=0)\n",
        "                angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
        "                new_region = np.array(new_region)[np.argsort(angles)]\n",
        "\n",
        "                new_regions.append(new_region.tolist())\n",
        "            \n",
        "        line_coord.sort()\n",
        "        line_coord = list(k for k,_ in groupby(line_coord))\n",
        "\n",
        "        return new_regions, np.asarray(new_vertices), line_coord\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        df_silhouette_cat_cols = set(df_silhouette.columns) - set(num_cols)\n",
        "        df_best_clusterer_centroids_T = df_best_clusterer_centroids.T.reset_index()\n",
        "\n",
        "        #grab the first row for the header, take the data less the header row and set the header row\n",
        "        new_header = df_best_clusterer_centroids_T.iloc[0]\n",
        "        df_best_clusterer_centroids_T = df_best_clusterer_centroids_T[1:]\n",
        "        df_best_clusterer_centroids_T.columns = new_header\n",
        "\n",
        "        for each_col in df_best_clusterer_centroids_T:\n",
        "            if each_col in df_silhouette_cat_cols:\n",
        "                df_best_clusterer_centroids_T.drop(each_col, axis=1, inplace=True)\n",
        "\n",
        "        embedding = MDS(n_components=2, max_iter=50)\n",
        "        polygon_data = embedding.fit_transform(df_best_clusterer_centroids_T.iloc[:,1:])\n",
        "        df_polygon_data = pd.DataFrame(polygon_data, columns=['X','Y'])\n",
        "        df_polygon_data['cluster_id'] = df_best_clusterer_centroids.columns[1:]\n",
        "\n",
        "        polygon_data = np.c_[df_polygon_data['X'], df_polygon_data['Y']]\n",
        "        # compute Voronoi tesselation\n",
        "        vor = Voronoi(polygon_data)\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "    else:\n",
        "        display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for generating the <b>Sammon\\'s Projection.</b>.</span>'))\n",
        "        \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:31.554415Z",
          "start_time": "2020-12-14T14:40:30.020184Z"
        },
        "code_folding": [
          4
        ],
        "scrolled": false,
        "id": "lhCGzd9qYywf"
      },
      "source": [
        "## generating the sammons plot\n",
        "\n",
        "value = \"Sammon's Plot Generation\"\n",
        "\n",
        "def polygon_adjustment(far_points_list,vertex_val):\n",
        "    '''\n",
        "    Function to adjust the polygon shape for clusters 3 and 4\n",
        "    '''\n",
        "    slope = (centroids[r][1] - vertex_val[1])/(centroids[r][0] - vertex_val[0])\n",
        "\n",
        "    if centroids[r][0] > 0 and centroids[r][1] > 0:\n",
        "        x_new = centroids[r][0] + 6*sqrt(1/(1 + slope**2))\n",
        "        y_new = centroids[r][1] + slope*6*sqrt(1/(1 + slope**2))\n",
        "    elif centroids[r][0] > 0 and centroids[r][1] < 0:\n",
        "        x_new = centroids[r][0] + 6*sqrt(1/(1 + slope**2))\n",
        "        y_new = centroids[r][1] + slope*6*sqrt(1/(1 + slope**2))\n",
        "    elif centroids[r][0] < 0 and centroids[r][1] > 0:\n",
        "        x_new = centroids[r][0] - 6*sqrt(1/(1 + slope**2))\n",
        "        y_new = centroids[r][1] - slope*6*sqrt(1/(1 + slope**2))\n",
        "    else:\n",
        "        x_new = centroids[r][0] - 6*sqrt(1/(1 + slope**2))\n",
        "        y_new = centroids[r][1] - slope*6*sqrt(1/(1 + slope**2))\n",
        "\n",
        "    rect_x = [far_points_list[0][0], x_new, far_points_list[1][0]]\n",
        "    rect_y = [far_points_list[0][1], y_new, far_points_list[1][1]]\n",
        "    \n",
        "    return rect_x, rect_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
        "\n",
        "        centroids = [(k,v) for k,v in zip(df_polygon_data['X'], df_polygon_data['Y'])]\n",
        "        regions, vertices, line_coord = voronoi_finite_polygons_2d(vor)\n",
        "\n",
        "        if regions is None:\n",
        "            print(\"Requires 2D input!\")\n",
        "        else:\n",
        "            sammons_num_cols = df_best_clusterer_centroids_T.columns.tolist()[1:]\n",
        "            sammons_tabs = [Panel() for i in range(len(sammons_num_cols))]\n",
        "\n",
        "            # generating the plot across the tabs\n",
        "            for i,each_tab in enumerate(sammons_tabs):\n",
        "                col_selected = df_best_clusterer_centroids_T[sammons_num_cols[i]].tolist()\n",
        "\n",
        "                # normalize chosen colormap\n",
        "                norm_colors = mcolors.Normalize(vmin=min(col_selected), vmax=max(col_selected), clip=True)\n",
        "                mapper = cm.ScalarMappable(norm=norm_colors, cmap=cm.get_cmap('viridis', 256))\n",
        "\n",
        "                color_mapper = LinearColorMapper(palette=\"Viridis256\", low=min(col_selected), high=max(col_selected))\n",
        "                color_bar = ColorBar(color_mapper=color_mapper, label_standoff=12, border_line_color=None, location=(0,0))\n",
        "\n",
        "                # plotting the location of centroids for each clusters        \n",
        "                source = ColumnDataSource(data=dict(x=df_polygon_data['X'], \n",
        "                                                    y=df_polygon_data['Y'], \n",
        "                                                    clus_id=df_polygon_data['cluster_id']))\n",
        "\n",
        "                hover = HoverTool(tooltips=[\n",
        "                    (\"index\", \"$index\"),\n",
        "                    (\"(x,y)\", \"(@x, @y)\"),\n",
        "                    ('cluster_id', '@clus_id'),\n",
        "                ])\n",
        "                save = SaveTool()\n",
        "                reset = ResetTool()\n",
        "\n",
        "                colors = factor_cmap('clus_id', palette=Category20[19], \n",
        "                                     factors=df_polygon_data['cluster_id'].unique())\n",
        "\n",
        "                plot_sammons = figure(height=600, width=900, tools=[hover,save,reset])\n",
        "                plot_sammons.scatter(x='x',y='y',source=source, radius=0.1, alpha = 0.9, fill_color=colors)\n",
        "\n",
        "                # plotting the vertices that will create the segments\n",
        "                plot_sammons.asterisk(vor.vertices[:,0], vor.vertices[:,1], size=10)\n",
        "\n",
        "                # colorize\n",
        "                for r,region in enumerate(regions):\n",
        "                    polygon = vertices[region]\n",
        "\n",
        "                    rgb_val = [int(i*255) for i in mapper.to_rgba(col_selected[r])]\n",
        "                    hex_val = '#%02x%02x%02x' % (rgb_val[0], rgb_val[1], rgb_val[2])\n",
        "\n",
        "                    if len(line_coord) == 1:\n",
        "                        far_points_list = polygon.tolist()\n",
        "                        vertex_val = []\n",
        "                        for each_far in far_points_list:\n",
        "                            if each_far == line_coord[0]:\n",
        "                                vertex_val = line_coord[0]\n",
        "                                far_points_list.remove(each_far)\n",
        "\n",
        "                        rect_x, rect_y = polygon_adjustment(far_points_list,vertex_val)\n",
        "\n",
        "                        plot_sammons.patch(rect_x, rect_y, alpha=0.4, \n",
        "                                           fill_color=hex_val,\n",
        "                                           line_color=hex_val,\n",
        "                                           line_alpha=0)\n",
        "\n",
        "                    elif len(line_coord) == 2:\n",
        "                        each_poly = Polygon(polygon.tolist())\n",
        "                        each_cent = Point(centroids[r])\n",
        "\n",
        "                        if each_poly.contains(each_cent):\n",
        "                            pass\n",
        "                        else:\n",
        "                            far_points_list = polygon.tolist()\n",
        "                            vertex_val = []\n",
        "                            for each_lc in far_points_list:\n",
        "                                if each_lc in line_coord:\n",
        "                                    vertex_val = each_lc\n",
        "                                    far_points_list.remove(each_lc)\n",
        "\n",
        "                            rect_x, rect_y = polygon_adjustment(far_points_list,vertex_val)\n",
        "\n",
        "                            plot_sammons.patch(rect_x, rect_y, alpha=0.4, \n",
        "                                                fill_color=hex_val,\n",
        "                                                line_color=hex_val,\n",
        "                                                line_alpha=0)\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "                    plot_sammons.patch(*zip(*polygon), alpha=0.4,\n",
        "                                       fill_color=hex_val,\n",
        "                                       line_color=hex_val,\n",
        "                                       line_alpha=0)\n",
        "\n",
        "                plot_sammons.xgrid.grid_line_color = None\n",
        "                plot_sammons.ygrid.grid_line_color = None\n",
        "                plot_sammons.toolbar.logo = None\n",
        "                plot_sammons.x_range=Range1d(vor.min_bound[0] - 0.5, vor.max_bound[0] + 0.5)\n",
        "                plot_sammons.y_range=Range1d(vor.min_bound[1] - 0.5, vor.max_bound[1] + 0.5)\n",
        "                plot_sammons.add_layout(color_bar, 'right')\n",
        "\n",
        "                sammons_tabs[i] = Panel(child=plot_sammons, title=\"{}\".format(sammons_num_cols[i]))\n",
        "\n",
        "            # creating the tabs having plots\n",
        "            sammons_plt_tab = Tabs(tabs = sammons_tabs)\n",
        "\n",
        "            # displaying the tabs with plot\n",
        "            sammons_plot_layout = column(sammons_plt_tab)\n",
        "            clear_output()\n",
        "\n",
        "            display(Markdown('__Sammon\\'s projection plots generated!__'))\n",
        "            script, div = components(sammons_plot_layout)\n",
        "\n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        # display the error\n",
        "        clear_output()\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "    else:\n",
        "        display(HTML(html_plot.render(script=script, div=div)))\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ZSIYAhYywf"
      },
      "source": [
        "> __Notes__:\n",
        " \n",
        "```\n",
        "*Add notes here*\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HulLaWCYywg"
      },
      "source": [
        "## Parallel Coordinate Plot (PCP)\n",
        "\n",
        "The parallel coordinate plot or PCP is designed to visually identify clusters and patterns in multi-dimensional variable space. Each variable is represented as a (parallel) axis, and each observation consists of a line that connects points on the axes. Clusters consist of groups of lines (i.e., observations) that follow a similar path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LDOR11vYywg"
      },
      "source": [
        "##### For Numerical Columns\n",
        "\n",
        "When lines between two parallel axes are somewhat parallel to each other, it suggests a positive relationship between these two dimensions. When lines cross in a kind of superposition of X-shapes, it's a negative relationship. When lines cross randomly or are parallel, it shows there is no particular relationship. Outliers in a PCP are lines that show a very different pattern from the rest.\n",
        "\n",
        "Features:\n",
        "* We can select a section of a numerical column value and toggle it up and down the axis to highlight the path traversed \n",
        "* Re-ordering the axes can help in discovering patterns or correlations across variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:42.766232Z",
          "start_time": "2020-12-14T14:40:33.320220Z"
        },
        "scrolled": false,
        "id": "CVWa8ln8Yywg"
      },
      "source": [
        "# generating parallel coordinate plot for numerical columns\n",
        "value=\"PCP for num cols\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # input column name for color attribute\n",
        "        num_cols_vis = ' || '.join(num_cols)\n",
        "        print(colored(\"Numerical Columns:\",'magenta',attrs=['bold']),\"\\n{}\".format(num_cols_vis))\n",
        "        \n",
        "        while True:\n",
        "            col_color = input('\\nEnter the continuous column to be used for color distribution in the plot: ')\n",
        "            if col_color in num_cols:\n",
        "                break\n",
        "            else:\n",
        "                print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "                continue\n",
        "        \n",
        "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
        "        fig = px.parallel_coordinates(df[num_cols], color=col_color,\n",
        "                                         color_continuous_scale=px.colors.diverging.Tealrose,\n",
        "                                         #change the range based on the column specified as input\n",
        "                                         range_color=(0,1), color_continuous_midpoint=2,\n",
        "                                         height=600,width=2500)\n",
        "\n",
        "        clear_output()\n",
        "        fig.show(config={'displaylogo': False})\n",
        "        \n",
        "        track_cell(value, flag)\n",
        "    except Exception as err:\n",
        "        clear_output()\n",
        "        # display the error\n",
        "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
        "        flag = 0\n",
        "        err = str(err)\n",
        "        track_cell(value, flag, err)\n",
        "        \n",
        "else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "446etGVzYywg"
      },
      "source": [
        "##### For Categorical Columns\n",
        "\n",
        "This plot lets you identify the relationship between various categorical variables and trace their journey.\n",
        "\n",
        "Features:\n",
        "* We can drag the columns both vertically and horizontally to reorder the columns or their values\n",
        "* Place the cursor on a value on any axis to trace its path\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:40:54.340344Z",
          "start_time": "2020-12-14T14:40:48.284001Z"
        },
        "scrolled": false,
        "id": "ohmMLg9vYywh"
      },
      "source": [
        "\n",
        "# input column name for color attribute\n",
        "num_cols_vis = ' || '.join(num_cols)\n",
        "print(colored(\"Numerical Columns:\",'magenta',attrs=['bold']),\"\\n{}\".format(num_cols_vis))\n",
        "\n",
        "while True:\n",
        "    col_color_cat = input('\\nEnter the continuous column to be used for color distribution in the plot: ')\n",
        "    if col_color_cat in num_cols:\n",
        "        break\n",
        "    else:\n",
        "        print(colored('Please enter column name properly.','red',attrs=['bold']))\n",
        "        continue\n",
        "\n",
        "display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
        "# selecting a non-decimal column name for the colors\n",
        "fig = px.parallel_categories(df, color=col_color_cat,\n",
        "                              color_continuous_scale=px.colors.sequential.ice,\n",
        "                              color_continuous_midpoint=6, \n",
        "                              #change the range based on the column specified as input\n",
        "                              range_color=(0,1),\n",
        "                              height=600)\n",
        "\n",
        "clear_output()\n",
        "fig.show(config={'displaylogo': False})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_bIojsYywh"
      },
      "source": [
        "***\n",
        "\n",
        "# Executive Summary\n",
        "\n",
        "(SAMPLE)\n",
        "\n",
        "In this notebook, we explored `hotel_new` data, where we performed some basic formatting, checking if there is any duplicates which have to be removed, obtained the list of numerical and categorical columns. Before performing any analysis, we checked for missing values, and observed that for this data, we are getting almost **21% of data for column `building_type`** have missing values. It's common sense that for one column having so much missing values, there was no need to drop those many rows of other columns. *Hence we removed the column `building_type`*.\n",
        "\n",
        "Now, we performed univariate outlier analysis using 3 different techniques to observe the number of outliers for column occupancy, and it was noticed that very few hotels have high occupancy, irrespective of the location_type or class. However, using multivariate outlier analysis, we obtained a very different result - which was expected. Irrespective of the amount of occupancy, most of the hotels in DOWNTOWN `location_type` with bad class falls under outliers.\n",
        "\n",
        "From histogram, it was interesting to notice that amount of booking via call is more than web. Also, `occupancy `showed normal distribution. In the categorical variable, there are more **bad** ratings, and most of the hotels lie at the **SUBURBAN** location.\n",
        "\n",
        "From correlation matrix, we saw that `percentleisure` and `percentbusiness` are perfectly negatively correlated. Also, `direct_call_booking_pct`-`direct_web_booking_pct`, and `direct_call_booking_pct`-`call_center_booking_pct` are also negatively correlated, which is expected. Also, we noticed that there is a positive correlation of almost similar value between `occupancy`-`compet_occupancy` and `avgdailyrate`-`compet_avgdailyrate`. Based on these, we generated scatter plot, from which we can get an idea of possible outliers, although not explicitly highlighted.\n",
        "\n",
        "We observed that `class` and `location_type` are dependent, and **AIRPORT** based locality are the best class.\n",
        "\n",
        "Using Trellis plot, we observed that `occupancy` and `compet_occupancy` holds a positively collinear relationship with respect to the quantile segmentation of `avgdailyrate`.\n",
        "\n",
        "While performing factor and cluster analysis, we dropped a certain number of columns that is causing singularity. Once they are removed, we first analysed that whether we can go ahead with factor analysis by performing Adequecy Test and it passed. Using Scree Plot, we obtained the **optimal number of factors - 10**, and performed factor loading, where we observed that `occupancy`, `compet_occupancy` and `class` can be coupled to form one factor, `avgdailyrate` and `compet_avgdailyrate` into another, and various modes of booking into one factor. This can significantly optimize our features. We also noticed that significantly less amount of categories of categorical data doesn't fir neatly with the factors using uniqueness.\n",
        "\n",
        "In cluster analysis, the **optimal number of clusters obtained is 3** based on Silhouette Score but they are not equivalently distributed. Most of the highly left skewed columns are contributing least to the cluster #1 apart from the `occupancy` and `compet_occupancy` i.e., most of the data likely to be an outlier for these columns is in this cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBnGwG4IYywh"
      },
      "source": [
        "***\n",
        "\n",
        "# Generating HTML Report\n",
        "\n",
        "Execute the __following two cells__ to generate HTML report of this notebook. It will __not display any code cells__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:47:30.286195Z",
          "start_time": "2020-12-14T14:47:30.280366Z"
        },
        "id": "k0RADzYVYywh"
      },
      "source": [
        "%%javascript\n",
        "IPython.notebook.kernel.execute(`notebook_name = '${IPython.notebook.notebook_name}'`);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-14T14:47:33.199290Z",
          "start_time": "2020-12-14T14:47:31.126379Z"
        },
        "id": "wmZvTZbCYywi"
      },
      "source": [
        "notebook_path = os.getcwd()+'\\{}'.format(notebook_name)\n",
        "x = os.system('jupyter nbconvert \"{}\" --no-input --no-prompt --template toc2 --to=html'.format(notebook_path))\n",
        "if x != 0:\n",
        "    x = os.system('jupyter nbconvert \"{}\" --no-input --no-prompt --to html'.format(notebook_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96DJ0iPBYywi"
      },
      "source": [
        "***\n",
        "\n",
        "# Appendix\n",
        "\n",
        "## Understanding type of data <a name='data_types'></a>\n",
        "\n",
        "Data is a transmissible and storable information collected together for reference or statistical analysis. Data can be categorized as follows:\n",
        "\n",
        "1. __Structured Data__ - Tabular data that normally reside in a relational database or data warehouse having a fixed schema\n",
        "    1. __Qualitative Data (Categorical)__\n",
        "        1. __Nominal Data__ - Unordered, categories which are mutually exclusive (`male`/`female`, `pass`/`fail`, `black`/`brown`/`white`)\n",
        "        2. __Ordinal Data__ - Ordered, categories representing a score or range of value (`less likely` - `likely` - `very likely`, $1^{st}$-$2^{nd}$-$3^{rd}$)\n",
        "    2. __Quantitative Data (Numeric)__\n",
        "        1. __Discrete Data__ - Finite count of numbers/decimals/ratio\n",
        "        2. __Continuous Data__ - Value within a range of number/decimals/ratio\n",
        "2. _Unstructured Data_ - Images, Video, Audio\n",
        "\n",
        "In this notebook, we will be exploring various techniques of EDA that can be implemented in __structured data__.\n",
        "\n",
        "[...back](#intro)\n",
        "\n",
        "## Hypothetical Example of Cluster Analysis <a name='clus_eg'></a>\n",
        "\n",
        "The nature of cluster analysis will be illustrated by a simple example involving the identification of customer segments in a retail setting.\n",
        "\n",
        "Suppose a marketing researcher wishes to determine __market segments__ in a community based on patterns of loyalty to brands and stores. A small sample of 7 respondents is selected as a pilot test of how cluster analysis is applied. Two measures of loyalty, __$V_1$ (store loyalty)__ and __$V_2$ (brand loyalty)__, were measured for each respondent on a 0–10 scale. The values for each of the 7 respondents are shown in the table below, along with a scatter diagram depicting each observation on the 2 variables.\n",
        "\n",
        "Clustering Variables | Resp. A | Resp. B | Resp. C | Resp. D | Resp. E | Resp. F | Resp. G |\n",
        ":-------------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:\n",
        "$V_1$ | 3 | 4 | 4 | 2 | 6 | 7 | 6\n",
        "$V_2$ | 2 | 5 | 7 | 7 | 6 | 7 | 4\n",
        "\n",
        "![](https://github.com/ml-nest/EDA/blob/master/Source/sample_cluster_analysis_1.png?raw=1)\n",
        "\n",
        "The primary objective of cluster analysis is to define the structure of the data by placing the most similar observations into groups. To accomplish this task, we must address three basic questions:\n",
        "\n",
        "1. __How do we measure the similarity?__ We require a method of simultaneously comparing observations on the two clustering variables ($V_1$ and $V_2$). Several methods are possible, including the correlation between objects or perhaps a measure of their proximity in 2D space such that the distance between observations indicates similarity.\n",
        "2. __How do we form clusters?__ No matter how similarity is measured, the procedure must group those observations that are most similar into a cluster, thereby determining the cluster group membership of each observation for each set of clusters formed.\n",
        "3. __How many groups do we form?__ The final task is to select one set of clusters as the final solution. In doing so, we face a trade-off: _fewer clusters and less within-group homogeneity_ versus a _larger number of clusters and more within-group homogeneity_. A simple structure is reflected in a few clusters. _Yet as the number of clusters decreases, the heterogeneity within the clusters necessarily increases._ Thus, a balance must be made between defining the most basic structure (fewer clusters) that still achieves an acceptable level of heterogeneity between the clusters.\n",
        "\n",
        "![](https://github.com/ml-nest/EDA/blob/master/Source/sample_cluster_analysis_2.png?raw=1)\n",
        "\n",
        "[...back](#cluster)\n",
        "\n",
        "## Hypothetical Example of Factor Analysis <a name='fact_eg'></a>\n",
        "\n",
        "An illustrative example of a simple application of factor analysis is shown in the figure below, which represents the correlation matrix for __9 different characteristics of retail stores and their services__. which includes product offering, store personnel, price levels, and in-store service and experiences. The question a researcher may wish to address is: _Are all of these elements separate in their evaluative properties or do they group into some more general areas of evaluation?_\n",
        "\n",
        "Visual inspection of the original correlation matrix (__Part 1__) does not easily reveal any specific pattern. Among scattered high correlations, _variable groupings are not apparent_. However, the application of factor analysis results in the grouping of variables as reflected in __Part 2__.\n",
        "\n",
        "![](https://github.com/ml-nest/EDA/blob/master/Source/sample_factor_analysis.png?raw=1)\n",
        "\n",
        "Here some interesting patterns emerge. First, 4 variables ($V_3$, $V_8$, $V_9$, $V_2$) relating to the __in-store experience__ of shoppers are grouped together. Then, 3 variables ($V_6$, $V_7$, $V_4$) describing the __product assortment and availability__ are grouped together. Finally, product quality and price levels are grouped. \n",
        "\n",
        "Each group represents a set of highly interrelated variables that may reflect a more general evaluative dimension. In this case, we might label the three variable _groupings by the labels in-store experience, product offerings, and product value_.\n",
        "\n",
        "[...back](#factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQNJE2NYywi"
      },
      "source": [
        "<div style=\"text-align: right;\">\n",
        "    <i>&copy; 2020 Copyright Mu Sigma Inc.</i>\n",
        "</div>"
      ]
    }
  ]
}