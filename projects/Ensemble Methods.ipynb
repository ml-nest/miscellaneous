{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Wisdom of the crowd or averaging results or taking a majority vote is way better than going for a single best thing. That is why ensemble prediction is way better than a single models output.\n",
    "A group of predictors is called an ensemble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have multiple predictors with around 80% accuracy. We can be sure that false positive and negatives of all these predictors are not common, so just create a majority voting between them will result in a model that is correct more than 80% of the time. This is called hard voting.\n",
    "<br>This works best when the predictors are as diverse as possible because they will make different mistakes than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img.PNG\">\n",
    "<img src=\"img2.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "LogisticRegression 0.9710144927536232\n",
      "RandomForestClassifier 1.0\n",
      "SVC 0.9420289855072463\n",
      "VotingClassifier 0.9855072463768116\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver = 'lbfgs',max_iter=100000)\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard')\n",
    "\n",
    "print(data.data.shape)\n",
    "\n",
    "X_train = data.data[:500]\n",
    "y_train = data.target[:500]\n",
    "\n",
    "X_test = data.data[500:]\n",
    "y_test = data.target[500:]\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the predictors are able to predict class probabilties instead of 0 or 1, we can call upon soft voting where probabilties are averaged out for all the predictors and the highest one is selected. Gives better result than hard voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9710144927536232\n",
      "RandomForestClassifier 0.9855072463768116\n",
      "GaussianNB 0.9710144927536232\n",
      "VotingClassifier 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf1 = LogisticRegression(multi_class='multinomial', random_state=1,max_iter=100000)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "       ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "       voting='soft', weights=[2,1,1],\n",
    "       flatten_transform=True)\n",
    "\n",
    "eclf3.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (clf1, clf2, clf3, eclf3):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9710144927536232\n",
      "RandomForestClassifier 0.9855072463768116\n",
      "SVC 0.9420289855072463\n",
      "VotingClassifier 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "log_clf = LogisticRegression(solver = 'lbfgs',max_iter=100000)\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using multiple algorithms on the same dataset to give better accuracy, we can use the same algorithm and train it on random subsets of dataset to produce predictors.\n",
    "<br>When these subsets are made with replacement they are called bagging(boostrap aggregating) and when they are made without replacement it is called pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img3.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is aggregated as a majority vote in case of classification and an average in case of regression.\n",
    "<br> Individual predictors have a huge bias but the aggregated result has a reduced bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skit-learn example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855072463768116"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),  # model to be used\n",
    "    n_estimators=500,  # number of predictors\n",
    "    max_samples=0.7, # number of rows in the subset, float between 0 and 1 signifies the portion of the dataset to be sampled\n",
    "    bootstrap=True,  # using bagging\n",
    "    n_jobs=-1  # using all the cores\n",
    "    )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9710144927536232"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Pasting\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),  # model to be used\n",
    "    n_estimators=500,  # number of predictors\n",
    "    max_samples=0.7, # number of rows in the subset, float between 0 and 1 signifies the portion of the dataset to be sampled\n",
    "    bootstrap=False,  # using pasting\n",
    "    n_jobs=-1  # using all the cores\n",
    "    )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though the accuracy remains same between 1 tree vs 500 trees, the descision boudary seems much more generalized\n",
    "<img src=\"img4.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training the DT's on a subset of data, the remaining data is called out of bag dataset and the DT is not trained on this so can be leveraged to calculate accuracy. Used when the training dataset is not big enough, to not keep a validaion dataset aside and reduce the length of the dataset.\n",
    "<br> If one row is taken from OOB sample and tested on the DT's which were not trained on this. They will return an accuracy of how many of them were able to predict it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                            n_estimators=500,\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1,\n",
    "                            oob_score=True  # calculating the oob_score for each predictor\n",
    "                           )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This OOB score when observed for individual DT can be seen in the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86734694, 0.13265306],\n",
       "       [0.99375   , 0.00625   ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88636364, 0.11363636],\n",
       "       [0.83146067, 0.16853933],\n",
       "       [0.69886364, 0.30113636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96216216, 0.03783784],\n",
       "       [0.93641618, 0.06358382],\n",
       "       [0.94472362, 0.05527638],\n",
       "       [0.78977273, 0.21022727],\n",
       "       [1.        , 0.        ],\n",
       "       [0.85416667, 0.14583333],\n",
       "       [0.6402439 , 0.3597561 ],\n",
       "       [0.92857143, 0.07142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97252747, 0.02747253],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01587302, 0.98412698],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.79885057, 0.20114943],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97142857, 0.02857143],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78333333, 0.21666667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93717277, 0.06282723],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7606383 , 0.2393617 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43010753, 0.56989247],\n",
       "       [0.72674419, 0.27325581],\n",
       "       [0.01530612, 0.98469388],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91612903, 0.08387097],\n",
       "       [0.45251397, 0.54748603],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9127907 , 0.0872093 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16363636, 0.83636364],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9760479 , 0.0239521 ],\n",
       "       [0.89795918, 0.10204082],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99537037, 0.00462963],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [0.02197802, 0.97802198],\n",
       "       [0.01724138, 0.98275862],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.97647059, 0.02352941],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.        , 1.        ],\n",
       "       [0.53107345, 0.46892655],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26368159, 0.73631841],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [0.98492462, 0.01507538],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03061224, 0.96938776],\n",
       "       [0.73417722, 0.26582278],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21714286, 0.78285714],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06914894, 0.93085106],\n",
       "       [0.41847826, 0.58152174],\n",
       "       [0.18686869, 0.81313131],\n",
       "       [0.5862069 , 0.4137931 ],\n",
       "       [0.23157895, 0.76842105],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.73134328, 0.26865672],\n",
       "       [0.69387755, 0.30612245],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04347826, 0.95652174],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82840237, 0.17159763],\n",
       "       [0.01970443, 0.98029557],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05988024, 0.94011976],\n",
       "       [0.66853933, 0.33146067],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02617801, 0.97382199],\n",
       "       [0.02247191, 0.97752809],\n",
       "       [0.00609756, 0.99390244],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94358974, 0.05641026],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.05454545, 0.94545455],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.55445545, 0.44554455],\n",
       "       [0.94736842, 0.05263158],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99390244, 0.00609756],\n",
       "       [0.43157895, 0.56842105],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03571429, 0.96428571],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95698925, 0.04301075],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.57142857, 0.42857143],\n",
       "       [0.19270833, 0.80729167],\n",
       "       [0.74431818, 0.25568182],\n",
       "       [0.05263158, 0.94736842],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.05769231, 0.94230769],\n",
       "       [0.32768362, 0.67231638],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05418719, 0.94581281],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.79187817, 0.20812183],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01530612, 0.98469388],\n",
       "       [0.85945946, 0.14054054],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04651163, 0.95348837],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04469274, 0.95530726],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96491228, 0.03508772],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.01775148, 0.98224852],\n",
       "       [0.        , 1.        ],\n",
       "       [0.78021978, 0.21978022],\n",
       "       [0.65968586, 0.34031414],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01570681, 0.98429319],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00584795, 0.99415205],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03954802, 0.96045198],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94505495, 0.05494505],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63483146, 0.36516854],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84357542, 0.15642458],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9368932 , 0.0631068 ],\n",
       "       [0.45679012, 0.54320988],\n",
       "       [0.04166667, 0.95833333],\n",
       "       [0.51578947, 0.48421053],\n",
       "       [0.93103448, 0.06896552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97237569, 0.02762431],\n",
       "       [0.7027027 , 0.2972973 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86956522, 0.13043478],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96470588, 0.03529412],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.55721393, 0.44278607],\n",
       "       [0.        , 1.        ],\n",
       "       [0.74850299, 0.25149701],\n",
       "       [0.23595506, 0.76404494],\n",
       "       [0.32258065, 0.67741935],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93922652, 0.06077348],\n",
       "       [0.80225989, 0.19774011],\n",
       "       [0.97297297, 0.02702703],\n",
       "       [0.56725146, 0.43274854],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97969543, 0.02030457],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.29189189, 0.70810811],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13661202, 0.86338798],\n",
       "       [0.11242604, 0.88757396],\n",
       "       [0.93650794, 0.06349206],\n",
       "       [1.        , 0.        ],\n",
       "       [0.13407821, 0.86592179],\n",
       "       [0.01546392, 0.98453608],\n",
       "       [0.97790055, 0.02209945],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02197802, 0.97802198],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95287958, 0.04712042],\n",
       "       [0.26395939, 0.73604061],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10526316, 0.89473684],\n",
       "       [0.05820106, 0.94179894],\n",
       "       [0.06282723, 0.93717277],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2       , 0.8       ],\n",
       "       [0.14371257, 0.85628743],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42592593, 0.57407407],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.97206704, 0.02793296],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70351759, 0.29648241],\n",
       "       [0.995     , 0.005     ],\n",
       "       [0.53157895, 0.46842105],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.22      , 0.78      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86206897, 0.13793103],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0257732 , 0.9742268 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27173913, 0.72826087],\n",
       "       [0.45303867, 0.54696133],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.12105263, 0.87894737],\n",
       "       [0.01639344, 0.98360656],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02298851, 0.97701149],\n",
       "       [0.01754386, 0.98245614],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06321839, 0.93678161],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02139037, 0.97860963],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03553299, 0.96446701],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92391304, 0.07608696],\n",
       "       [0.00952381, 0.99047619],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93229167, 0.06770833],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.359375  , 0.640625  ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23626374, 0.76373626],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.1043956 , 0.8956044 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01522843, 0.98477157],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04761905, 0.95238095],\n",
       "       [0.00497512, 0.99502488],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8172043 , 0.1827957 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.08045977, 0.91954023],\n",
       "       [0.94594595, 0.05405405],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.16326531, 0.83673469],\n",
       "       [0.10526316, 0.89473684],\n",
       "       [0.        , 1.        ],\n",
       "       [0.75144509, 0.24855491],\n",
       "       [0.08205128, 0.91794872],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01796407, 0.98203593],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15      , 0.85      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38586957, 0.61413043],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11666667, 0.88333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.59770115, 0.40229885],\n",
       "       [0.03092784, 0.96907216],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21052632, 0.78947368],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61988304, 0.38011696],\n",
       "       [0.52941176, 0.47058824],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01604278, 0.98395722],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.51396648, 0.48603352],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03061224, 0.96938776],\n",
       "       [0.25698324, 0.74301676],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01020408, 0.98979592],\n",
       "       [0.98870056, 0.01129944],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02173913, 0.97826087],\n",
       "       [0.93989071, 0.06010929],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00598802, 0.99401198],\n",
       "       [0.005     , 0.995     ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24479167, 0.75520833],\n",
       "       [0.98039216, 0.01960784],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98802395, 0.01197605],\n",
       "       [0.1005291 , 0.8994709 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10614525, 0.89385475],\n",
       "       [0.21341463, 0.78658537],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0104712 , 0.9895288 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05524862, 0.94475138],\n",
       "       [0.03763441, 0.96236559],\n",
       "       [0.        , 1.        ],\n",
       "       [0.17112299, 0.82887701],\n",
       "       [0.29473684, 0.70526316],\n",
       "       [0.08602151, 0.91397849],\n",
       "       [0.03867403, 0.96132597],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03825137, 0.96174863],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.35428571, 0.64571429],\n",
       "       [0.00617284, 0.99382716],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.44808743, 0.55191257],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27607362, 0.72392638],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.06666667, 0.93333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24157303, 0.75842697],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09375   , 0.90625   ],\n",
       "       [0.03902439, 0.96097561],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.453125  , 0.546875  ],\n",
       "       [0.04142012, 0.95857988],\n",
       "       [0.12637363, 0.87362637],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.03684211, 0.96315789],\n",
       "       [0.54973822, 0.45026178],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09883721, 0.90116279],\n",
       "       [0.12690355, 0.87309645],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB score is calculated using a subset of DT's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_  # this is likable accuracy for future test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Patches** - When both the training instances(rows) and training features are varied. Useful in images with high dimesionality\n",
    "<br>**Random Subspaces** - Only number of features is varied and all the rows are taken into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees trained on the random subsets when ensembled together using bagging to give one output.\n",
    "<br> We can use random forest regressor for regression with the same technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike DT, RF searches the feature to split a node from a subset of features and not all of them. This increases bias and reduce variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DT, the nodes are broken down using the best possible threshold. When we randomize this threshold in a random forest we get Extremely Randomized Trees(Extra-Trees). This is faster than RF as time consumed to find the threshold is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius 0.040822706311416025\n",
      "mean texture 0.014522751655622114\n",
      "mean perimeter 0.051239837392426685\n",
      "mean area 0.041385647557718286\n",
      "mean smoothness 0.005554177731460974\n",
      "mean compactness 0.015323090716651987\n",
      "mean concavity 0.04629303062964822\n",
      "mean concave points 0.09864868398068713\n",
      "mean symmetry 0.0028658193132507716\n",
      "mean fractal dimension 0.004581219074849208\n",
      "radius error 0.014525286295540833\n",
      "texture error 0.0035896656859457417\n",
      "perimeter error 0.01377513734700074\n",
      "area error 0.03382974100438362\n",
      "smoothness error 0.003545307096423924\n",
      "compactness error 0.0035029955846609443\n",
      "concavity error 0.005170196620826863\n",
      "concave points error 0.004836041458759045\n",
      "symmetry error 0.0033436996327992526\n",
      "fractal dimension error 0.00489638439827724\n",
      "worst radius 0.10404617374495274\n",
      "worst texture 0.015579194675895663\n",
      "worst perimeter 0.14855747319295562\n",
      "worst area 0.106203294137047\n",
      "worst smoothness 0.017652388374837766\n",
      "worst compactness 0.011363815775368514\n",
      "worst concavity 0.0306504027416024\n",
      "worst concave points 0.1363121796320713\n",
      "worst symmetry 0.009669067399726446\n",
      "worst fractal dimension 0.007714590837192961\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(data[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worst radius, perimerter, area, concave points are higly important features in this forest. The features closer to roots are important and the ones closer to leaves are unimportant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble method that can combine several weak learners to make a strong learner in a sequential manner correcting its predecessors mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique where model weights predictor weights are changed sequentially to fit the misclassified training instances. Thus, focusing on misclassified instances.\n",
    "<img src=\"img5.PNG\">\n",
    "After training all the predictors are used to give a vote, with a weight attached to each of their vote, based on their training accuracy.\n",
    "<br><br><br>\n",
    "**Explaining Algorithm**\n",
    "<br>Let there be 100 instances for which the model needs to be trained. Initially set weights for all of them as 1/100.\n",
    "<br>Evaluate your model and come up with this score.\n",
    "<img src=\"img6.PNG\">\n",
    "<br>This error rate is evaluated for a predictor. Very obviously if there is some weight that needs to be given to the predictor, this quantity should be inversely related.\n",
    "<img src=\"img7.PNG\">\n",
    "This is the weight that is directly attached to the predictor while bagging the final predictions. Here, N is the learning rate.\n",
    "<br>Finally the weights of those 100 rows are updated using the below equation.\n",
    "<img src=\"img8.PNG\">\n",
    "If the output is giving good accuracy the alpha will be greater and more focus in fixing weights for misclassified rows. All the weights are normalized back to 1 and this process will run in a loop until the desired predictors are reached or perfect predictor is found.\n",
    "<br>The prediction is made based on majority vote by their weighted predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855072463768116"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),  # also called as decision stump, 1 node 2 leaves\n",
    "    n_estimators=200,\n",
    "    algorithm=\"SAMME.R\",  # Stagewise Additive Modeling using a Multiclass Exponential loss function.\n",
    "                          # R(Real) when predict_proba() can be calculated\n",
    "    learning_rate=0.5\n",
    "    )\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}