{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:block\">\n",
    "    <div style=\"width: 10%; display: inline-block; text-align: left;\">\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/en/a/ad/Naruto_-_Shippuden_DVD_season_1_volume_1.jpg\" style=\"height:75px; margin-left:0px\" />\n",
    "    </div>\n",
    "    <div style=\"width: 69%; display: inline-block\">\n",
    "        <h5  style=\"color:maroon; text-align: center; font-size:25px;\">Bricks - Exploratory Data Analysis for Panel Data in Python - v20.06.03</h5>\n",
    "        <div style=\"width: 90%; text-align: center; display: inline-block;\"><i>Author(s): </i> <strong>Naruto</strong> </div>\n",
    "    </div>\n",
    "    <div style=\"width: 20%; text-align: right; display: inline-block;\">\n",
    "        <div style=\"width: 100%; text-align: left; display: inline-block;\">\n",
    "            <i>Modified: June 100th, 2200</i>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T16:16:36.210199Z",
     "start_time": "2020-01-14T16:16:36.205423Z"
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an approach to analyzing data sets and summarizing their main characteristics with visualizations. It is an essential step before modeling in any data analytics project. In order to better understand the data associated with the problem, we need to perform certain activities to ensure that we get relevant insights and decide on the appropriate next steps.\n",
    "\n",
    "This notebook can handle two different types of data:\n",
    "\n",
    "* __Time-Series data__ is a collection of observations(behavior) for a __single subject__(entity) at different time intervals(generally equally spaced).\n",
    "* __Panel data__ is basically a __cross-sectional time-series data__ as it a collection of observations for __multiple subjects__ at multiple instances (sequence of time).\n",
    "\n",
    "## Installing the necessary packages (Mandatory)\n",
    "\n",
    "Please ensure all the packages are installed in your system. Follow the instructions in __README__ file before executing any cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Functions (for aesthetics)\n",
    "\n",
    "The following chunks of codes help improve the usability of the notebook. Please make sure you execute these codes before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell is a function that helps __highlight negative values__ in red color and rest as black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:11:05.489868Z",
     "start_time": "2020-06-04T09:11:05.477919Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the function \"color_negative_red\" which color-code values across Notebook\n",
    "\n",
    "from IPython.display import HTML, display, Markdown, clear_output\n",
    "\n",
    "def color_negative_red(val):\n",
    "    '''\n",
    "    Function to color-code negative values with red and others as black.\n",
    "    \n",
    "    input: \n",
    "        val : value as string\n",
    "    return: \n",
    "        `color: red` or `color: black`\n",
    "    '''\n",
    "    \n",
    "    color = 'black'\n",
    "    if type(val) == int or type(val) == float:\n",
    "        color = 'red' if val < 0 else 'black'\n",
    "    else:\n",
    "        pass\n",
    "    return 'color: {}'.format(color)\n",
    "\n",
    "display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #1 for <b>color-coded highlights</b> is EXECUTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell enables __suppressing warnings__ that can get generated across the Notebook and __initiating seed__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:11:09.409034Z",
     "start_time": "2020-06-04T09:11:09.405162Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c80a4202e27c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m369\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #2 for <b>suppressing warning</b> is EXECUTED!</span>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "# importing libraries for setting random seed and supress warnings\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# supress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# setting random seed - 369 in this case\n",
    "random.seed(369)\n",
    "\n",
    "display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #2 for <b>suppressing warning</b> is EXECUTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell __center aligns__ the plots and images generated in the notebook console and creates a __loading symbol__ to indicate a cell processing to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:11:11.059557Z",
     "start_time": "2020-06-04T09:11:11.055243Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the notebook output to be in the center and creating loading symbol\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".output_png img {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "}\n",
    " \n",
    ".loader {\n",
    "  border: 5px solid #f3f3f3;\n",
    "  border-radius: 50%;\n",
    "  border-top: 5px solid teal;\n",
    "  border-right: 5px solid grey;\n",
    "  border-bottom: 5px solid maroon;\n",
    "  border-left: 5px solid tan;\n",
    "  width: 20px;\n",
    "  height: 20px;\n",
    "  -webkit-animation: spin 1s linear infinite;\n",
    "  animation: spin 1s linear infinite;\n",
    "  float: left;\n",
    "}\n",
    "\n",
    "@-webkit-keyframes spin {\n",
    "  0% { -webkit-transform: rotate(0deg); }\n",
    "  100% { -webkit-transform: rotate(360deg); }\n",
    "}\n",
    "\n",
    "@keyframes spin {\n",
    "  0% { transform: rotate(0deg); }\n",
    "  100% { transform: rotate(360deg); }\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #3 for <b>image and table aesthetics</b> is EXECUTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell will enable to keep __track of the activities__ performed across the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell will help __generating bokeh plots in the HTML report__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:11:15.027412Z",
     "start_time": "2020-06-04T09:11:14.402012Z"
    }
   },
   "outputs": [],
   "source": [
    "# enabling bokeh plots to be rendered in HTML report\n",
    "\n",
    "from jinja2 import Template\n",
    "from bokeh.embed import components\n",
    "\n",
    "html_plot = Template(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en-US\">\n",
    "\n",
    "<link\n",
    "    href=\"http://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.css\"\n",
    "    rel=\"stylesheet\" type=\"text/css\"\n",
    ">\n",
    "<script src=\"http://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\"></script>\n",
    "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-1.4.0.min.js\"></script>\n",
    "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-tables-1.4.0.min.js\"></script>\n",
    "\n",
    "<body>\n",
    "    {{ script }}\n",
    "    {{ div }}\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "\"\"\")\n",
    "\n",
    "display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #5 for <b>embedding Bokeh plots in HTML</b> is EXECUTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell is a function that will help to __display dataframes using Plotly with scrollable rows__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:11:16.992785Z",
     "start_time": "2020-06-04T09:11:16.440069Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def display_data(data_table):\n",
    "    data_table_series = [data_table[i] for i in data_table.columns]\n",
    "\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(data_table.columns),\n",
    "                    fill_color='grey',\n",
    "                    align='center',\n",
    "                    font=dict(color='white', size=15)\n",
    "                   ),\n",
    "        cells=dict(values=data_table_series,\n",
    "                   fill_color='lightblue',\n",
    "                   align='center',\n",
    "                   font=dict(color='black', size=10)\n",
    "                  ))\n",
    "    ])\n",
    "\n",
    "    if data_table.shape[0] <= 5:\n",
    "        fig_ht = 50*data_table.shape[0]\n",
    "    elif data_table.shape[0] > 5 and data_table.shape[0] <= 22:\n",
    "        fig_ht = 20*data_table.shape[0]\n",
    "    else:\n",
    "        fig_ht = 500\n",
    "    \n",
    "    fig.update_layout(width=150*len(data_table.columns), \n",
    "                      height=fig_ht,\n",
    "                      margin=dict(l=0,r=0,b=0,t=0,pad=0))\n",
    "    fig.show(config={'displaylogo': False})\n",
    "    \n",
    "display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Prerequisite Code #6 for <b>displaying dataframes using Plotly</b> is EXECUTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Import libraries \n",
    "\n",
    "## Importing libraries for data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T10:25:35.876722Z",
     "start_time": "2020-06-04T10:25:35.694790Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the following libraries for data loading and processing\n",
    "value= \"Import Libraries for processing\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; IMPORTING libraries for data loading and processing</h2></div>'))\n",
    "    \n",
    "    from math import ceil, sqrt, pi, isnan\n",
    "    from statistics import mean\n",
    "    from pathlib import Path\n",
    "    from itertools import combinations, groupby, product\n",
    "    from io import StringIO\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.options.display.float_format = '{:.4f}'.format\n",
    "    \n",
    "    from scipy import stats\n",
    "    from scipy.signal import periodogram\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from statsmodels.stats.anova import anova_lm\n",
    "    from statsmodels.stats.multicomp import MultiComparison\n",
    "    from statsmodels.tsa.stattools import acf, pacf, grangercausalitytests, coint\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    from statsmodels.tsa.api import VAR\n",
    "\n",
    "    from factor_analyzer import FactorAnalyzer, Rotator\n",
    "    from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "    from statsmodels.multivariate.factor import Factor, FactorResults\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "     \n",
    "    import pymssql\n",
    "    import psycopg2\n",
    "    import pandas.io.sql as psql\n",
    "    \n",
    "    from arch.unitroot import ADF, KPSS, VarianceRatio, PhillipsPerron\n",
    "    \n",
    "    from spectrum import aryule, AIC\n",
    "    \n",
    "    import concurrent.futures\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    if __name__ == '__main__':\n",
    "        clear_output()\n",
    "    \n",
    "    print(err)\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)\n",
    "else:\n",
    "    if __name__ == '__main__':\n",
    "        clear_output()\n",
    "    display(Markdown('<span style=\"color:darkgreen; font-style: bold; font-size: 15px\">All the libraries for <b>data loading and processing</b> are successfully IMPORTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries for charts and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:11:25.207472Z",
     "start_time": "2020-06-04T09:11:24.523168Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the following libraries for charts and visualization\n",
    "value= \"Import Libraries for Viz\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; IMPORTING libraries for charts and visualization</h2></div>'))\n",
    "    \n",
    "    from IPython.display import display, Markdown, HTML\n",
    "    \n",
    "    from termcolor import colored\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import gridspec, cm\n",
    "    \n",
    "    import seaborn as sns\n",
    "    \n",
    "    import plotly\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.offline import plot, iplot\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = \"notebook\"\n",
    "    \n",
    "    from bokeh.models.widgets import Panel, Tabs, TextInput, DataTable, TableColumn\n",
    "    from bokeh.io import output_file, show, reset_output, output_notebook\n",
    "    from bokeh.plotting import figure\n",
    "    from bokeh.layouts import column, row, gridplot, widgetbox\n",
    "    from bokeh.models import Div, ColumnDataSource, Plot, LinearAxis, Grid, Range1d, Band, LinearColorMapper, CategoricalColorMapper, ColorBar, FactorRange, Legend\n",
    "    from bokeh.models.glyphs import Text\n",
    "    from bokeh.models.tools import HoverTool, SaveTool, ResetTool, PanTool, WheelZoomTool, CrosshairTool\n",
    "    from bokeh.palettes import Category20\n",
    "    from bokeh.transform import factor_cmap\n",
    "    from bokeh.embed import components\n",
    "    \n",
    "    from scipy.cluster.hierarchy import dendrogram\n",
    "    \n",
    "    from dtaidistance import clustering\n",
    "    from dtaidistance import dtw\n",
    "\n",
    "    reset_output()\n",
    "    output_notebook()\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    if __name__ == '__main__':\n",
    "        clear_output()\n",
    "    print(err)\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)\n",
    "    \n",
    "else:\n",
    "    if __name__ == '__main__':\n",
    "        clear_output()\n",
    "    display(Markdown('<span style=\"color:darkgreen; font-style: bold; font-size: 15px\">All the libraries for <b>charts and visualization</b> are successfully IMPORTED!</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Import Dataset\n",
    "\n",
    "Data Loading can be done in 3 ways from different data sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset from LOCAL\n",
    "\n",
    "The user can provide __absolute__ or __relative path__ to execute the cell below for accessing the data from your __computer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:12:09.879721Z",
     "start_time": "2020-06-04T09:11:28.060519Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading the data from computer (local)\n",
    "value=\"Loading data from local\"\n",
    "\n",
    "display(Markdown('### Loading data from LOCAL'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    display(Markdown('Enter the path for CSV file and press `Enter`.'))\n",
    "    display(Markdown('E.g.- \\nFor __Ubuntu/macOS__: `./Sample_Datasets/HotelPanel_100.csv`'))\n",
    "    display(Markdown('While running on __JupyterHub__ in __Ubuntu/macOS__, place your datasets in the `Sample_Datasets` folder and enter the path in the above mentioned format.'))\n",
    "    display(Markdown('For __Windows__, replace forward slash `/` with __double__ backward slashes `\\`.'))\n",
    "    display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
    "\n",
    "    # read the data from computer\n",
    "    data_path = input('Path: ')\n",
    "\n",
    "    if data_path == '':\n",
    "        print(colored('\\nNO path entered!','red',attrs=['bold']),\n",
    "              colored('Please run this cell again to enter the path.','grey'))\n",
    "    else:\n",
    "        try:\n",
    "            # data is read\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
    "\n",
    "            # obtaining the file name from the data path provided\n",
    "            if '/' in data_path:                     # if the OS is Ubuntu or macOS\n",
    "                file_name = data_path.split('/')[-1]\n",
    "            elif '\\\\' in data_path:                  # if the OS is Windows\n",
    "                file_name = data_path.split('\\\\')[-1]\n",
    "            else:                                   # if the notebook and the data is in the same directory\n",
    "                file_name = data_path\n",
    "\n",
    "            df = pd.read_csv(data_path)\n",
    "            clear_output()\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING data from computer</h2></div>'))\n",
    "\n",
    "            track_cell(value, flag)\n",
    "        except Exception as err:\n",
    "            # display the error\n",
    "            clear_output()\n",
    "            print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "            flag = 0\n",
    "            err = str(err)\n",
    "            track_cell(value, flag, err)\n",
    "\n",
    "        else:\n",
    "            # displaying the necessary information\n",
    "            clear_output()\n",
    "            display(Markdown('File `{}` having __{} rows__ and __{} columns__ loaded.'.format(file_name, df.shape[0], df.shape[1])))\n",
    "            \n",
    "            if df.shape[0] > 2500:\n",
    "                display_data(df.head(2500))\n",
    "            else:\n",
    "                display_data(df)\n",
    "else:\n",
    "    data_path = './Sample_Datasets/HotelPanel_100.csv'\n",
    "    file_name = data_path.split('/')[-1]\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    display(Markdown('File `{}` having __{} rows__ and __{} columns__ loaded.'.format(file_name, df.shape[0], df.shape[1])))\n",
    "    display_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset from a URL\n",
    "\n",
    "Execute the cell below to access the data from a __URL__. _Link provided by the user needs to be a __direct link to the CSV__._\n",
    "\n",
    "Access the set of sample datasets [here](https://vincentarelbundock.github.io/Rdatasets/datasets.html), choose your dataset, right-click on the `CSV` hyperlink, copy the link and paste it in the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T02:51:01.146911Z",
     "start_time": "2020-04-29T02:51:01.073476Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the data from URL\n",
    "value=\"Loading data from URL\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    display(Markdown('### Loading data from URL'))\n",
    "    display(Markdown('Enter your URL and press `Enter`.'))\n",
    "    display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
    "\n",
    "    cell_name = 'load_from_url'\n",
    "\n",
    "    # library to send the request and recieve the content from URL\n",
    "    from requests import get\n",
    "\n",
    "    # once the box appears after executing this cell, enter your URL and press 'Enter'\n",
    "    enter_url = input('URL: ')\n",
    "\n",
    "    if enter_url == '':\n",
    "        print(colored('\\nNO URL entered!','red',attrs=['bold']),\n",
    "              colored('Please run this cell again to enter the URL.','grey'))\n",
    "    else:\n",
    "        try:\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; DOWNLOADING data from URL </h2></div>'))\n",
    "            # send request to obtain the data from the URL\n",
    "            data_from_url = get(enter_url)\n",
    "            clear_output()\n",
    "\n",
    "            # CSV file name obtained from online\n",
    "            file_name = enter_url.split('/')[-1]\n",
    "\n",
    "            # file that will be created in your local when the data is pulled from the URL\n",
    "            file_path = Path(os.path.join(os.getcwd(), 'Sample_Datasets', file_name))\n",
    "\n",
    "            # create the CSV file in your local and write the data pulled in it\n",
    "            with open(file_path, \"w\") as my_empty_csv:\n",
    "                pass\n",
    "\n",
    "            file_path.write_bytes(data_from_url.content)\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING the data </h2></div>'))\n",
    "            # read the data\n",
    "            df = pd.read_csv(file_path)\n",
    "            clear_output()\n",
    "\n",
    "            track_cell(value, flag)\n",
    "\n",
    "        except Exception as err:\n",
    "            clear_output()\n",
    "            # display the error\n",
    "            print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "            flag = 0\n",
    "            err = str(err)\n",
    "            track_cell(value, flag, err)\n",
    "\n",
    "        else:\n",
    "            # displaying necessary information\n",
    "            clear_output()\n",
    "            display(Markdown('File `{}` having __{} rows__ and __{} columns__ loaded.'.format(file_name, df.shape[0], df.shape[1])))\n",
    "            display_data(df)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset from a DATABASE\n",
    "\n",
    "Execute the cell below to access the data from a __Database Server__. Change the __Host IP Address and Credentials__ based on your details.\n",
    "\n",
    "Given below are example snippets to connect to MS SQL and PostgreSQL database servers. You can connect to other database servers using the same logic as below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Connection through __PostgreSQL__ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T12:30:21.959425Z",
     "start_time": "2020-03-13T12:28:11.073413Z"
    }
   },
   "outputs": [],
   "source": [
    "# # pulling data through PostgreSQL\n",
    "\n",
    "# value = \"Loading data from DATABASE (postgreSQL)\"\n",
    "\n",
    "# try:\n",
    "#     display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING data from DATABASE (POSTGRESQL) </h2></div>'))\n",
    "    \n",
    "#     #code to establish a connection to the server\n",
    "#     conn = psycopg2.connect(\"dbname='postgres' user='' host='10.1.2.60' password=''\")\n",
    "\n",
    "#     #code to fetch the dataset from the server\n",
    "#     df = psql.read_sql(\"\"\"SELECT * FROM \"msu_greco\".\"cosmo_store_table\" \"\"\", conn)\n",
    "\n",
    "#     track_cell(value, flag)\n",
    "# except Exception as err:\n",
    "#     clear_output()\n",
    "#     # display the error\n",
    "#     print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "#     flag = 0\n",
    "#     err = str(err)\n",
    "#     track_cell(value, flag, err)\n",
    "\n",
    "# else:\n",
    "#     # displaying necessary information\n",
    "#     clear_output()\n",
    "#     display(Markdown('File having __{} rows__ and __{} columns__ loaded.\\\n",
    "#     The first 10 rows are shown below:'.format(df.shape[0], df.shape[1])))\n",
    "#     display(df.head(10).style.applymap(color_negative_red).highlight_null(null_color='lightblue'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:25:02.401239Z",
     "start_time": "2020-02-06T09:25:02.396254Z"
    }
   },
   "source": [
    "2) Connection through __MS SQL__ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:58:25.474672Z",
     "start_time": "2020-02-12T17:58:25.112784Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # pulling data through ms sql\n",
    "\n",
    "# value = \"Loading data from DATABASE (MS SQL)\"\n",
    "\n",
    "# try:\n",
    "#     display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING data from DATABASE (MS SQL) </h2></div>'))  \n",
    "#     server = 'INBAAVVMMSUSQL'\n",
    "#     db = 'MSU_2016'\n",
    "#     user = ''\n",
    "#     pw = ''\n",
    "    \n",
    "#     # code to establish a connection to the server\n",
    "#     conn = pymssql.connect(server, user, pw, db)\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     #code to fetch the dataset from the server\n",
    "#     df = pd.DataFrame(cursor.fetchall())\n",
    "# #     df.columns = [desc[0] for desc in cursor.description]\n",
    "#     track_cell(value, flag)\n",
    "\n",
    "# except Exception as err:\n",
    "#     clear_output()\n",
    "#     # display the error\n",
    "#     print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "#     flag = 0\n",
    "#     err = str(err)\n",
    "#     track_cell(value, flag, err)\n",
    "\n",
    "# else:\n",
    "#     # displaying necessary information\n",
    "#     clear_output()\n",
    "#     display(Markdown('File having __{} rows__ and __{} columns__ loaded.\\\n",
    "#     The first 10 rows are shown below:'.format(df.shape[0], df.shape[1])))\n",
    "#     display(df.head(10).style.applymap(color_negative_red).highlight_null(null_color='lightblue'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Dataset Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:12:28.319035Z",
     "start_time": "2020-06-04T09:12:28.282803Z"
    }
   },
   "outputs": [],
   "source": [
    "# replacing any empty cell of data type string in any column with NaN\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "# replacing inf with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "display(Markdown('The __data summary__ is shown below:'.format(file_name)))\n",
    "print('_'*75+'\\n')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting irrelevant columns\n",
    "\n",
    "The cell below is a user-defined function created accept user input and generate a list of column names to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:12:31.796314Z",
     "start_time": "2020-06-04T09:12:31.789562Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to accept list of column names and entering column name(s) to be deleted\n",
    "\n",
    "def columns_to_delete(column_names):\n",
    "    '''\n",
    "    Function to delete column(s) based on the user input. For multiple columns, the user enters the names \n",
    "    in commma separated format.\n",
    "    It validates to check if the column name entered is available in the dataframe.\n",
    "    If incorrectly entered, it will keep on prompting the user to enter until given properply.\n",
    "    \n",
    "    input:\n",
    "        column_names : List of columns available in the dataframe\n",
    "    return:\n",
    "        col_to_drop : List of columns to be deleted\n",
    "    '''\n",
    "    col_to_drop = []\n",
    "    \n",
    "    # user will enter column names and it will keep on going for infinite loop \n",
    "    # until it is in correct format\n",
    "    while True:\n",
    "        col_to_drop_input = input(\"\\nEnter column name(s) to drop: \")\n",
    "\n",
    "        # if user enters 'None', then it will break the infinite loop and skip this operation\n",
    "        if col_to_drop_input == '':\n",
    "            break\n",
    "        else:\n",
    "            # if it is a single column entry, keep on asking for input unless correctly entered\n",
    "            if ',' not in col_to_drop_input:\n",
    "                if ' ' in col_to_drop_input:\n",
    "                    print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
    "                    continue\n",
    "                else:\n",
    "                    if col_to_drop_input in column_names:\n",
    "                        col_to_drop = [col_to_drop_input]\n",
    "                        break\n",
    "                    else:\n",
    "                        print(colored(\"\\nINCORRECT COLUMN NAME. Please try again!\",\"red\",attrs=['bold']))\n",
    "                        continue\n",
    "            # if it is multiple column entry and properly entered, then it is stored in list\n",
    "            else:\n",
    "                # split the string with ',' and convert it into list\n",
    "                # check if any element is empty, then eliminate it\n",
    "                col_to_drop = [i.strip() for i in col_to_drop_input.split(',') if i]\n",
    "\n",
    "                # if all the column names are correctly entered, then proceed\n",
    "                if all(item in column_names for item in col_to_drop):\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored(\"\\nINCORRECT COLUMN NAME(S). Please enter all the names again!\",\"red\",attrs=['bold']))\n",
    "                    continue\n",
    "                    \n",
    "    return col_to_drop\n",
    "\n",
    "display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for performing <b>column deletion</b>.</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply this function and obtain the modified dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T09:12:39.701903Z",
     "start_time": "2020-06-04T09:12:34.790695Z"
    }
   },
   "outputs": [],
   "source": [
    "# eliminate the columns\n",
    "value=\"Deleting Cols\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        display(Markdown('__PLEASE READ THE INSTRUCTION TO DELETE COLUMN BEFORE PROCEEDING!__'))\n",
    "        display(Markdown('Enter the column name(s) from the list shown _(comma `,` separated for multiple)_ and press `Enter`.'))\n",
    "        display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
    "        print('_'*75)\n",
    "\n",
    "        df_cols = df.columns\n",
    "        num_cols = df._get_numeric_data().columns.tolist()\n",
    "        cat_cols = list(set(df_cols) - set(num_cols))\n",
    "\n",
    "        # display list of numerical column names for ease\n",
    "        num_cols_vis = ' || '.join(num_cols)\n",
    "        print(colored(\"\\nNumerical Columns:\\nCount:\",'magenta',attrs=['bold']),\"{}\\n{}\".format(len(num_cols), \n",
    "                                                                                             num_cols_vis))\n",
    "        # display list of categorical column names for ease\n",
    "        cat_cols_vis = ' || '.join(cat_cols)\n",
    "        print(colored(\"\\nCategorical Columns:\\nCount:\",'blue',attrs=['bold']),\"{}\\n{}\".format(len(cat_cols), \n",
    "                                                                                              cat_cols_vis))\n",
    "\n",
    "        col_to_drop = columns_to_delete(df_cols)\n",
    "        if col_to_drop == []:\n",
    "            clear_output()\n",
    "            display(Markdown('__No columns were dropped!__'))\n",
    "        else:\n",
    "            clear_output()\n",
    "            df.drop(col_to_drop, axis=1, inplace=True)\n",
    "\n",
    "            # updated list of columns in dataframe\n",
    "            cols = df.columns\n",
    "\n",
    "            # updated list of numerical columns\n",
    "            num_cols = df._get_numeric_data().columns.tolist()\n",
    "\n",
    "            # updated list of categorical columns\n",
    "            cat_cols = list(set(cols) - set(num_cols))\n",
    "\n",
    "            display(Markdown('__Column(s) dropped__ : {}'.format(col_to_drop)))\n",
    "            \n",
    "            # displaying the updated dataframe\n",
    "            print('_'*75)\n",
    "            display(Markdown('__Updated data type summary__ :'))\n",
    "            display(df.info())\n",
    "            \n",
    "    else:\n",
    "        df.drop('X', axis=1, inplace=True)\n",
    "        display(Markdown('__Column dropped__ : X'))\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting & renaming columns\n",
    "\n",
    "##### Formatting Columns\n",
    "\n",
    "Here we lower case every column name and if there is any _white space_ or _dot_ `.` or _comma_ `,` separating words in any column name, it is replaced with an underscore for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T10:00:09.180992Z",
     "start_time": "2020-06-04T10:00:09.153518Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# formatting the columns\n",
    "# value=\"Formatting Cols\"\n",
    "value=\"Formatting Cols 2\"\n",
    "\n",
    "try:\n",
    "    # before formatting\n",
    "    original_cols = df.columns\n",
    "    cols_vis = ' || '.join(original_cols)\n",
    "    print(colored(\"\\nBEFORE FORMATTING Column Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
    "\n",
    "    # lower-case and replace any white-space/dot/comma seperator with '_' for every column name\n",
    "    df = df.rename(columns = lambda x: (x.lower()).replace(' ','_').replace('.','_').replace(',','_').replace(\"'\",\"\"))\n",
    "\n",
    "    updated_cols = df.columns\n",
    "    cols_vis = ' || '.join(updated_cols)\n",
    "    print(colored(\"\\nAFTER FORMATTING Column Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
    "\n",
    "    # displaying the necessary information\n",
    "    print('_'*75)\n",
    "    display(Markdown('__Updated data summary is as follows:__'))\n",
    "    display(df.info())\n",
    "  \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming Columns\n",
    "\n",
    "If the user wishes to __change (or rename)__ the column name, then execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:52:57.757691Z",
     "start_time": "2020-06-03T14:52:54.864714Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "value=\"Renaming Cols\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        # displaying the instruction\n",
    "        display(Markdown('__PLEASE READ THE INSTRUCTION TO RENAME COLUMN BEFORE PROCEEDING!__'))\n",
    "        display(Markdown('Enter the column name from the list shown, press `enter` and then give your custom column name.'))\n",
    "        display(Markdown('If you wish to proceed for more than one column, type `y` else `n`, and press `Enter`.'))\n",
    "        display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `enter` once the user input box appears.'))\n",
    "        print('_'*75)\n",
    "\n",
    "        # the list of columns in dataframe\n",
    "        original_cols = df.columns\n",
    "        cols_vis = ' || '.join(original_cols)\n",
    "        print(colored(\"\\nColumn Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
    "        print('_'*75)\n",
    "\n",
    "        # creating an empty dictionary to keep the old and new column name in pair\n",
    "        col_dict = dict()\n",
    "        no_change = False\n",
    "\n",
    "        # entering an infinite loop until the user doesn't want to change column names anymore\n",
    "        while True:\n",
    "            # check if the user is entering the right column name\n",
    "            while True:\n",
    "                org_col_in = input('\\nEnter the original column name: ')\n",
    "                if org_col_in in original_cols or org_col_in == '':\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored(\"\\nINCORRECT COLUMN NAME. Please try again!\",\"red\",attrs=['bold']))\n",
    "                    continue\n",
    "\n",
    "            if org_col_in == '':\n",
    "                no_change = True\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # check if the user is entering the new name in correct format\n",
    "            while True:\n",
    "                new_col_in = input('\\nEnter the new column name (use under-score [_] to separate multiple words): ')\n",
    "\n",
    "                if new_col_in in original_cols:\n",
    "                    print(colored(\"\\nSAME AS ORIGINAL NAME. Please try again!\",\"red\",attrs=['bold']))\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(new_col_in.split()) == 1 or '_' in new_col_in:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(colored(\"\\nINCORRECT FORMAT. Please try again!\",\"red\",attrs=['bold']))\n",
    "                        continue\n",
    "\n",
    "            # keep the pair of old and new column name in the dictionary\n",
    "            col_dict[org_col_in] = new_col_in\n",
    "\n",
    "            # check if the user wishes to rename more columns\n",
    "            while True:\n",
    "                rename_another_col = input('\\nDo you want to rename another column? (Y/N): ').lower()\n",
    "                if rename_another_col.lower() == 'y' or rename_another_col.lower() == 'n':\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored(\"\\nINCORRECT ENTRY. Please select y or n!\",\"red\",attrs=['bold']))\n",
    "                    continue\n",
    "\n",
    "            # if user wants to rename columns again, then repeat else break from this infinite loop\n",
    "            if rename_another_col == 'y':\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "        if no_change:\n",
    "            clear_output()\n",
    "            display(Markdown('__No columns renamed!__'))\n",
    "        else:\n",
    "            # renaming the columns in the original dataframe\n",
    "            df.rename(columns=col_dict, inplace=True)\n",
    "\n",
    "            # obtain the list of updated column names and display them\n",
    "            updated_cols = df.columns\n",
    "            updated_cols_vis = ' || '.join(updated_cols)\n",
    "            print('_'*75)\n",
    "            print(colored(\"\\nUpdated Column Names:\",'grey',attrs=['bold']),\"\\n{}\".format(updated_cols_vis))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of data\n",
    "\n",
    "Here, enter what type of data it is - panel-level or a single panel time series data. Depending on this input, rest of the analysis will flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:01.068339Z",
     "start_time": "2020-06-03T14:52:59.324670Z"
    }
   },
   "outputs": [],
   "source": [
    "# select what kind of panel data is it\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_type = input(\"Enter the type of the data (MP for Multi-Panel/SP for Single-Panel): \")\n",
    "    clear_output()\n",
    "\n",
    "    if dataset_type.lower() == 'mp':\n",
    "        display(Markdown(\"The selected datatype is: __Multi-Panel__\"))\n",
    "    else:\n",
    "        display(Markdown(\"The selected datatype is: __Single-Panel Time Series__\"))\n",
    "\n",
    "else:\n",
    "    dataset_type = 'mp'\n",
    "    display(Markdown(\"The selected datatype is: __Multi-Panel__\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning a unique row/panel identifier\n",
    "\n",
    "Here, the user will enter the column name which will be treated as a unique panel identifier throughout the notebook. It will be neglected while performing any analysis on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:02.759533Z",
     "start_time": "2020-06-03T14:53:02.694573Z"
    }
   },
   "outputs": [],
   "source": [
    "# assigning panel identifier\n",
    "value=\"Assigning panel identifier\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            display(Markdown('\\nEnter the name of column to be used as the __panel identifier__.'))\n",
    "\n",
    "            # the list of columns in dataframe\n",
    "            original_cols = df.columns\n",
    "            cols_vis = ' || '.join(original_cols)\n",
    "            print(colored(\"\\nColumn Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
    "            print('_'*75)\n",
    "\n",
    "            while True:\n",
    "                panel_col = input(\"\\nEnter the column name: \")\n",
    "\n",
    "                if panel_col not in df.columns:\n",
    "                    print(colored(\"\\nPlease enter the column name properly!\",\"red\",attrs=['bold']))\n",
    "                    continue\n",
    "                else:\n",
    "                    clear_output()\n",
    "                    display(Markdown(\"The column which will be considered as the __unique panel identifier__ is: __{}__\".format(panel_col)))\n",
    "                    # list down the panels\n",
    "                    panel_ids = list(set(df[panel_col]))\n",
    "                    display(Markdown('__Panel Information:__'))\n",
    "                    \n",
    "                    panel_shape = []\n",
    "                    for each_panel in panel_ids:\n",
    "                        panel_data = df.groupby(panel_col).get_group(each_panel)\n",
    "                        panel_shape.append(panel_data.shape[0])\n",
    "                    \n",
    "                    panel_info = pd.DataFrame()\n",
    "                    panel_info['panel_names'] = panel_ids\n",
    "                    panel_info['size'] = panel_shape\n",
    "\n",
    "                    display_data(panel_info)\n",
    "\n",
    "                    break\n",
    "        else:\n",
    "            display(Markdown('Not applicable for __single panel time series data__!'))\n",
    "            \n",
    "    else:\n",
    "        panel_col = 'fac_id'\n",
    "        panel_ids = list(set(df[panel_col]))\n",
    "        display(Markdown(\"The column which will be considered as the __unique panel identifier__ is: __{}__\".format(panel_col)))\n",
    "\n",
    "    # track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Target Variable\n",
    "\n",
    "Here, the user will enter the column name of the target variable to be used in univariate and mutlivariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:11.028674Z",
     "start_time": "2020-06-03T14:53:04.759548Z"
    }
   },
   "outputs": [],
   "source": [
    "# assigning target variable\n",
    "value=\"Assigning target variable\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        display(Markdown('Enter the name of column to be used as the __target variable__.'))\n",
    "\n",
    "        # the list of columns in dataframe\n",
    "        original_cols = df.columns\n",
    "        cols_vis = ' || '.join(original_cols)\n",
    "        print(colored(\"\\nColumn Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
    "        print('_'*75)\n",
    "\n",
    "        while True:\n",
    "            target_var = input(\"\\nEnter the column name: \")\n",
    "            if target_var not in original_cols:\n",
    "                print(colored(\"\\nPlease enter the column name properly!\",\"red\",attrs=['bold']))\n",
    "                continue\n",
    "            else:\n",
    "                clear_output()\n",
    "                display(Markdown(\"The column which will be considered as the __target variable__ is: __{}__\".format(target_var)))\n",
    "                break\n",
    "    else:\n",
    "        target_var = 'occupancy'\n",
    "        display(Markdown(\"The column which will be considered as the __target variable__ is: __{}__\".format(target_var)))\n",
    "\n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting date-time column(s)\n",
    "\n",
    "In this section, we will choose the date-time column and convert that to proper date-time format for future use. This is a very important as only after selecting this we can select other important components of time series, such as `frequency` of the data, `hierarchy` of the data, etc.\n",
    "\n",
    "_Sample Format_\n",
    "\n",
    "|       date-time       | Format                   |\n",
    "| ----------------------| ------------------------ |\n",
    "| 01/01/2013 00:00:00   |  \"%d/%m/%Y %H:%M:%S\"     |\n",
    "| 01:01:2013 00:00:01   |  \"%d:%m:%Y %H:%M:%S\"     |\n",
    "| 01-01-2013 00:00:02   |  \"%d-%m-%Y %H:%M:%S\"     |\n",
    "| 01012013 00:00:03     |  \"%d%m%Y %H:%M:%S\"       |\n",
    "| 01:01:2013:00:00:04 AM|  \"%d:%m:%Y:%H:%M:%S %p\"  |\n",
    "| 01/01-2013 00:00:05 PM|  \"%d/%m-%Y %H:%M:%S %p\"  |\n",
    "\n",
    "**NOTE:** The date-time formatting can't handle any date-time before `1677-09-22 00:12:43.145225`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:31.918479Z",
     "start_time": "2020-06-03T14:53:12.948620Z"
    }
   },
   "outputs": [],
   "source": [
    "# formatting date-time\n",
    "value=\"Formatting date-time\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        while True:\n",
    "\n",
    "            # Selection of date-time column\n",
    "            date_time_col = input(\"Enter the date time column: \")\n",
    "            ts_format = input(\"Enter the existing format of datetime column (E.g. %Y-%m-%d): \")\n",
    "\n",
    "            # Converting Date-Time to Date-Time format\n",
    "            if type(df.index) is pd.core.indexes.datetimes.DatetimeIndex:\n",
    "                print(colored(\"Selected datetime column is already in datetime format.\",\"green\",attrs=[\"bold\"]))\n",
    "                break\n",
    "            else:\n",
    "                df[date_time_col] = pd.to_datetime(df[date_time_col], format= ts_format)\n",
    "                \n",
    "                if '/' in ts_format:\n",
    "                    pass\n",
    "                else:\n",
    "                    if \"-\" in ts_format:\n",
    "                        ts_format_final = ts_format\n",
    "                    else:\n",
    "                        ts_format_final = '-'.join(ts_format[i:i+2] for i in range(0, len(ts_format), 2))\n",
    "\n",
    "                    df[date_time_col] = df[date_time_col].map(lambda x: x.strftime(ts_format_final))\n",
    "\n",
    "                while True:\n",
    "                    display(Markdown('If your dataset has time stamps and you want to retain it along with the date, press `Y`.\\\n",
    "                    Otherwise, if there is date only and you want to keep it that way, press `N`.'))\n",
    "                    is_timestamp = input('\\nDo you want to keep both the date and timestamp? (Y/N): ')\n",
    "                    if is_timestamp.lower() == 'y':\n",
    "                        df[date_time_col] = pd.to_datetime(df[date_time_col])\n",
    "                        break\n",
    "                    elif is_timestamp.lower() == 'n':\n",
    "                        df[date_time_col] = pd.to_datetime(df[date_time_col]).dt.date\n",
    "                        break\n",
    "                    else:\n",
    "                        print(colored('Incorrect entry! Please try again.','red',attrs=['bold']))\n",
    "                        continue\n",
    "\n",
    "                clear_output()\n",
    "                display(Markdown(\"The selected date-time column is: __{}__\".format(date_time_col)))\n",
    "                display(Markdown('Datetime column converted!'))\n",
    "                \n",
    "                df = df.sort_values(date_time_col)\n",
    "                \n",
    "                if df.shape[0] > 2500:\n",
    "                    display_data(df.head(2500).round(4))\n",
    "                else:\n",
    "                    display_data(df.round(4))\n",
    "\n",
    "                break\n",
    "                \n",
    "    else:\n",
    "        date_time_col = 'yearmonth'\n",
    "        ts_format = '%Y%m'\n",
    "        df[date_time_col] = pd.to_datetime(df[date_time_col], format=ts_format)\n",
    "        ts_format_final = '-'.join(ts_format[i:i+2] for i in range(0, len(ts_format), 2))\n",
    "        df[date_time_col] = df[date_time_col].map(lambda x: x.strftime(ts_format_final))\n",
    "        df[date_time_col] = pd.to_datetime(df[date_time_col]).dt.date\n",
    "        \n",
    "        df = df.sort_values(date_time_col)\n",
    "        \n",
    "        display(Markdown(\"The selected date-time column is: __{}__\".format(date_time_col)))\n",
    "        display(Markdown('Datetime column converted!'))\n",
    "\n",
    "        if df.shape[0] > 2500:\n",
    "            display_data(df.head(2500).round(4))\n",
    "        else:\n",
    "            display_data(df.round(4))\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing data type of columns\n",
    "\n",
    "In this section, data types for selected column(s) can be updated.\n",
    "\n",
    "__NOTE__ : If your data contains special character such as `$` or `@` that you want to ignore during conversion, then mention it in the list `special_chars` located in the beginning of the code by separating with `|`. Since `|` itself can't be handled like this, we are separately handling it at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:46.042834Z",
     "start_time": "2020-06-03T14:53:44.739638Z"
    }
   },
   "outputs": [],
   "source": [
    "# type-cast columns for proper analysis\n",
    "value='Type Cast Columns'\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        # list of special characters to be ignored\n",
    "        special_chars = r'[?|$|,|@|#|%|!]'\n",
    "        \n",
    "        # displaying the instruction\n",
    "        display(Markdown('__PLEASE READ THE INSTRUCTION TO TYPE-CAST COLUMN BEFORE PROCEEDING!__'))\n",
    "        display(Markdown('Enter the column name from the list shown, press `Enter` and then give your data type.'))\n",
    "        display(Markdown('Enter the name of column(s) comma `,` separated for __multiple inputs__.'))\n",
    "        display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
    "        print('_'*75)\n",
    "\n",
    "        # the list of columns in dataframe\n",
    "        display(Markdown('__Original data type of columns:__'))\n",
    "        display(df.dtypes)\n",
    "        print('_'*75)\n",
    "\n",
    "        col_type_cast = []\n",
    "\n",
    "        # user will enter column names and it will keep on going for infinite loop \n",
    "        # until it is in correct format\n",
    "        while True:\n",
    "            col_type_cast_input = input(\"\\nEnter the column name(s): \")\n",
    "\n",
    "            # if user enters 'None', then it will break the infinite loop and skip this operation\n",
    "            if col_type_cast_input == '':\n",
    "                col_type_cast = []\n",
    "                break\n",
    "            else:\n",
    "                # if it is a single column entry, keep on asking for input unless correctly entered\n",
    "                if ',' not in col_type_cast_input:\n",
    "                    if ' ' in col_type_cast_input:\n",
    "                        print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
    "                        continue\n",
    "                    else:\n",
    "                        if col_type_cast_input in df.columns:\n",
    "                            col_type_cast = [col_type_cast_input]\n",
    "                            break\n",
    "                        else:\n",
    "                            print(colored(\"\\nIncorrect column name! Please try again.\",\"red\",attrs=['bold']))\n",
    "                            continue\n",
    "                # if it is multiple column entry and properly entered, then it is stored in list\n",
    "                else:\n",
    "                    # split the string with ',' and convert it into list\n",
    "                    # check if any element is empty, then eliminate it\n",
    "                    # remove extra white spaces from column name using `strip`\n",
    "\n",
    "                    col_type_cast = [i.strip() for i in col_type_cast_input.split(',') if i]\n",
    "                    break\n",
    "\n",
    "        if col_type_cast == []:\n",
    "            clear_output()\n",
    "            display(Markdown('__No data type conversion took place!__'))\n",
    "            display(Markdown('__Original data type of the columns is:__'))\n",
    "            print('_'*75)\n",
    "            pass\n",
    "        else:\n",
    "            for each_col in col_type_cast:\n",
    "                # ask the user the desired type cast\n",
    "                what_type = input('\\nEnter the data type (int/str/float) for column `{}`: '.format(each_col))\n",
    "                if 'int' in what_type:\n",
    "                    try:\n",
    "                        if df[each_col].dtype == object:\n",
    "                            df[each_col] = df[each_col].str.replace('|','')\n",
    "                            df[each_col] = df[each_col].map(lambda x:re.sub(special_chars,r'',x)).astype('int64')\n",
    "                        else:\n",
    "                            df[each_col] = df[each_col].astype('int64')\n",
    "                    except Exception as err:\n",
    "                        # display the error\n",
    "                        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "                    else:\n",
    "                        print(colored('\\nCONVERTED!:','green',attrs=['bold']))\n",
    "\n",
    "                elif 'str' in what_type:\n",
    "                    try:\n",
    "                        df[each_col] = df[each_col].astype('str')\n",
    "                    except Exception as err:\n",
    "                        # display the error\n",
    "                        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "                    else:\n",
    "                        print(colored('\\nCONVERTED!:','green',attrs=['bold']))\n",
    "\n",
    "                elif 'float' in what_type:\n",
    "                    try:\n",
    "                        if df[each_col].dtype == object:\n",
    "                            df[each_col] = df[each_col].str.replace('|','')\n",
    "                            df[each_col] = df[each_col].map(lambda x:re.sub(special_chars,r'',x)).astype('float64')\n",
    "                        else:\n",
    "                            df[each_col] = df[each_col].astype('float64')\n",
    "                    except Exception as err:\n",
    "                        # display the error\n",
    "                        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "                    else:\n",
    "                        print(colored('\\nCONVERTED!:','green',attrs=['bold']))\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            print(\"_\"*75)\n",
    "            display(Markdown('__Updated data type of columns:__'))\n",
    "\n",
    "        display(df.dtypes)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "    \n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking and removing duplicates\n",
    "\n",
    "Presence of duplicate observations can be misleading, this sections helps get rid of such rows in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:47.690508Z",
     "start_time": "2020-06-03T14:53:47.609926Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for duplicate rows\n",
    "value=\"Check Duplicates\"\n",
    "\n",
    "try:\n",
    "    # length of original data\n",
    "    len_df_before = len(df)\n",
    "    display(Markdown('__The length of the original dataframe__ : {}'.format(len_df_before)))\n",
    "\n",
    "    # check for duplicate rows using parameter 'keep' having 3 possible values\n",
    "    # \n",
    "    # *first (Default) : considers (counts) duplicates except for the first occurrence.\n",
    "    # *last : considers (counts) duplicates except for the last occurrence.\n",
    "    # *False : considers (counts) all duplicates.\n",
    "\n",
    "    # collect duplicate using 'last'\n",
    "    df_duplicate_avoid_one_val = df[df.duplicated(keep='last')]\n",
    "\n",
    "    # collect all the duplicates\n",
    "    df_duplicate_all = df[df.duplicated(keep=False)]\n",
    "\n",
    "    # get unique count of duplicates\n",
    "    len_df_duplicate_avoid_one_val = len(df_duplicate_avoid_one_val)\n",
    "    len_df_duplicate_all = len(df_duplicate_all)\n",
    "    count_unique_val_with_duplicates = len_df_duplicate_all - len_df_duplicate_avoid_one_val\n",
    "    display(Markdown('\\n__The number of unique duplicates__ : {}'.format(count_unique_val_with_duplicates)))\n",
    "\n",
    "    if count_unique_val_with_duplicates == 0:\n",
    "        print(colored(\"NO DUPLICATES FOUND!\",'green',attrs=['bold']))\n",
    "    else:\n",
    "        print(colored(\"DUPLICATES ARE SPOTTED!\",'red',attrs=['bold']))\n",
    "        \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    clear_output()\n",
    "    # display the error\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting duplicate rows\n",
    "\n",
    "If there are duplicate values, then it is __recommended to delete__ those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:49.180555Z",
     "start_time": "2020-06-03T14:53:49.122299Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop rows having missing values across all the columns\n",
    "value=\"Treat duplicate data\"\n",
    "\n",
    "try:\n",
    "    if count_unique_val_with_duplicates == 0:\n",
    "        print(colored('NO DUPLICATE VALUES to remove!','green',attrs=['bold']))\n",
    "    else:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Processing</h2></div>'))\n",
    "\n",
    "        print('Number of rows of the original dataframe: {}'.format(df.shape[0]))\n",
    "\n",
    "        # dropping the rows\n",
    "        df.drop_duplicates(keep='last',inplace=True)\n",
    "        clear_output()\n",
    "\n",
    "        print(colored('DUPLICATE VALUES removed.','red',attrs=['bold']))\n",
    "        print('\\nAfter removing duplicate values, the number of rows in the dataframe is: {}'.format(df.shape[0]))\n",
    "        display(Markdown('The __data type summary__ is shown below:'.format(file_name)))\n",
    "        df.info() \n",
    "        \n",
    "    track_cell(value, flag)        \n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of numerical & categorical column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:50.446531Z",
     "start_time": "2020-06-03T14:53:50.435933Z"
    }
   },
   "outputs": [],
   "source": [
    "# the list of columns in dataframe\n",
    "df_cols = df.columns\n",
    "\n",
    "# obtain the list of numerical columns\n",
    "num_cols = df._get_numeric_data().columns.tolist()\n",
    "num_cols_vis = ' || '.join(num_cols)\n",
    "print(colored(\"Numerical Columns:\\nCount:\",'magenta',attrs=['bold']),\"{}\\n{}\".format(len(num_cols), \n",
    "                                                                                     num_cols_vis))\n",
    "\n",
    "# obtain the list of categorical columns\n",
    "cat_cols = list(set(df_cols) - set(num_cols))\n",
    "cat_cols_vis = ' || '.join(cat_cols)\n",
    "print(colored(\"\\nCategorical Columns:\\nCount:\",'blue',attrs=['bold']),\"{}\\n{}\".format(len(cat_cols), \n",
    "                                                                                      cat_cols_vis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering dataset for analysis\n",
    "\n",
    "Here, the user can filter the data by rows by executing a query. Check the code for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:52.912386Z",
     "start_time": "2020-06-03T14:53:51.541693Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filtering datset\n",
    "value=\"Filtering data\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        display(Markdown('__NOTE__ : If you execute this code, this will filter out from the original data and __this filtered data will be used throughout the rest of the notebook__.'))\n",
    "\n",
    "        original_df_shape = df.shape\n",
    "\n",
    "        while True:\n",
    "            operation_input = input(\"Do you want to filter the data? (Y/N): \")\n",
    "\n",
    "            # if user enters 'n', then it will break the infinite loop and skip this operation\n",
    "            if operation_input == 'N' or operation_input =='n' or operation_input =='':\n",
    "                break\n",
    "            elif operation_input == 'Y' or operation_input =='y':\n",
    "                # add conditions to filter data below\n",
    "\n",
    "                display(Markdown('Please provide a query based on your dara to filter. Take a look at this __sample query__ :\\\n",
    "                *location_type == \"HIGHWAY\" and rooms1mile>=50*'))\n",
    "\n",
    "                filter_query = input('Enter your query: ')\n",
    "                if filter_query == '':\n",
    "                    display(Markdown('No query entered. _Breaking from the filter operation!_'))\n",
    "                    break\n",
    "                else:\n",
    "\n",
    "                    # sample query based filtering\n",
    "                    df_filtered = df.query(filter_query)\n",
    "                    if df_filtered.shape[0] == 0:\n",
    "                        print(colored('\\nIncorrect query! Please try again.\\n','red',attrs=['bold']))\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(colored('\\nFiltering operation successful!\\n','green',attrs=['bold']))\n",
    "                        df = df_filtered.copy()\n",
    "                        break\n",
    "            else:\n",
    "                print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
    "                continue\n",
    "\n",
    "        modified_df_shape = df.shape\n",
    "\n",
    "        if modified_df_shape[0] == original_df_shape[0]:\n",
    "            clear_output()\n",
    "            display(Markdown('__No filtering operation performed!__'))\n",
    "        else:\n",
    "            clear_output()\n",
    "            display(Markdown('__Filtered data__: {} rows and {} columns.'.format(modified_df_shape[0], modified_df_shape[1])))\n",
    "            display(df.info())\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    clear_output()\n",
    "    # display the error\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Missing Value Analysis <a name='missing_val'></a>\n",
    "\n",
    "Missing value in the training data can lead to a biased model because we have not analyzed the behavior and relationship of those values with other variables correctly. It can lead to a wrong prediction or classification. Missing values can be of 3 types:\n",
    "1. **Missing Completely At Random (MCAR)**: When missing data are MCAR, the presence/absence of data is completely independent of observable variables and parameters of interest. This is a case when the __probability of missing variables is the same__ for all observations. _For example, respondents of the data collection process decide that they will declare they're earning after tossing a fair coin. If a head occurs, the respondent declares his / her earnings & vice versa._\n",
    "2. **Missing At Random (MAR)**: When missing data is not random but can be related to an observed variable where there is complete information. This kind of missing data can induce a bias in the analysis, especially if it unbalances the data because of many missing values in a certain category. _For example, we are collecting data for age and female has higher missing value compare to male._\n",
    "3. **Missing Depending on Unobserved Predictors**: This is a case when the missing values are not random and are related to the unobserved input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value on entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:55.757610Z",
     "start_time": "2020-06-03T14:53:55.747342Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the percentage of total missing values in the data\n",
    "percent_msng_val = (df.isnull().sum().sum()/(df.shape[0]*df.shape[1]))*100\n",
    "\n",
    "display(Markdown('Percentage of missing value in entire dataset is: __{}%__'.format(round(percent_msng_val,4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value on Column-level\n",
    "\n",
    "The following code is to visualize the missing values (if any) using bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:53:57.124539Z",
     "start_time": "2020-06-03T14:53:57.012258Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the missing data\n",
    "value=\"Visualizing missing data\"\n",
    "\n",
    "try:\n",
    "    # calculate the sum\n",
    "    total_msng_val = df.isnull().sum().sort_values(ascending=False)\n",
    "    \n",
    "    if sum(total_msng_val.tolist()) == 0:\n",
    "        print(colored('NO MISSING VALUES to visualize!','green',attrs=['bold']))\n",
    "    else:\n",
    "        if __name__ == '__main__':\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating plot</h2></div>'))\n",
    "        \n",
    "            # calculate the percentage\n",
    "            percent_msng_val = ((df.isnull().sum()/df.isnull().count())*100).sort_values(ascending=False)\n",
    "            # generate a table for displaying the information\n",
    "            missing_data = pd.concat([total_msng_val, percent_msng_val], axis=1, keys=['Total', 'Percentage']).reset_index()\n",
    "\n",
    "            missing_data.rename(columns = {'index': 'columns'}, inplace=True)\n",
    "\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Bar(x=missing_data['Percentage'], y=missing_data['columns'],\n",
    "                                 orientation='h'))\n",
    "            fig.update_traces(marker_color='maroon')\n",
    "            fig.update_xaxes(title = \"Percentage missing (%)\",range=[0, 100])\n",
    "            fig.update_yaxes(title = \"Columns\")\n",
    "            \n",
    "            if df.shape[1] < 30:\n",
    "                fig_ht = 600\n",
    "            else:\n",
    "                fig_ht = int(600*(df.shape[1]/100))\n",
    "            fig.update_layout(title =\"Percentage of Missing Value for every columns\",\n",
    "                              height = fig_ht, width = 900)\n",
    "            \n",
    "            clear_output()\n",
    "            fig.show(config={'displaylogo': False})\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value on Panel-level\n",
    "\n",
    "The following code is to visualize the missing values (if any) using heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:15.669838Z",
     "start_time": "2020-06-03T14:54:15.604313Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the missing value on panel level\n",
    "value = \"Panel-level Missing Value\"\n",
    "\n",
    "try:\n",
    "    if dataset_type.lower() == 'mp':\n",
    "        # calculate the sum\n",
    "        total_msng_val = df.isnull().sum().sum()\n",
    "\n",
    "        if total_msng_val == 0:\n",
    "            print(colored('No Missing Values!','green',attrs=['bold']))\n",
    "        else:\n",
    "            cols_except_id = df_cols.tolist()\n",
    "            cols_except_id.remove(panel_col)\n",
    "            cols_except_id.remove(date_time_col)\n",
    "\n",
    "            df_missing_heatmap = df.groupby(panel_col)[cols_except_id].apply(lambda x: x.isnull().sum()/len(x)*100)\n",
    "\n",
    "            # generate and display the plot\n",
    "            fig_height = 30\n",
    "            col_count = df_missing_heatmap.shape[0]\n",
    "            if col_count > 30:\n",
    "                fig_height = 30*((col_count/30) - (col_count//30)) + 30\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            plt.figure(figsize=(30,fig_height))\n",
    "            sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "            sns.heatmap(df_missing_heatmap.round(3), cmap='RdYlGn', annot=True, alpha=0.9)\n",
    "            clear_output()\n",
    "\n",
    "            # display the heatmap\n",
    "            display(Markdown('__Missing Value Heatmap on Panel-level__ '))\n",
    "            plt.show()\n",
    "    else:\n",
    "        display(Markdown('This operation is __not applicable__ for __single-panel__ data!'))\n",
    "        \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Time-Stamp(s)\n",
    "\n",
    "Identification of missing time-stamp is very important while doing a time-series analysis.\n",
    "\n",
    "Missing timestamps can lead to misinterpretation of any kind of forecasting model. In order to get optimal outcome from a fitted time-series model, these missing time-stamps need to be treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:21.796185Z",
     "start_time": "2020-06-03T14:54:17.530755Z"
    }
   },
   "outputs": [],
   "source": [
    "value='missing time stamp'\n",
    "\n",
    "try:\n",
    "    if dataset_type.lower() == 'mp':\n",
    "        \n",
    "        # Creating a padded time-series with all dates\n",
    "        unique_timestamps = sorted(set(df[date_time_col].astype(str)))\n",
    "\n",
    "        panel_ts_dict = {}\n",
    "        mssng_panels = []\n",
    "        mssng_panels_cnt = []\n",
    "\n",
    "        for each_id in panel_ids:\n",
    "            panel_data =  df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "            if panel_data[date_time_col].shape[0] == len(unique_timestamps):\n",
    "                panel_ts_dict[each_id] = {'timestamps': panel_data[date_time_col].tolist(), \n",
    "                                          'missing_count': 0}\n",
    "            else:\n",
    "                missing_timestamp = list(set(unique_timestamps) - set(panel_data[date_time_col].astype(str)))\n",
    "                panel_ts_dict[each_id] = {'timestamps': panel_data[date_time_col].tolist(), \n",
    "                                          'missing_count': len(missing_timestamp)}\n",
    "                print('_'*75)\n",
    "                display(Markdown('Panel ID: __{}__'.format(each_id)))\n",
    "                print('Missing Timestamps count: {}'.format(len(missing_timestamp)))\n",
    "                mssng_panels.append(each_id)\n",
    "                mssng_panels_cnt.append(len(missing_timestamp))\n",
    "                panel_ids.remove(each_id)\n",
    "\n",
    "        mssng_panels_df = pd.DataFrame({'panel':mssng_panels, 'count': mssng_panels_cnt})\n",
    "        \n",
    "        if mssng_panels_df.shape[0] == 0:\n",
    "            display(Markdown('__NO IMBALANCED PANELS FOUND!__'))\n",
    "        else:\n",
    "            plot_msng_ts = figure(plot_width=900, plot_height=600, title='Count of missing time-stamps in the panels',\n",
    "                                   y_axis_label=\"count\", x_axis_label='panel', \n",
    "                                   x_range=mssng_panels_df['panel'].astype(str).to_list())\n",
    "            plot_msng_ts.vbar(x=mssng_panels_df['panel'].astype(str).to_list(), \n",
    "                               top=mssng_panels_df['count'].astype(int).to_list(), width=0.5)\n",
    "            plot_msng_ts.xaxis.major_label_orientation = pi/4\n",
    "            plot_msng_ts.toolbar.logo = None    \n",
    "            show(plot_msng_ts)\n",
    "\n",
    "            display(Markdown('<span style=\"color:red; font-size: 15px\"><b>NOTE</b>: The above panels are <b>IMBALANCED</b> and hence will be <b>IGNORED FROM REST OF THE ANALYSIS</b>!</span>'))\n",
    "\n",
    "    else:\n",
    "        df_test = df.sort_values(by=date_time_col)\n",
    "        d = df_test[date_time_col].diff()\n",
    "\n",
    "        periodicity = input('Enter the periodicity (Y/m/d/H/M/S): ')\n",
    "        missing_timestamp = []\n",
    "        missing_timestamp_start = []\n",
    "        missing_timestamp_end = []\n",
    "        for i,each_gap in enumerate(d.to_list()):\n",
    "            if each_gap > d.mode()[0] and i <= len(d)-2:\n",
    "                prev_date = df_test[date_time_col].to_list()[i-1]\n",
    "                next_date = df_test[date_time_col].to_list()[i+1]\n",
    "                missing_timestamp_start.append(prev_date)\n",
    "                missing_timestamp_end.append(next_date)\n",
    "                missing_timestamp.append(pd.date_range(prev_date, next_date,freq=periodicity).astype(str).to_list())\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        display(Markdown('__Missing Timestamps Count__: {}'.format(len(missing_timestamp))))\n",
    "        missing_timestamp_df = pd.DataFrame({'missing_timestamp_start':missing_timestamp_start,\n",
    "                                             'missing_timestamp_end':missing_timestamp_end})\n",
    "        if len(missing_timestamp) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            display(missing_timestamp_df.T)\n",
    "        \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop column(s) with missing values\n",
    "\n",
    "The cell below accepts user input and drops the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:31.673791Z",
     "start_time": "2020-06-03T14:54:23.564805Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# eliminate the columns\n",
    "value=\"Deleting Cols missing\"\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        # columns with missing values\n",
    "        total_msng_val = df.isnull().sum()\n",
    "    \n",
    "        if total_msng_val.sum() == 0:\n",
    "            print(colored('No Columns with Missing Values to drop!','green',attrs=['bold']))\n",
    "        \n",
    "        else:\n",
    "            display(Markdown('__PLEASE READ THE INSTRUCTION TO DELETE COLUMN DUE TO MISSING VALUE BEFORE PROCEEDING!__'))\n",
    "            display(Markdown('Enter the column name(s) from the list shown _(comma `,` separated for multiple)_ and press `Enter`.'))\n",
    "            display(Markdown('__NOTE__ : If you want to __skip__ this step, simply press `Enter` once the user input box appears.'))\n",
    "            print('_'*75)\n",
    "        \n",
    "            missing_cols = missing_data[missing_data['Percentage']>0]['columns'].to_list()\n",
    "            display(Markdown(\"Columns with Missing Values: {}\".format(missing_cols)))\n",
    "        \n",
    "            col_to_drop = columns_to_delete(df_cols)\n",
    "            if col_to_drop == []:\n",
    "                clear_output()\n",
    "                display(Markdown('__No columns were dropped due to missing value!__'))\n",
    "            else:\n",
    "                clear_output()\n",
    "                df.drop(col_to_drop, axis=1, inplace=True)\n",
    "\n",
    "                # updated list of columns in dataframe\n",
    "                cols = df.columns\n",
    "\n",
    "                # updated list of numerical columns\n",
    "                num_cols = df._get_numeric_data().columns.tolist()\n",
    "\n",
    "                # updated list of categorical columns\n",
    "                cat_cols = list(set(cols) - set(num_cols))\n",
    "\n",
    "                display(Markdown('__Column(s) dropped__ : {}'.format(col_to_drop)))\n",
    "\n",
    "                # displaying the updated dataframe\n",
    "                print('_'*75)\n",
    "                display(Markdown('__Updated data type summary__ :'))\n",
    "                display(df.info())\n",
    "            \n",
    "    else:\n",
    "        display(Markdown('__No columns were dropped due to missing value!__'))\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop row(s) with missing values\n",
    "\n",
    "\n",
    "If there are missing values, then it is __recommended to delete__ those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:34.095524Z",
     "start_time": "2020-06-03T14:54:34.020441Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop rows having missing values across all the columns\n",
    "value=\"Treat missing data\"\n",
    "\n",
    "try:\n",
    "    # calculate the sum\n",
    "    total_msng_val = df.isnull().sum()\n",
    "    \n",
    "    if sum(total_msng_val.tolist()) == 0:\n",
    "        print(colored('NO MISSING VALUES to remove!','green',attrs=['bold']))\n",
    "    else:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Processing</h2></div>'))\n",
    "\n",
    "        print('Number of rows of the original dataframe: {}'.format(df.shape[0]))\n",
    "\n",
    "        # dropping the rows\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "        clear_output()\n",
    "\n",
    "        print(colored('MISSING VALUES removed!','green',attrs=['bold']))\n",
    "        print('\\nAfter removing missing values, the number of rows in the dataframe is: {}'.format(df.shape[0]))\n",
    "    \n",
    "    display(Markdown('The __data summary__ is shown below:'))\n",
    "    display(df.info())\n",
    "    \n",
    "    track_cell(value, flag)        \n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Final Dataset Summary\n",
    "\n",
    "All further operations will be performed on the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:36.822378Z",
     "start_time": "2020-06-03T14:54:36.545951Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# displays the final data along with the datatype summary\n",
    "value = 'final sata summary'\n",
    "\n",
    "try:\n",
    "    display(Markdown('Final dataframe contains __{} rows__ and __{} columns__.'.format(df.shape[0], df.shape[1])))\n",
    "    \n",
    "    if dataset_type.lower() == 'mp':\n",
    "        display(Markdown('The unique panel identifier is __{}__.'.format(panel_col)))\n",
    "\n",
    "    if dataset_type.lower() == 'mp':\n",
    "        display(Markdown(\"The selected datatype is: __Multi-Panel__\"))\n",
    "        display(Markdown(\"Total number of unique panels: __{}__\".format(len(panel_ids))))\n",
    "        if mssng_panels != None:\n",
    "            display(Markdown('The imbalanced panel(s): _{}_'.format(mssng_panels)))\n",
    "        else:\n",
    "            display(Markdown('No panels to be ignored!'))\n",
    "    else:\n",
    "        display(Markdown(\"The selected datatype is: __Single-Panel__\"))\n",
    "        if len(missing_timestamp) == 0:\n",
    "            display(Markdown('__Missing Timestamps Count__: {}'.format(len(missing_timestamp))))\n",
    "        else:\n",
    "            display(Markdown('__Missing Timestamps Count__: {}'.format(len(missing_timestamp))))\n",
    "            display(missing_timestamp_df.T)\n",
    "\n",
    "    display(Markdown('The target variable is: __{}__'.format(target_var)))\n",
    "    \n",
    "    if df.shape[0] > 2500:\n",
    "        display_data(df.head(2500).round(4))\n",
    "    else:\n",
    "        display_data(df.round(4))\n",
    "    \n",
    "    display(Markdown('The __data summary__ is shown below:'.format(file_name)))\n",
    "    print('_'*75)\n",
    "    df.info()\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of Numerical & Categorical columns\n",
    "\n",
    "We will be using the column name entered as a __unique panel identifier__ for the rest of the analysis to flow properly, and hence will be ignored when calling the list of numerical and categorical column names.\n",
    "\n",
    "__NOTE__: The column is not deleted, just ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:44.126330Z",
     "start_time": "2020-06-03T14:54:44.110998Z"
    }
   },
   "outputs": [],
   "source": [
    "# the list of columns in dataframe\n",
    "\n",
    "df_cols = df.columns\n",
    "\n",
    "# obtain the list of numerical columns\n",
    "num_cols = df._get_numeric_data().columns.tolist()\n",
    "\n",
    "if dataset_type.lower() == 'mp':\n",
    "    cols_to_ignore = [panel_col,date_time_col]\n",
    "else:\n",
    "    cols_to_ignore = [date_time_col]\n",
    "\n",
    "for col_name in cols_to_ignore:\n",
    "    if col_name in num_cols:\n",
    "        num_cols.remove(row_id_input)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "num_cols_vis = ' || '.join(num_cols)\n",
    "print(colored(\"Numerical Columns:\\nCount:\",'magenta',attrs=['bold']),\"{}\\n{}\".format(len(num_cols), \n",
    "                                                                                     num_cols_vis))\n",
    "\n",
    "# obtain the list of categorical columns\n",
    "cat_cols = list(set(df_cols) - set(num_cols))\n",
    "\n",
    "for col_name in cols_to_ignore:\n",
    "    if col_name in cat_cols:\n",
    "        cat_cols.remove(col_name)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "cat_cols_vis = ' || '.join(cat_cols)\n",
    "print(colored(\"\\nCategorical Columns:\\nCount:\",'blue',attrs=['bold']),\"{}\\n{}\".format(len(cat_cols), \n",
    "                                                                                      cat_cols_vis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selection of panels for analysis\n",
    "\n",
    "The following function will allow the user to either perform EDA on every panels or some selective ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:45.755739Z",
     "start_time": "2020-06-03T14:54:45.729623Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating the function for the user to select panels of their choices\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    def panel_selection(panel_ids):\n",
    "        '''\n",
    "        Function to allow the users to select panels for panel data\n",
    "        '''\n",
    "\n",
    "        while True:\n",
    "            selected_panels = None\n",
    "            panel_choice = input('Do you want to generate results for all {} panels? (Y/N): '.format(len(panel_ids)))\n",
    "            if panel_choice.lower() == 'y':\n",
    "                selected_panels = panel_ids\n",
    "                break\n",
    "            elif panel_choice.lower() == 'n':\n",
    "                while True:\n",
    "                    display(Markdown('List of panels:\\n{}'.format(panel_ids)))\n",
    "                    panel_in = input('Enter the panel names (comma separated for multiple): ')\n",
    "\n",
    "                    if ',' not in panel_in:\n",
    "                        if ' ' in panel_in:\n",
    "                            print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
    "                            continue\n",
    "                        else:\n",
    "                            if panel_in in panel_ids:\n",
    "                                selected_panels = [panel_in]\n",
    "                                break\n",
    "                            else:\n",
    "                                print(colored(\"\\nINCORRECT PANEL NAME. Please try again!\",\"red\",attrs=['bold']))\n",
    "                                continue\n",
    "                    # if it is multiple column entry and properly entered, then it is stored in list\n",
    "                    else:\n",
    "                        # split the string with ',' and convert it into list\n",
    "                        # check if any element is empty, then eliminate it\n",
    "                        selected_panels = [i.strip() for i in panel_in.split(',') if i]\n",
    "\n",
    "                        # if all the column names are correctly entered, then proceed\n",
    "                        if all(item in panel_ids for item in selected_panels):\n",
    "                            break\n",
    "                        else:\n",
    "                            print(colored(\"\\nINCORRECT PANEL NAME(S). Please enter all the names again!\",\"red\",attrs=['bold']))\n",
    "                            continue\n",
    "            else:\n",
    "                print(colored('Incorrect Entry! Please try again.','red',attrs=['bold']))\n",
    "                continue\n",
    "\n",
    "            if selected_panels != None:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return selected_panels\n",
    "\n",
    "    display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Code cell to activate function for <b>panel selection</b> is EXECUTED!</span>'))\n",
    "    \n",
    "else:\n",
    "    selected_panels = 'H136400C09'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:09:06.669265Z",
     "start_time": "2020-06-03T14:09:06.663883Z"
    }
   },
   "source": [
    "##### Selection of columns for analysis\n",
    "\n",
    "The following function will allow the user to either perform EDA on every columns or some selective ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:47.048674Z",
     "start_time": "2020-06-03T14:54:47.028957Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    def col_selection(cols):\n",
    "        while True:\n",
    "            selected_cols = None\n",
    "            if cols == num_cols:\n",
    "                col_choice = input('Do you want to generate results for all numerical {} columns? (Y/N): '.format(len(cols)))\n",
    "            else:\n",
    "                col_choice = input('Do you want to generate results for all categorical {} columns? (Y/N): '.format(len(cols)))\n",
    "            if col_choice.lower() == 'y':\n",
    "                selected_cols = cols\n",
    "                break\n",
    "            elif col_choice.lower() == 'n':\n",
    "                while True:\n",
    "                    display(Markdown('List of cols:\\n{}'.format(cols)))\n",
    "                    col_in = input('Enter the column names (comma separated for multiple): ')\n",
    "                    \n",
    "                    if ',' not in col_in:\n",
    "                        if ' ' in col_in:\n",
    "                            print(colored(\"\\nPlease read the instruction and enter properly!\",\"red\",attrs=['bold']))\n",
    "                            continue\n",
    "                        else:\n",
    "                            if col_in in cols:\n",
    "                                selected_cols = [col_in]\n",
    "                                break\n",
    "                            else:\n",
    "                                print(colored(\"\\nINCORRECT COLUMN NAME. Please try again!\",\"red\",attrs=['bold']))\n",
    "                                continue\n",
    "                    # if it is multiple column entry and properly entered, then it is stored in list\n",
    "                    else:\n",
    "                        # split the string with ',' and convert it into list\n",
    "                        # check if any element is empty, then eliminate it\n",
    "                        selected_cols = [i.strip() for i in col_in.split(',') if i]\n",
    "\n",
    "                        # if all the column names are correctly entered, then proceed\n",
    "                        if all(item in cols for item in selected_cols):\n",
    "                            break\n",
    "                        else:\n",
    "                            print(colored(\"\\nINCORRECT col NAME(S). Please enter all the names again!\",\"red\",attrs=['bold']))\n",
    "                            continue\n",
    "            else:\n",
    "                print(colored('Incorrect Entry! Please try again.','red',attrs=['bold']))\n",
    "                continue\n",
    "\n",
    "            if selected_cols != None:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return selected_cols\n",
    "    \n",
    "    display(Markdown('<span style=\"color:darkgreen; font-style: italic; font-size: 15px\">Code cell to activate function for <b>column selection</b> is EXECUTED!</span>'))\n",
    "    \n",
    "else:\n",
    "    selected_cols = num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series plots of all variables on Panel-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:53.685183Z",
     "start_time": "2020-06-03T14:54:48.513229Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating the time series plot for every numerical columns across panels\n",
    "value = 'time series plot'\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            \n",
    "            selected_cols = col_selection(num_cols)\n",
    "            \n",
    "            if len(selected_cols) <= 4:\n",
    "                row_cnt = 1\n",
    "                col_cnt = len(selected_cols)\n",
    "            else:\n",
    "                col_cnt = 4\n",
    "                row_cnt = len(selected_cols)//4+1\n",
    "                if len(selected_cols)%4 == 0:\n",
    "                    row_cnt = len(selected_cols)//4\n",
    "\n",
    "            fig = make_subplots(rows=row_cnt, cols=col_cnt, subplot_titles=selected_cols)\n",
    "\n",
    "            # calculating the number of rows and columns\n",
    "            r, c = list(range(1,(len(selected_cols)//4+2))), list(range(1,5))\n",
    "            rc_pair = list(product(r, c))\n",
    "\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "\n",
    "            # performing the operation on panel level\n",
    "            for p_id,each_panel in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_panel)\n",
    "                panel_data = panel_data.sort_values(date_time_col)\n",
    "                vis = True if each_panel == selected_panels[0] else False\n",
    "\n",
    "                # generating the plots for every numerical columns\n",
    "                for c_id,each_col in enumerate(selected_cols):\n",
    "                    fig_r = rc_pair[c_id][0]\n",
    "                    fig_c = rc_pair[c_id][1]\n",
    "\n",
    "                    if each_col == target_var:\n",
    "                        lin_col = 'firebrick'\n",
    "                    else:\n",
    "                        lin_col = 'royalblue'\n",
    "                    fig.add_trace(go.Scatter(x=panel_data[date_time_col], y=panel_data[each_col],\n",
    "                                             line=dict(color=lin_col, width=1), visible=vis), \n",
    "                                  row=fig_r, col=fig_c)\n",
    "            \n",
    "            panel_dict_list = []\n",
    "            # creating the drop-down option\n",
    "            for each_panel in selected_panels:\n",
    "                vis_check = [[True]*len(selected_cols) if i==each_panel else [False]*len(selected_cols) for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                panel_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
    "                                                    {\"title\": \"Observation of each columns in panel: {}\".format(each_panel)}],\n",
    "                                            label=each_panel, method=\"update\"))\n",
    "\n",
    "            # Add dropdown by opening the option on horizontal direction\n",
    "            fig.update_layout(updatemenus=[dict(buttons=list(panel_dict_list),\n",
    "                                                direction=\"right\",\n",
    "                                                x=0, xanchor=\"left\", y=1.15, yanchor=\"top\")],\n",
    "                             showlegend=False, title_x=0.5)\n",
    "\n",
    "        else:\n",
    "            selected_cols = col_selection(num_cols)\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "\n",
    "            row_cnt = len(selected_cols)//2+1\n",
    "            if len(selected_cols)%2 == 0:\n",
    "                row_cnt = len(selected_cols)//2\n",
    "\n",
    "            fig = make_subplots(rows=row_cnt, cols=2, subplot_titles=selected_cols)\n",
    "\n",
    "            # calculating the number of rows and columns\n",
    "            r, c = list(range(1,(len(selected_cols)//2+2))), list(range(1,3))\n",
    "            rc_pair = list(product(r, c))\n",
    "\n",
    "            for c_id,each_col in enumerate(selected_cols):\n",
    "                fig_r = rc_pair[c_id][0]\n",
    "                fig_c = rc_pair[c_id][1]\n",
    "\n",
    "                if each_col == target_var:\n",
    "                    lin_col = 'firebrick'\n",
    "                else:\n",
    "                    lin_col = 'royalblue'\n",
    "                fig.add_trace(go.Scatter(x=df[date_time_col], y=df[each_col],\n",
    "                                         line=dict(color=lin_col, width=1)),\n",
    "                              row=fig_r, col=fig_c)\n",
    "\n",
    "#         fig.update_xaxes(tickangle=90)\n",
    "        if len(selected_cols) <= 4:\n",
    "            fig_ht = 550\n",
    "        elif len(selected_cols) > 4 and len(selected_cols) <= 20:\n",
    "            fig_ht = 1000\n",
    "        else:\n",
    "            fig_ht = 1500\n",
    "        fig.update_layout(width=900, height=fig_ht, showlegend=False, title_x=0.5)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        fig.show(config={'displaylogo': False})\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "    \n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics on Panel-level\n",
    "\n",
    "A moving mean (or standard deviation) for a given time period is the (arithmetic) average (or standard deviation) of the values in that time period and those close to it. \n",
    "\n",
    "A window of size `k` means k consecutive values at a time. So, the smoothness pattern changes with respect to the window size. For example, the smoothness factor for a short time-series data can be visible with a small windows size, but can't be the same case with a longer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:58:47.993470Z",
     "start_time": "2020-06-03T14:58:47.773434Z"
    }
   },
   "outputs": [],
   "source": [
    "# time series for selected target column across panels\n",
    "value='rolling stats'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        col_color_pair = {target_var:'blue', 'roll_mean':'orange',\n",
    "                         'roll_std_upper':'green', 'roll_std_lower':'red'}\n",
    "        fig = go.Figure()\n",
    "\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            # performing the operation on panel level\n",
    "            for p_id,each_panel in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_panel)\n",
    "                panel_data = panel_data.sort_values(date_time_col)\n",
    "                \n",
    "                window_size = int(0.3*panel_data.shape[0])\n",
    "                \n",
    "                # calculating the rolling mean\n",
    "                col_rolling_mean = panel_data[target_var].rolling(window=window_size).mean().to_frame()\n",
    "                col_rolling_mean.columns = ['roll_mean']\n",
    "\n",
    "                # calculating the rolling std\n",
    "                col_rolling_std = panel_data[target_var].rolling(window=window_size).std().to_frame()\n",
    "\n",
    "                # concatenating the original dataframe with the dataframe storing the rolling mean result\n",
    "                df_rolling_ts = pd.concat([panel_data,col_rolling_mean],axis=1)\n",
    "                # calculating the upper and lower bound \n",
    "                df_rolling_ts['roll_std_upper'] = col_rolling_mean.iloc[:,0] + col_rolling_std.iloc[:,0]\n",
    "                df_rolling_ts['roll_std_lower'] = col_rolling_mean.iloc[:,0] - col_rolling_std.iloc[:,0]\n",
    "                \n",
    "                visible = True if each_panel == selected_panels[0] else False\n",
    "                for k,v in col_color_pair.items():\n",
    "                    fig.add_trace(go.Scatter(x=df_rolling_ts[date_time_col], y=df_rolling_ts[k], name=k,\n",
    "                                         line=dict(color=v, width=1), visible = visible))\n",
    "            \n",
    "            panel_dict_list = []\n",
    "            # creating the drop-down option\n",
    "            for each_panel in selected_panels:\n",
    "                vis_check = [[True]*4 if i==each_panel else [False]*4 for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                panel_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
    "                                                    {\"title\": \"Rolling statistics in panel: {}\".format(each_panel)}],\n",
    "                                            label=each_panel, method=\"update\"))\n",
    "\n",
    "            # Add dropdown by opening the option on horizontal direction\n",
    "            fig.update_layout(updatemenus=[dict(buttons=list(panel_dict_list),\n",
    "                                                direction=\"right\",\n",
    "                                                x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                             showlegend=False, width=900, title_x=0.5)\n",
    "\n",
    "        else:\n",
    "            window_size = int(0.3*df.shape[0])\n",
    "            \n",
    "            # calculating the rolling mean\n",
    "            col_rolling_mean = df[target_var].rolling(window=window_size).mean().to_frame()\n",
    "            col_rolling_mean.columns = ['roll_mean']\n",
    "\n",
    "            # calculating the rolling std\n",
    "            col_rolling_std = df[target_var].rolling(window=window_size).std().to_frame()\n",
    "\n",
    "            df_rolling_ts = pd.concat([df[[target_var, date_time_col]],col_rolling_mean],axis=1)\n",
    "            df_rolling_ts['roll_std_upper'] = col_rolling_mean.iloc[:,0] + col_rolling_std.iloc[:,0]\n",
    "            df_rolling_ts['roll_std_lower'] = col_rolling_mean.iloc[:,0] - col_rolling_std.iloc[:,0]\n",
    "            \n",
    "            for k,v in col_color_pair.items():\n",
    "                fig.add_trace(go.Scatter(x=df_rolling_ts[date_time_col], y=df_rolling_ts[k], name=k,\n",
    "                                     line=dict(color=v, width=1)))\n",
    "            \n",
    "            fig.update_layout(width=900, showlegend=False, title_x=0.5)\n",
    "        \n",
    "        fig.update_layout(hovermode='x unified')\n",
    "        fig.update_yaxes(title_text= target_var)\n",
    "        fig.update_xaxes(tickangle=90)\n",
    "        clear_output()\n",
    "\n",
    "        display(Markdown('__Rolling statistics plots for target variable `{}` with `window size {}` generated!__'.format(target_var, window_size)))\n",
    "        fig.show(config={'displaylogo': False})\n",
    "        track_cell(value, flag)\n",
    "\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:06:45.931328Z",
     "start_time": "2020-05-06T12:06:45.926909Z"
    }
   },
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:34:08.603566Z",
     "start_time": "2020-03-24T09:34:08.601672Z"
    }
   },
   "source": [
    "***\n",
    "\n",
    "# Variable Analysis\n",
    "\n",
    "## Numerical features\n",
    "\n",
    "### Histogram plots for numerical columns\n",
    "\n",
    "A __Histogram__ groups values into bins of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential, etc. It can help us to spot any skewness in its shape when the distribution is nearly regular but has some anomalies.\n",
    "\n",
    "There is also another, often clearer, way to grasp the distribution - __Density Plots__ or, more formally, __Kernel Density Plots__. They can be considered a smoothed version of the histogram. Their main advantage over the histogram is that they do not depend on the size of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:58:55.113204Z",
     "start_time": "2020-06-03T14:58:52.383893Z"
    }
   },
   "outputs": [],
   "source": [
    "# histogram plots on the numerical column(s)\n",
    "\n",
    "value=\"Plotting Numeric Cols\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if num_cols == []:\n",
    "            display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
    "\n",
    "        # for multiple numerical columns\n",
    "        else:\n",
    "            # initializing the tabs\n",
    "            fig = go.Figure()\n",
    "            selected_cols = col_selection(num_cols)\n",
    "            \n",
    "            for each_col in selected_cols:\n",
    "                vis = True if each_col == num_cols[0] else False\n",
    "                fig.add_trace(go.Histogram(x=df[each_col], opacity=0.5, histnorm='probability density',\n",
    "                                          visible=vis))\n",
    "\n",
    "            tab_dict_list = []\n",
    "            for each_col in selected_cols:\n",
    "                vis_check = [[True] if i==each_col else [False] for i in selected_cols]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
    "                                                  {\"title\": \"Histogram plots for: {}\".format(each_col)}],\n",
    "                                          label=each_col, method=\"update\"))\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                                  showlegend=False, title_x=0.5)\n",
    "\n",
    "            clear_output()\n",
    "            fig.update_yaxes(title_text= 'KDE')\n",
    "            display(Markdown('__Histogram Plots for the numerical columns are generated!__'))\n",
    "            fig.show(config={'displaylogo': False})\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        # display the error\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:34:08.603566Z",
     "start_time": "2020-03-24T09:34:08.601672Z"
    }
   },
   "source": [
    "### Descriptive statistics on Column-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:58:57.312559Z",
     "start_time": "2020-06-03T14:58:57.207174Z"
    }
   },
   "outputs": [],
   "source": [
    "# descriptive and numerical statistics of numerical column(s)\n",
    "\n",
    "if num_cols == []:\n",
    "    display(Markdown('__NO NUMERICAL COLUMNS AVAILABLE!__'))\n",
    "else:\n",
    "    num_desc = df[num_cols].describe().T\n",
    "    num_desc['count'] = num_desc['count'].astype(int)\n",
    "    num_desc['median'] = df[num_cols].median()\n",
    "    num_desc.drop(['25%','50%','75%'], axis=1 ,inplace = True)\n",
    "    num_desc.insert(loc=0, column='columns', value=num_desc.index)\n",
    "    \n",
    "    display_data(num_desc.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:34:08.603566Z",
     "start_time": "2020-03-24T09:34:08.601672Z"
    }
   },
   "source": [
    "### Descriptive statistics on Panel-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:58:58.933560Z",
     "start_time": "2020-06-03T14:58:58.770005Z"
    }
   },
   "outputs": [],
   "source": [
    "# dot and whisker plot of descriptive stats on panel level\n",
    "\n",
    "value = 'num desc stats'\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        if dataset_type.lower() == 'mp':\n",
    "\n",
    "            def dot_whisk(selected_panels, stat):\n",
    "                dw_plot_slots = [Panel() for i in range(len(selected_cols))]\n",
    "\n",
    "                hover = HoverTool()\n",
    "                save = SaveTool()\n",
    "\n",
    "                # performing the operation on every numerical columns\n",
    "                for idx,each_col in enumerate(selected_cols):\n",
    "                    panel_data_q1 = []\n",
    "                    panel_data_q3 = []\n",
    "                    panel_data_mean = []\n",
    "                    if stat == 'Standard Deviation':\n",
    "                        panel_data_std = []\n",
    "\n",
    "                    # calculating the stats on every panels\n",
    "                    for each_id in selected_panels:\n",
    "                        panel_data = df.groupby(panel_col)[each_col].get_group(each_id)\n",
    "                        panel_data_mean.append(panel_data.mean())\n",
    "                        panel_data_q1.append(panel_data.quantile(.25))\n",
    "                        panel_data_q3.append(panel_data.quantile(.75))\n",
    "                        if stat == 'Standard Deviation':\n",
    "                            panel_data_std.append(panel_data.std())\n",
    "\n",
    "                    if len(selected_panels) < 10:\n",
    "                        dw_fig_height = 100*(len(selected_panels)+1)\n",
    "                    else:\n",
    "                        dw_fig_height = 10*(len(selected_panels)+1)\n",
    "\n",
    "                    if stat == 'Mean':\n",
    "                        # storing the result in dataframe\n",
    "                        dw_df = pd.DataFrame({'panels':selected_panels, 'mean':panel_data_mean,\n",
    "                                              'Q1':panel_data_q1, 'Q3':panel_data_q3}).sort_values('mean').reset_index()\n",
    "\n",
    "                        # creating the plot\n",
    "                        dw_plot = figure(y_range=dw_df['panels'], plot_width=900, plot_height=dw_fig_height, \n",
    "                                         tools=[hover, save])\n",
    "                        # plotting the mean values\n",
    "                        dw_plot.circle(dw_df['mean'], dw_df['panels'], \n",
    "                                       size=5, color=\"navy\", alpha=0.5, legend_label='Mean')\n",
    "                        # plotting the 1st and 3rd quantiles\n",
    "                        for each_row in range(dw_df.shape[0]):\n",
    "                            dw_plot.line([dw_df.loc[each_row,'Q1'], dw_df.loc[each_row,'Q3']], \n",
    "                                         [dw_df.loc[each_row,'panels'], dw_df.loc[each_row,'panels']],\n",
    "                                         line_width=1, color=\"navy\", alpha=0.5, legend_label='Whisker (Q1 & Q3)')\n",
    "                        # plotting the mean of the mean values of every panel\n",
    "                        dw_plot.line([mean(panel_data_mean), mean(panel_data_mean)], [0, len(selected_panels)],\n",
    "                                     line_width=2, color=\"maroon\", line_dash='dashed', legend_label='Column Mean wrt MEAN')\n",
    "                    else:\n",
    "                        # storing the result in a dataframe\n",
    "                        dw_df = pd.DataFrame({'panels':selected_panels, 'std':panel_data_std}).sort_values('std').reset_index()\n",
    "\n",
    "                        # generating the plot\n",
    "                        dw_plot = figure(y_range=dw_df['panels'], plot_width=900, plot_height=dw_fig_height, \n",
    "                                         tools=[hover, save])\n",
    "                        # plotting the std\n",
    "                        dw_plot.circle(dw_df['std'], dw_df['panels'], \n",
    "                                       size=5, color=\"navy\", alpha=0.5, legend_label='Standard Deviation')\n",
    "                        # plotting the mean of std of every panel\n",
    "                        dw_plot.line([mean(panel_data_std), mean(panel_data_std)], [0, len(selected_panels)],\n",
    "                                     line_width=2, color=\"maroon\", line_dash='dashed', legend_label='Column Mean wrt STD')\n",
    "\n",
    "                    dw_plot.legend.location = \"bottom_right\"\n",
    "                    dw_plot.toolbar.logo = None\n",
    "\n",
    "                    dw_plot_slots[idx] = Panel(child=dw_plot, title=each_col)\n",
    "\n",
    "                # creating the tabs having plots\n",
    "                dw_plot_tabs = Tabs(tabs = dw_plot_slots)\n",
    "                dw_plot_layout = column(dw_plot_tabs)\n",
    "\n",
    "                plot_title = Div(text='''<span style=\"font-size: 15px\"><b>Dot(Mean) & Whisker(IQR) Plot</b> for every panel is generated!</span>''',\n",
    "                                width=900)\n",
    "                if stat == 'Standard Deviation':\n",
    "                    plot_title = Div(text='''<span style=\"font-size: 15px\"><b>Dot(Standard Deviation) Plot</b> for every panel is generated!</span>''',\n",
    "                                    width=900)\n",
    "                title_widg = widgetbox(plot_title)\n",
    "\n",
    "                return dw_plot_layout, title_widg\n",
    "\n",
    "\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            selected_cols = col_selection(num_cols)\n",
    "    #         selected_panels = ['H453700C08', 'H346600C03', 'H136400C09', 'H635900C03']\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "        \n",
    "            mean_df = pd.DataFrame(columns=selected_cols)\n",
    "            std_df = pd.DataFrame(columns=selected_cols)\n",
    "\n",
    "            num_desc_dfs = {'Mean':mean_df, 'Standard Deviation':std_df}\n",
    "\n",
    "            # performing the operation on panel level\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                desc_df = panel_data.describe().round(3)\n",
    "                mean_df.loc[mean_df.shape[0]] = desc_df.loc['mean',:].to_list()\n",
    "                std_df.loc[std_df.shape[0]] = desc_df.loc['std',:].to_list()\n",
    "\n",
    "            summary_plot_slots = [Panel() for i in range(len(num_desc_dfs))]\n",
    "\n",
    "            data_table = []\n",
    "            # generate the result on `mean` and `std` on every panel\n",
    "            for idx,(each_id,each_df) in enumerate(num_desc_dfs.items()):\n",
    "                each_df.insert(loc=0, column='panels', value=selected_panels)\n",
    "\n",
    "                table_height = 150\n",
    "                if each_df.shape[0] > 5:\n",
    "                    table_height = int(150*((each_df.shape[0]/2) - (each_df.shape[0]//2)) + 150)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                # generate the table output \n",
    "                data_table.append(DataTable(columns=[TableColumn(field=Ci, title=Ci) for Ci in each_df.columns],\n",
    "                                       source=ColumnDataSource(each_df), height=table_height, width=900))\n",
    "\n",
    "                dw_plot, title_widg = dot_whisk(selected_panels, each_id)\n",
    "                data_table_plot = gridplot([[data_table[idx]], [title_widg], [dw_plot]], plot_width=900)\n",
    "\n",
    "                clear_output()\n",
    "                summary_plot_slots[idx] = Panel(child=data_table_plot, title=each_id)\n",
    "\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            # creating the tabs having plots\n",
    "            summary_plot_tabs = Tabs(tabs = summary_plot_slots)\n",
    "            summary_plot_layout = column(summary_plot_tabs)\n",
    "\n",
    "            show(summary_plot_layout)\n",
    "        else:\n",
    "            clear_output()\n",
    "            display(Markdown('This operation is __not applicable__ for __single-panel__ data!'))\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features\n",
    "\n",
    "### Frequency plots for categorical columns\n",
    "\n",
    "Here, we will get a __frequency table__, which shows how frequent each value of the categorical variable is, and using a __bar plot__, we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:01.342318Z",
     "start_time": "2020-06-03T14:59:01.336450Z"
    }
   },
   "outputs": [],
   "source": [
    "# frequency bar plot of categories in categorical column(s) \n",
    "\n",
    "def cat_feature_viz(data, categorical_column):\n",
    "    '''\n",
    "    Function to visualize the counts of categories in categorical variables. \n",
    "    If there are more than 20 categories in a variable, we calculate the \n",
    "    mean of value count. Then, any categories whose value count is less than \n",
    "    the mean value is kept in a category 'Other'.\n",
    "    input:\n",
    "        data               : data frame\n",
    "        categotical_column : list of categorical columns\n",
    "    return:\n",
    "        None. It displays a table of value count of categories and a horizontal \n",
    "        bar plot in the console output for each categorical column(s).\n",
    "    '''\n",
    "    \n",
    "    # getting the value count against each category (key) and storing it as key:value pair\n",
    "    cat_value_pair = dict(data[categorical_column].value_counts().items())\n",
    "    \n",
    "    # list of category names and their corresponding values\n",
    "    cat_value_pair_keys = list(cat_value_pair.keys())\n",
    "    cat_value_pair_values = list(cat_value_pair.values())\n",
    "    \n",
    "    # map the keys under the `categorical_column` column and value count under `count` column\n",
    "    cat_dict = {categorical_column:cat_value_pair_keys, 'count':cat_value_pair_values}\n",
    "    \n",
    "    # create the dataframe\n",
    "    cat_value_df = pd.DataFrame(cat_dict).set_index(categorical_column)\n",
    "    cat_value_df.reset_index(inplace=True)\n",
    "    \n",
    "    return cat_value_df\n",
    "    \n",
    "display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for generating <b>Frequency Plot</b>.</span>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:03.850184Z",
     "start_time": "2020-06-03T14:59:01.832161Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating the plots (if any)\n",
    "value=\"Plotting Cat Cols\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if cat_cols == []:\n",
    "            display(Markdown('__NO CATEGORICAL COLUMNS AVAILABLE!__'))\n",
    "            script, div = None, None\n",
    "\n",
    "        # for multiple categorical columns\n",
    "        else:\n",
    "            selected_cols = col_selection(cat_cols)\n",
    "            \n",
    "            fig = go.Figure()\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            ignored_cols = []\n",
    "            for c in selected_cols:\n",
    "                vis = True if c == selected_cols[0] else False\n",
    "                if df[c].nunique()>30:\n",
    "                    ignored_cols.append(c)\n",
    "                    vis = False\n",
    "                else:\n",
    "                    cat_value_df = cat_feature_viz(df, c)\n",
    "                    fig.add_trace(go.Bar(x=cat_value_df[c], y=cat_value_df['count'],width=0.5, visible=vis))\n",
    "            \n",
    "            tab_dict_list = []\n",
    "            for each_col in selected_cols:\n",
    "                vis_check = [[True] if i==each_col else [False] for i in selected_cols]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
    "                                                  {\"title\": \"Frequency plots for: {}\".format(each_col)}],\n",
    "                                          label=each_col, method=\"update\"))\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                                  showlegend=False, title_x=0.5)\n",
    "            \n",
    "            fig.update_xaxes(tickangle=45)\n",
    "            clear_output()\n",
    "            display(Markdown('__Plots generated!__'))\n",
    "            if ignored_cols == []:\n",
    "                pass\n",
    "            else:\n",
    "                display(Markdown('Plots of the following columns are NOT generated for having __too many categories__!'))\n",
    "                display(Markown('__Columns__ : {}'.format(ignored_cols)))\n",
    "            \n",
    "            fig.update_yaxes(title='Frequency')\n",
    "            fig.show(config={'displaylogo':False})\n",
    "\n",
    "#         track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        # display the error\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics on Column-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:05.913044Z",
     "start_time": "2020-06-03T14:59:05.868031Z"
    }
   },
   "outputs": [],
   "source": [
    "# descriptive statistics of categorical column(s)\n",
    "\n",
    "if cat_cols == []:\n",
    "    display(Markdown('__NO CATEGORICAL COLUMNS AVAILABLE!__'))\n",
    "else:\n",
    "    cat_desc = df.describe(include=[np.object]).T\n",
    "    cat_desc.insert(loc=0, column='columns', value=cat_desc.index)\n",
    "    display_data(cat_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T10:14:38.628950Z",
     "start_time": "2020-03-26T10:14:38.625638Z"
    }
   },
   "source": [
    "### Descriptive statistics on Panel-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:07.353292Z",
     "start_time": "2020-06-03T14:59:07.301763Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculating the descriptive statistics on categorical columns\n",
    "value = 'cat desc stats'\n",
    "\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        if selected_cols == []:\n",
    "            display(Markdown('__NO CATEGORICAL COLUMNS AVAILABLE!__'))\n",
    "        else:\n",
    "            if dataset_type.lower() == 'mp':\n",
    "                selected_panels = panel_selection(panel_ids)\n",
    "#                 selected_panels = ['H453700C08', 'H346600C03', 'H136400C09', 'H635900C03']\n",
    "                selected_cols = col_selection(cat_cols)\n",
    "\n",
    "                display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the results</h2></div>'))\n",
    "                \n",
    "                unique_df = pd.DataFrame(columns=selected_cols)\n",
    "                top_df = pd.DataFrame(columns=selected_cols)\n",
    "\n",
    "                cat_desc_dfs = {'Unique':unique_df, 'Top':top_df}\n",
    "\n",
    "                # performing the operation on panel level\n",
    "                for each_id in selected_panels:\n",
    "                    panel_data = df.groupby(panel_col)[selected_cols].get_group(each_id)\n",
    "\n",
    "                    desc_df = panel_data.describe(include=[np.object])\n",
    "                    unique_df.loc[unique_df.shape[0]] = desc_df.loc['unique',:].to_list()\n",
    "                    top_df.loc[top_df.shape[0]] = desc_df.loc['top',:].to_list()\n",
    "\n",
    "                fig = go.Figure()\n",
    "\n",
    "                for name,data_table in cat_desc_dfs.items():\n",
    "                    data_table.insert(loc=0, column='panels', value=selected_panels)\n",
    "                    data_table_series = [data_table[i] for i in data_table.columns]\n",
    "\n",
    "                    fig.add_traces(data=[go.Table(\n",
    "                        header=dict(values=list(data_table.columns),\n",
    "                                    fill_color='grey',\n",
    "                                    align='center',\n",
    "                                    font=dict(color='white', size=15)\n",
    "                                   ),\n",
    "                        cells=dict(values=data_table_series,\n",
    "                                   fill_color='lightblue',\n",
    "                                   align='center',\n",
    "                                   font=dict(color='black', size=10)\n",
    "                                  ))\n",
    "                    ])\n",
    "\n",
    "                    if data_table.shape[0] <= 5:\n",
    "                        fig_ht = 50*data_table.shape[0]\n",
    "                    elif data_table.shape[0] > 5 and data_table.shape[0] <= 22:\n",
    "                        fig_ht = 20*data_table.shape[0]\n",
    "                    else:\n",
    "                        fig_ht = 500\n",
    "\n",
    "                    fig.update_layout(width=150*len(data_table.columns), \n",
    "                                      height=fig_ht,\n",
    "                                      margin=dict(l=0,r=0,b=0,t=0,pad=0))\n",
    "\n",
    "                fig.update_layout(updatemenus=[dict(type=\"buttons\", direction=\"right\", active=0,\n",
    "                                                    x=0.1, y=1.2, \n",
    "                                                    buttons=list([dict(label=\"Unique\",\n",
    "                                                                      method=\"update\",\n",
    "                                                                      args=[{\"visible\": [True, False]}]),\n",
    "                                                                  dict(label=\"Top\",\n",
    "                                                                      method=\"update\",\n",
    "                                                                      args=[{\"visible\": [False, True]}])\n",
    "                                                                 ])\n",
    "                                                   )\n",
    "                                              ])\n",
    "\n",
    "                clear_output()\n",
    "                display(Markdown('__Summary generated!__'))\n",
    "                fig.show(config={'displaylogo': False})\n",
    "\n",
    "            else:\n",
    "                clear_output()\n",
    "                display(Markdown('This operation is __not applicable__ for __single-panel__ data!'))\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    track_cell(value, flag)\n",
    "except Exception as err:\n",
    "    # display the error\n",
    "    clear_output()\n",
    "    print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "    flag = 0\n",
    "    err = str(err)\n",
    "    track_cell(value, flag, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stacked Bar Chart for distrbution of categories of categorical columns across panels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:08.732169Z",
     "start_time": "2020-06-03T14:59:08.700394Z"
    }
   },
   "outputs": [],
   "source": [
    "def cat_dist(data, selected_cols):\n",
    "    # generating the pivot table\n",
    "    cnt_data = pd.DataFrame(columns=['cat','count','column'])\n",
    "    for each_col in selected_cols:\n",
    "        cat_prop = pd.DataFrame(data[each_col].value_counts()).reset_index()\n",
    "        cat_prop['column'] = [each_col]*cat_prop.shape[0]\n",
    "        cat_prop.columns = cnt_data.columns\n",
    "        cnt_data = cnt_data.append(cat_prop, ignore_index=True)\n",
    "\n",
    "    data_pivot = cnt_data.pivot(index='column',columns='cat',values='count').fillna(0)\n",
    "    data_pivot.index = [x for x in data_pivot.index]\n",
    "\n",
    "    # converting the pivot table to a dictionary supported to generate the plot\n",
    "    data_cols = data_pivot.columns.to_list()\n",
    "    data = {}\n",
    "    for each_col in data_cols:\n",
    "        data[each_col] = data_pivot[each_col].to_list()\n",
    "    data['column'] = data_pivot.index.to_list()\n",
    "\n",
    "    # generate the plot\n",
    "    p_stacked_bar = figure(x_range=data_pivot.index.to_list(), \n",
    "                           plot_height=600, plot_width=900, toolbar_location=None, \n",
    "                           tools=\"hover\", tooltips=\"$name @column: @$name\")\n",
    "\n",
    "    viridis_cmap = cm.get_cmap('viridis', 256)\n",
    "    viridis_cat_rgba = viridis_cmap(np.linspace(0,1,len(data_cols)))\n",
    "\n",
    "    color_range = []\n",
    "    for each_rgb in viridis_cat_rgba:\n",
    "        r_val, g_val, b_val = 255*each_rgb[0], 255*each_rgb[1], 255*each_rgb[2]\n",
    "        hex_val = '#%02x%02x%02x' % (int(r_val), int(g_val), int(b_val))\n",
    "        color_range.append(hex_val)\n",
    "\n",
    "    p_stacked_bar_0 = p_stacked_bar.vbar_stack(data_cols, x='column', width=0.6, \n",
    "                                               source=data, \n",
    "                                               fill_color=color_range)\n",
    "\n",
    "    vbar_comp = []\n",
    "    for p_id,each_part in enumerate(p_stacked_bar_0):\n",
    "        vbar_comp.append([p_stacked_bar_0[p_id]])\n",
    "\n",
    "    legend_list = list(zip(data_cols,vbar_comp))\n",
    "\n",
    "    if len(legend_list) <= 6:\n",
    "        data_legend = Legend(items=legend_list, location=(100, 0), orientation=\"horizontal\")\n",
    "        p_stacked_bar.add_layout(data_legend, 'below')\n",
    "    else:\n",
    "        slicing_index = list(range(0, len(legend_list), 6))\n",
    "        for i in range(len(slicing_index) - 1):\n",
    "            idx_start = slicing_index[i]\n",
    "            idx_end = slicing_index[i+1]\n",
    "            data_legend = Legend(items=legend_list[idx_start:idx_end], location=(100, i*10), orientation=\"horizontal\")\n",
    "            p_stacked_bar.add_layout(data_legend, 'below')\n",
    "\n",
    "        if idx_end != len(legend_list):\n",
    "            i += 1\n",
    "            data_legend = Legend(items=legend_list[idx_end:], location=(100, i*10), orientation=\"horizontal\")\n",
    "            p_stacked_bar.add_layout(data_legend, 'below')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    p_stacked_bar.toolbar.logo = None\n",
    "    p_stacked_bar.xaxis.major_label_orientation = pi/4\n",
    "\n",
    "    return p_stacked_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:12.360550Z",
     "start_time": "2020-06-03T14:59:09.827039Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of categories on categorical variables across panels\n",
    "value = 'cat dist plot'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if cat_cols == []:\n",
    "        display(Markdown('__NO CATEGORICAL COLUMNS AVAILABLE!__'))\n",
    "    else:\n",
    "        try:\n",
    "            if dataset_type.lower() == 'mp':\n",
    "                selected_panels = panel_selection(panel_ids)\n",
    "                # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "                selected_cols = col_selection(cat_cols)\n",
    "\n",
    "                display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "\n",
    "                cat_freq_plot_slots = [Panel() for i in range(len(selected_panels))]\n",
    "\n",
    "                # performing the operation on panel level\n",
    "                for idx,each_id in enumerate(selected_panels):\n",
    "                    panel_data = df.groupby(panel_col)[selected_cols].get_group(each_id)\n",
    "\n",
    "                    p_stacked_bar = cat_dist(panel_data, selected_cols)\n",
    "\n",
    "                    cat_freq_plot_slots[idx] = Panel(child=p_stacked_bar, title=each_id)\n",
    "\n",
    "                cat_freq_plot_tabs = Tabs(tabs = cat_freq_plot_slots)\n",
    "                cat_freq_plot_layout = column(cat_freq_plot_tabs)\n",
    "\n",
    "                script, div = components(cat_freq_plot_layout)\n",
    "\n",
    "            else:\n",
    "                selected_cols = col_selection(cat_cols)\n",
    "                p_stacked_bar = cat_dist(df, selected_cols)\n",
    "                script, div = components(p_stacked_bar)\n",
    "\n",
    "            track_cell(value, flag)\n",
    "\n",
    "        except Exception as err:\n",
    "            # display the error\n",
    "            clear_output()\n",
    "            print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "            flag = 0\n",
    "            err = str(err)\n",
    "            track_cell(value, flag, err)\n",
    "\n",
    "        else:\n",
    "            clear_output()\n",
    "            if script== None and div == None:\n",
    "                display(Markdown('This operation is __not applicable__ for __single-panel__ data!'))\n",
    "            else:\n",
    "                display(Markdown('__Plots generated!__'))\n",
    "                display(HTML(html_plot.render(script=script, div=div)))\n",
    "            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Univariate Analysis\n",
    "\n",
    "## Decomposition\n",
    "\n",
    "The decomposition of time series data is a statistical task that de-constructs a time series into several components to define its characteristics. Usually, a time-series data contains 4 types of pattern:\n",
    "\n",
    "* __Trend (T)__ : The general change (long-term upward or downward pattern) in the level of the data over a duration longer than a year.\n",
    "* __Seasonal (S)__ : The regular wavelike fluctuations of constant length, repeating themselves within each 12-month period year after year.\n",
    "* __Cyclical (C)__ : The _quasi-regular_ (4 phases) wavelike fluctuations - from peak (prosperity) to contractions (recession) to trough (depression) to expansion (recovery) - around the long-term trend, lasting longer than a year.\n",
    "* __Irregular (R)__ : The short-duration and non-repeating random variations of the data that exist after taking into account the unforeseen events such as strikes or natural disaster.\n",
    "\n",
    "However, when we decompose the time-series into components, we combine the trend and cycle component into one time-series component to get a __trend-cycle__ component, also known as __Trend__. Thus, time series can be considered of comprising 3 major components: \n",
    "\n",
    "* __Trend-cycle__ or __Trend__ component\n",
    "* __Seasonal__ component\n",
    "* __Remainder__ component (containing noise)\n",
    "\n",
    "For decomposition, we need to define the __type__ of the observed series, which is stored in `series_type` variable. It can either be __additive__ or __multiplicative__.\n",
    "* __Additive__ : The seasonal, cyclical and random variations are absolute deviations from the trend. It is _linear_ where seasonality changes over time are consistently made by having almost the _same frequency_ (width of cycles) and _amplitude_ (height of cycles).\n",
    "\n",
    "$$\\begin{gather*}\n",
    "y(t) = T_{t}+S_{t}+C_{t}+R_{t}\n",
    "\\end{gather*}$$\n",
    "\n",
    "* __Multiplicative__ : The seasonal, cyclical and random variations are relative (percentage) deviations from the trend. It is nonlinear, such as quadratic or exponential and non-linear seasonality has an increasing or decreasing frequency and/or amplitude over time.\n",
    "\n",
    "$$\\begin{gather*}\n",
    "y(t) = T_{t}*S_{t}*C_{t}*R_{t}\n",
    "\\end{gather*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:18.124277Z",
     "start_time": "2020-06-03T14:59:15.339566Z"
    }
   },
   "outputs": [],
   "source": [
    "value=\"Decomposition Plot\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('Enter `A` or `M` for `additive` or `multiplicative` series type.'))\n",
    "        while True:\n",
    "            series_type_inp = input('Enter the series_type (A/M): ')\n",
    "            if series_type_inp.lower() == 'a':\n",
    "                series_type = 'additive'\n",
    "                break\n",
    "            elif series_type_inp.lower() == 'm':\n",
    "                series_type = 'multiplicative'\n",
    "                break\n",
    "            else:\n",
    "                print(colored('Invalid choice! Try again.','red',attrs=['bold']))\n",
    "                continue\n",
    "        \n",
    "        display(Markdown('In order to give the frequency, see the following mapping and provide the __integer value__:'))\n",
    "        display(Markdown('[_month_ - 12] [_week_ - 52] [_day_ - 365] [_hour_ - 60] [_minute_ - 3600]'))\n",
    "        \n",
    "        while True:\n",
    "            enter_freq = int(input('Enter the freq: '))\n",
    "            if enter_freq in [12,52,365,60,3600]:\n",
    "                break\n",
    "            else:\n",
    "                print(colored('Incorrect! Try again','red',attrs=['bold']))\n",
    "                continue\n",
    "        \n",
    "        decomp_df_cols =  ['Observed', 'Trend', 'Seasonal' ,'Residual']\n",
    "        fig = make_subplots(rows=4, cols=1, subplot_titles=tuple(decomp_df_cols))\n",
    "        \n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            \n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                \n",
    "                # Decomposing the time-series\n",
    "                decomp_res = seasonal_decompose(panel_data[target_var], model=series_type, \n",
    "                                                filt=None, freq=enter_freq)\n",
    "                decomp_df = pd.DataFrame({'Observed': decomp_res.observed, 'Trend':decomp_res.trend,\n",
    "                                          'Seasonal':decomp_res.seasonal, 'Residual':decomp_res.resid})\n",
    "                \n",
    "                decomp_df_cols = decomp_df.columns\n",
    "                decomp_df[date_time_col] = panel_data[date_time_col]\n",
    "                visible = True if each_id == selected_panels[0] else False\n",
    "                for i,each_col in enumerate(decomp_df_cols):\n",
    "                    j= i+1\n",
    "                    fig.add_trace(go.Scatter(x= decomp_df[date_time_col],y= decomp_df[each_col],\n",
    "                                             mode = 'lines', name = 'value', opacity= 0.8, showlegend= False,\n",
    "                                             visible = visible, legendgroup='value')\n",
    "                                  ,row = j, col = 1)\n",
    "            \n",
    "            tab_dict_list = []\n",
    "            for each_id in selected_panels:\n",
    "                vis_check = [[True]*len(decomp_df_cols) if i==each_id else [False]*len(decomp_df_cols) for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args=[{\"visible\": vis_check_flat},\n",
    "                                                {\"title\": \"Decomposition plots for panel: {}\".format(each_id)}],\n",
    "                                          label=each_id, method=\"update\"))\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                                  showlegend=False, title_x=0.5)\n",
    "        else:\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            decomp_res = seasonal_decompose(df[target_var], model=series_type, \n",
    "                                            filt=None, freq=enter_freq)\n",
    "            decomp_df = pd.DataFrame({'Observed': decomp_res.observed, 'Trend':decomp_res.trend,\n",
    "                                      'Seasonal':decomp_res.seasonal, 'Residual':decomp_res.resid})\n",
    "            decomp_df['Residual'].fillna(0, inplace=True)\n",
    "            decomp_df_cols = decomp_df.columns\n",
    "            for i,each_col in enumerate(decomp_df_cols):\n",
    "                j= i+1\n",
    "                fig.add_trace(go.Scatter(x= df[date_time_col],y= decomp_df[each_col],\n",
    "                                             mode = 'lines', name = 'value', opacity= 0.8, showlegend= False,\n",
    "                                             visible = True, legendgroup='value')\n",
    "                                  ,row = j, col = 1)\n",
    "        \n",
    "        fig.update_layout(height=800, hovermode='x unified')\n",
    "        clear_output()\n",
    "        display(Markdown('__Decomposition plot for `{}` generated!__'.format(target_var)))\n",
    "        fig.show(config={'displaylogo': False})\n",
    "        \n",
    "#         track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary Analysis\n",
    "\n",
    "For a time-series to have the [stationary](https://en.wikipedia.org/wiki/Stationary_process) property,\n",
    "the series should have three basic properties:\n",
    "\n",
    "* The mean of the series should NOT be a function of time and should be a constant.\n",
    "* The variance of the series should NOT be a function of time. This property is known as [homoscedasticity](https://www.statisticssolutions.com/homoscedasticity/).\n",
    "* The [covariance](https://www.investopedia.com/terms/c/covariance.asp) of the $i^{th}$ term and the $(i+k)^{th}$ term (`k` being the time lag) should NOT be a function of time.\n",
    "\n",
    "There are 3 different types of stationarity present in time-series analysis:\n",
    "\n",
    "* **Strict Stationary:**\n",
    "A strict stationary series satisfies the mathematical definition of a stationary process. For a strict stationary series, the __mean, variance and covariance are not the function of time__. The aim is to convert a non-stationary series into a strict stationary series for making predictions.\n",
    "\n",
    "* **Trend Stationary:**\n",
    "A __unit root__ is a feature of some stochastic/random processes (such as random walks). A series that has __NO unit root__ but exhibits a trend is referred to as a trend stationary series. __Once the trend is removed, the resulting series will be strict stationary__.\n",
    "\n",
    "* **Difference Stationary:**\n",
    "A time series that can be made strict stationary by differencing is known as difference stationary.\n",
    "\n",
    "There are several techniques to check whether a time-series is stationary or not such as:\n",
    "\n",
    "* **Look at Plots:** You can review a time series plot of your data and visually check if there are any obvious trends or seasonality. However, if a time-series has complex stationary patterns, it is not recommended to analyze that using this approach.\n",
    "* **Summary Statistics:** You can review the summary statistics for your data for seasons or random partitions and check for obvious or significant differences.\n",
    "* **Statistical Tests:** You can use statistical tests to check if the expectations of stationarity are met or have been violated.\n",
    "\n",
    "In this notebook, we will be discussing the most popular **statistical tests** to identify the presence of stationarity in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Test\n",
    "\n",
    "##### ADF (Augmented dickey-Fuller) Test\n",
    "\n",
    "The __[Augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)__ is a type of statistical test called a __unit root test__. The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend.\n",
    "\n",
    "ADF uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n",
    "\n",
    "* __Null Hypothesis ($H_0$)__ : The series contains __unit root__, and hence it is non-stationary. It has some time dependent structure.\n",
    "* __Alternative Hypothesis ($H_1$)__ : The series is weakly stationary. It does not have time-dependent structure.\n",
    "\n",
    "The function __`ADF`__ has the `trend` parameter with the following 4 options: \n",
    "* `nc` - `N`o `C`onstant trend\n",
    "* `c` - `C`onstant trend (default) \n",
    "* `ct` - `C`onstant and linear `T`ime trend\n",
    "* `ctt` - `C`onstant and linear `T`ime and quadratic `T`ime trends\n",
    "\n",
    "`lags` is the number of lags to use in the ADF regression. If omitted or None, `method` such as __AIC, BIC__ or __t-stat__ is used to automatically select the lag length.\n",
    "\n",
    "__HOW TO INTERPRET__ : If the __p-value__ obtained from the test is __less than the significance level of 0.05__, then we fail to accept the $H_0$ i.e., __the series is stationary__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:22.034407Z",
     "start_time": "2020-06-03T14:59:21.879346Z"
    }
   },
   "outputs": [],
   "source": [
    "# ADF test\n",
    "value=\"ADF test\"\n",
    "\n",
    "def adf_analysis(adf_test, each_id):\n",
    "    # assigning whether the null hypothesis is accepted\n",
    "    stationary_check = 1 if adf_test.pvalue < 0.05 else 0\n",
    "\n",
    "    # storing the result in a dataset\n",
    "    df_adf_output = pd.DataFrame([adf_test.stat, adf_test.pvalue, adf_test.lags, stationary_check],\n",
    "                                index=['Test Statistic','p-value','Number of Lags Used',\n",
    "                                      'Stationary Check']).reset_index()\n",
    "    df_adf_output.columns = (['Panel','value'])\n",
    "    df_adf_output['index'] = each_id\n",
    "\n",
    "    for k,v in adf_test.critical_values.items():\n",
    "        df_adf_output.loc[df_adf_output.index.max()+1] = (['Critical Value at {}'.format(k),v,each_id])\n",
    "\n",
    "    df_adf_output = df_adf_output.pivot(columns = 'Panel',values = 'value',index = 'index').round(4)\n",
    "    df_adf_output.index = [x for x in df_adf_output.index]\n",
    "    \n",
    "    df_adf_output[['Number of Lags Used', 'Stationary Check']] = df_adf_output[['Number of Lags Used', 'Stationary Check']].astype(int)\n",
    "    \n",
    "    df_adf_output.insert(loc=0, column='Panels', value=df_adf_output.index)\n",
    "    \n",
    "    return df_adf_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "\n",
    "            df_adf = pd.DataFrame()\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                # performing ADF test\n",
    "                adf_test = ADF(panel_data[panel_data[panel_col] == each_id][target_var])\n",
    "\n",
    "                df_adf_output = adf_analysis(adf_test, each_id)\n",
    "\n",
    "                df_adf = df_adf.append(df_adf_output)\n",
    "        else:\n",
    "            adf_test = ADF(df[target_var])\n",
    "\n",
    "            df_adf = adf_analysis(adf_test, 0)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "\n",
    "    else:\n",
    "        clear_output()\n",
    "        # Showing adf test summary\n",
    "        display(Markdown('__Results of Augmented Dickey-Fuller Test for `{}`:__'.format(target_var)))\n",
    "        if df_adf.shape[0] == 1:\n",
    "            display(df_adf)\n",
    "        else:\n",
    "            display_data(df_adf.round(4))\n",
    "            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KPSS (KwiatkowskiPhillipsSchmidtShin) Test\n",
    "\n",
    "The __KwiatkowskiPhillipsSchmidtShin test__ is used to determine whether differencing is required on the data. \n",
    "\n",
    "* __Null Hypothesis ($H_0$)__ : The series is weakly stationary.\n",
    "* __Alternative Hypothesis ($H_1$)__ : The series contains __unit root__, and hence it is non-stationary.\n",
    "\n",
    "Note that the $H_0$ and $H_1$ for the KPSS test are opposite to that of the ADF test, which often creates confusion.\n",
    "\n",
    "The function __`KPSS`__ has the `trend` parameter with the following 2 options:\n",
    "* `c` - `C`onstant trend (default) \n",
    "* `ct` - `C`onstant and linear `T`ime trend\n",
    "\n",
    "Also, the `lag` parameter can be manually added with maximum value being less than the length of the sample data, or automatically set to `12*(nobs/100)**(1/4)`, where `nobs` is the length of the sample size.\n",
    "\n",
    "__HOW TO INTERPRET__ : If the __p-value__ obtained from the test is __less than the significance level of 0.05__, then we fail to accept the $H_0$, i.e., __the series is non-stationary__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:23.842156Z",
     "start_time": "2020-06-03T14:59:23.723792Z"
    }
   },
   "outputs": [],
   "source": [
    "# KPSS test\n",
    "value=\"KPSS test\"\n",
    "\n",
    "\n",
    "def kpss_analysis(kpss_test, each_id):\n",
    "    # assigning whether the null hypothesis is accepted\n",
    "    stationary_check = 0 if kpss_test.pvalue < 0.05 else 1\n",
    "\n",
    "    # storing the result in dataframe\n",
    "    df_kpss_output = pd.DataFrame([kpss_test.stat, kpss_test.pvalue, kpss_test.lags, stationary_check],\n",
    "                                index=['Test Statistic','p-value','Number of Lags Used',\n",
    "                                      'Stationary Check']).reset_index()\n",
    "    df_kpss_output.columns = (['Panel','value'])\n",
    "    df_kpss_output['index'] = each_id\n",
    "\n",
    "    for k,v in kpss_test.critical_values.items():\n",
    "        df_kpss_output.loc[df_kpss_output.index.max()+1] = (['Critical Value at {}'.format(k),v,each_id])\n",
    "\n",
    "    df_kpss_output = df_kpss_output.pivot(columns = 'Panel',values = 'value',index = 'index').round(4)\n",
    "    df_kpss_output.index = [x for x in df_kpss_output.index]\n",
    "    \n",
    "    df_kpss_output[['Number of Lags Used', 'Stationary Check']] = df_kpss_output[['Number of Lags Used', 'Stationary Check']].astype(int)\n",
    "    \n",
    "    df_kpss_output.insert(loc=0, column='Panels', value=df_kpss_output.index)\n",
    "    \n",
    "    return df_kpss_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "\n",
    "            df_kpss = pd.DataFrame()\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                # performing KPSS test\n",
    "                kpss_test = KPSS(panel_data[panel_data[panel_col] == each_id][target_var])\n",
    "\n",
    "                df_kpss_output = kpss_analysis(kpss_test, each_id)\n",
    "                df_kpss = df_kpss.append(df_kpss_output)\n",
    "        else:\n",
    "            kpss_test = KPSS(df[target_var])\n",
    "\n",
    "            df_kpss = kpss_analysis(kpss_test, 0)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "\n",
    "    else:\n",
    "        clear_output()\n",
    "        # Showing kpss test summary\n",
    "        display(Markdown('__Results of KPSS Test for `{}`:__'.format(target_var)))\n",
    "        if df_kpss.shape[0] == 1:\n",
    "            display(df_kpss)\n",
    "        else:\n",
    "            display_data(df_kpss.round(4))\n",
    "            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Parametric Test\n",
    "\n",
    "##### Variance Ratio Test\n",
    "\n",
    "__Variance Ratio test__ is one of the most popular semi-parametric tests to check the _random walk_ hypothesis. Note that the variance ratio test is __NOT a unit root test__. This test is used to check whether the observed series is a _random walk_ or it has some predictability.\n",
    "\n",
    "* __Null Hypothesis ($H_0$)__ : The series is a ranodm walk series.\n",
    "* __Alternative Hypothesis ($H_1$)__ : The series is __NOT a random walk__ series.\n",
    "\n",
    "Rejection of the null with a positive test statistic indicates the presence of positive serial correlation in the time series.\n",
    "\n",
    "In `trend` parameter of the function __`VarianceRatio`__, `c` allows for a non-zero drift in the random walk, while `nc` requires that the increments to y are of mean 0.\n",
    "\n",
    "The `lags` must be at least 2, with maximum value that can be added is less than the length of the sample data.\n",
    "\n",
    "__HOW TO INTERPRET__ : If the __p-value__ obtained from the test is __less than the significance level of 0.05__, then we fail to accept the $H_0$, i.e., __there is no random walk__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:25.308691Z",
     "start_time": "2020-06-03T14:59:25.185956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variance Ratio test\n",
    "value=\"Variance Ratio test\"\n",
    "\n",
    "\n",
    "def var_ratio_analysis(var_ratio_test, each_id):\n",
    "    # assigning whether the null hypothesis is accepted\n",
    "    random_walk_check = 0 if var_ratio_test.pvalue < 0.05 else 1\n",
    "\n",
    "    # storing the result in the dataframe\n",
    "    df_var_ratio_output = pd.DataFrame([var_ratio_test.stat, var_ratio_test.pvalue, \n",
    "                                        var_ratio_test.lags, random_walk_check],\n",
    "                                       index=['Test Statistic','p-value',\n",
    "                                              'Number of Lags Used','Random Walk Check']).reset_index()\n",
    "    df_var_ratio_output.columns = (['Panel','value'])\n",
    "    df_var_ratio_output['index'] = each_id\n",
    "\n",
    "    for k,v in var_ratio_test.critical_values.items():\n",
    "        df_var_ratio_output.loc[df_var_ratio_output.index.max()+1] = (['Critical Value at {}'.format(k),v,each_id])\n",
    "\n",
    "    df_var_ratio_output = df_var_ratio_output.pivot(columns = 'Panel',values = 'value',index = 'index').round(4)\n",
    "    df_var_ratio_output.index = [x for x in df_var_ratio_output.index]\n",
    "    \n",
    "    df_var_ratio_output[['Number of Lags Used', 'Random Walk Check']] = df_var_ratio_output[['Number of Lags Used', 'Random Walk Check']].astype(int)\n",
    "    \n",
    "    df_var_ratio_output.insert(loc=0, column='Panels', value=df_var_ratio_output.index)\n",
    "    \n",
    "    return df_var_ratio_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "\n",
    "            df_var_ratio = pd.DataFrame()\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                # performing the variance ratio test\n",
    "                var_ratio_test = VarianceRatio(panel_data[panel_data[panel_col] == each_id][target_var])\n",
    "\n",
    "                df_var_ratio_output = var_ratio_analysis(var_ratio_test, each_id)\n",
    "\n",
    "                df_var_ratio = df_var_ratio.append(df_var_ratio_output)\n",
    "\n",
    "        else:\n",
    "            var_ratio_test = VarianceRatio(df[target_var])\n",
    "\n",
    "            df_var_ratio = var_ratio_analysis(var_ratio_test, 0)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "\n",
    "    else:\n",
    "        clear_output()\n",
    "        # Showing var_ratio test summary\n",
    "        display(Markdown('__Results of Variance Ratio Test for `{}`:__'.format(target_var)))\n",
    "        if df_var_ratio.shape[0] == 1:\n",
    "            display(df_var_ratio)\n",
    "        else:\n",
    "            display_data(df_var_ratio.round(4))\n",
    "            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric Test\n",
    "\n",
    "##### Phillips-Perron Test\n",
    "\n",
    "Compared with the ADF test, __Phillips-Perron__ unit root test makes correction to the test statistics and is robust to the unspecified autocorrelation and heteroscedasticity in the errors. There are two types of test statistics, $Z_{\\rho}$ and $Z_{\\tau}$, which have the same asymptotic distributions as ADF statistic. $Z_{\\tau}$ (default) is based on the t-stat and $Z_{\\rho}$ uses a test based on the length of time-series (`nobs`) times the re-centered regression coefficient.\n",
    "\n",
    "* __Null Hypothesis ($H_0$)__ : The series contains __unit root__, and hence it is non-stationary.\n",
    "* __Alternative Hypothesis ($H_1$)__ : The series is weakly stationary.\n",
    "\n",
    "The function __`PhillipsPerron`__ has the `trend` parameter with the following 3 options: \n",
    "* `nc` - `N`o `C`onstant trend\n",
    "* `c` - `C` onstant trend (default) \n",
    "* `ct` - `C`onstant and linear `T`ime trend\n",
    "\n",
    "Also, the `lag` parameter can be manually added with maximum value being less than the length of the sample data, or automatically set to `12*(nobs/100)**(1/4)`.\n",
    "\n",
    "__HOW TO INTERPRET__ : If the __p-value__ obtained from the test is __less than the significance level of 0.05__, then we fail to accept the $H_0$, i.e., __the series is stationary__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:26.642586Z",
     "start_time": "2020-06-03T14:59:26.547845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Phillips-Perron test\n",
    "value=\"Phillips-Perron test\"\n",
    "\n",
    "\n",
    "def pp_analysis(pp_test, each_id):\n",
    "    # assigning whether the null hypothesis is accepted\n",
    "    stationary_check = 1 if pp_test.pvalue < 0.05 else 0\n",
    "\n",
    "    # storing the result in dataframe\n",
    "    df_pp_output = pd.DataFrame([pp_test.stat, pp_test.pvalue, pp_test.lags, stationary_check],\n",
    "                                index=['Test Statistic','p-value','Number of Lags Used',\n",
    "                                      'Stationary Check']).reset_index()\n",
    "    df_pp_output.columns = (['Panel','value'])\n",
    "    df_pp_output['index'] = each_id\n",
    "\n",
    "    for k,v in pp_test.critical_values.items():\n",
    "        df_pp_output.loc[df_pp_output.index.max()+1] = (['Critical Value at {}'.format(k),v,each_id])\n",
    "\n",
    "    df_pp_output = df_pp_output.pivot(columns = 'Panel',values = 'value',index = 'index').round(4)\n",
    "    df_pp_output.index = [x for x in df_pp_output.index]\n",
    "    \n",
    "    df_pp_output[['Number of Lags Used', 'Stationary Check']] = df_pp_output[['Number of Lags Used', 'Stationary Check']].astype(int)\n",
    "    \n",
    "    df_pp_output.insert(loc=0, column='Panels', value=df_pp_output.index)\n",
    "    \n",
    "    return df_pp_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "\n",
    "            df_pp = pd.DataFrame()\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                # performing the PP test\n",
    "                pp_test = PhillipsPerron(panel_data[panel_data[panel_col] == each_id][target_var])\n",
    "\n",
    "                df_pp_output = pp_analysis(pp_test, each_id)\n",
    "\n",
    "                df_pp = df_pp.append(df_pp_output)\n",
    "\n",
    "        else:\n",
    "            pp_test = PhillipsPerron(df[target_var])\n",
    "\n",
    "            df_pp = pp_analysis(pp_test, 0)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "\n",
    "    else:\n",
    "        clear_output()\n",
    "        # Showing pp test summary\n",
    "        display(Markdown('__Results of Phillips-Perron Test for `{}`:__'.format(target_var)))\n",
    "        if df_pp.shape[0] == 1:\n",
    "            display(df_pp)    \n",
    "        else:\n",
    "            display_data(df_pp.round(4))\n",
    "            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation plots\n",
    "\n",
    "__Auto-Correlation__ and __Partial Auto-Correlation__ are measures of association between current and past series values to indicate which past series values are most useful in predicting future values. These plots can be used to determine the auto-regressive and moving average components of your forecasting model.\n",
    "\n",
    "* __Auto-Correlation Function (ACF):__ Autocorrelation is the correlation between a signal's observations as a function of the time-lag between them. So, __ACF plot__ is a plot of total correlation between different lag functions. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise.\n",
    "\n",
    "* __Partial Auto-Correlation Function (PACF):__ PACF plot is a plot of the partial correlation of a stationary time series with its own lagged values, controlling for the values of the time series at all shorter lags.\n",
    "\n",
    "__HOW TO INTERPRET__ : \n",
    "\n",
    "* If the PACF displays a sharp cutoff while the ACF decays more slowly (i.e., has significant spikes at higher lags), we say that the stationarized series displays an `AR signature`, meaning that the autocorrelation pattern can be explained more easily by adding AR terms.\n",
    "* Alternatively, if there is no correlation between $x(t)$ and $x(t - (n-1))$, the autocorrelation plot cuts off sharply at the $n^{th}$ lag, and the time series is said to display an `MA signature` i.e it is over differenced.\n",
    "\n",
    "**NOTE:** Here the plots are generated without differencing the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:31.079919Z",
     "start_time": "2020-06-03T14:59:27.952160Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing ACF and PACF on the target variable across panels\n",
    "value=\"ACF and PACF\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        fig = make_subplots(rows = 2, cols = 1, subplot_titles=('ACF','PACF'))\n",
    "        \n",
    "        def acf_pacf_plot(data,vis):\n",
    "            acf_res, acf_conf = acf(data[target_var], nlags= inp_lag, alpha=.05)\n",
    "            pacf_res, pacf_conf = pacf(data[target_var], nlags= inp_lag, alpha=.05)\n",
    "            # ACF\n",
    "            fig.add_trace(go.Bar(x= list(range(inp_lag)), y= acf_res.tolist(), \n",
    "                                 marker_color = 'maroon', width = 0.07,\n",
    "                                 showlegend= False,visible=vis), \n",
    "                          row = 1,col = 1)\n",
    "            fig.add_trace(go.Scatter(x=list(range(inp_lag)), y=acf_conf[:, 0] - acf_res,\n",
    "                                     line=dict(shape = 'spline',width = 0.01,color='lightgray'),\n",
    "                                     showlegend= False,visible=vis),\n",
    "                          row = 1,col = 1)\n",
    "            fig.add_trace(go.Scatter(x=list(range(inp_lag)), y=acf_conf[:, 1] - acf_res,\n",
    "                                     line=dict(shape = 'spline',width = 0.01,color='lightgray'),\n",
    "                                     showlegend= False,visible=vis, fill='tonexty'),\n",
    "                          row = 1,col = 1)\n",
    "            # PACF\n",
    "            fig.add_trace(go.Bar(x= list(range(inp_lag)), y= pacf_res.tolist(),\n",
    "                                 marker_color = 'maroon', width = 0.07,\n",
    "                                 showlegend= False,visible=vis), \n",
    "                          row = 2,col = 1)\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=list(range(inp_lag)),\n",
    "                                     y=pacf_conf[:, 0] - pacf_res,\n",
    "                                     line=dict(shape = 'spline',width = 0.01,color='lightgray'),\n",
    "                                     showlegend= False,visible=vis), \n",
    "                          row =2,col = 1)\n",
    "            fig.add_trace(go.Scatter(x=list(range(inp_lag)),\n",
    "                                     y=pacf_conf[:, 1] - pacf_res,\n",
    "                                     line=dict(shape = 'spline',width = 0.01,color='lightgray'),\n",
    "                                     showlegend= False,visible=vis, fill='tonexty'),\n",
    "                          row = 2,col = 1)\n",
    "        \n",
    "        \n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            \n",
    "            all_panel_length = []\n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                all_panel_length.append(panel_data.shape[0])\n",
    "\n",
    "            lag_thresh = 30\n",
    "            if min(all_panel_length) <= lag_thresh:\n",
    "                lag_thresh = min(all_panel_length)\n",
    "                display(Markdown('Length of the smallest panel data: __{}__'.format(min(all_panel_length))))\n",
    "\n",
    "            # asking the user to add the lag\n",
    "            while True:\n",
    "                inp_lag = int(input('Enter the lag (should be less than {}): '.format(lag_thresh)))\n",
    "                if inp_lag <= lag_thresh:\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored('Please enter the right lag value.','red',attrs=['bold']))\n",
    "                    continue\n",
    "            \n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            for i, each_id in enumerate(selected_panels):\n",
    "                vis = True if each_id == selected_panels[0] else False\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                acf_pacf_plot(panel_data, vis)\n",
    "            \n",
    "            tab_dict_list = []\n",
    "            for each_id in selected_panels:\n",
    "                vis_check = [[True]*6 if i==each_id else [False]*6 for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
    "                                                    {\"title\": \"ACF-PACF plots for panel: {}\".format(each_id)}],\n",
    "                                            label=each_id, method=\"update\"))\n",
    "\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                             showlegend=False, title_x=0.5)\n",
    "        else:\n",
    "            while True:\n",
    "                inp_lag = int(input('Enter the lag (at max 30): '))\n",
    "                if inp_lag <= 30:\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored('Please enter the right lag value.','red',attrs=['bold']))\n",
    "                    continue\n",
    "            \n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            acf_pacf_plot(df, True)\n",
    "        \n",
    "        fig.update_xaxes(title_text='Lags')\n",
    "        clear_output()\n",
    "        display(Markdown('__ACF-PACF plot for column `{}` generated!__'.format(target_var)))\n",
    "        fig.show(config={'displaylogo': False})\n",
    "        \n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Analysis\n",
    "\n",
    "Spectral analysis is the decomposition of a time series into underlying __sine and cosine__ functions of different frequencies using __Fourier transform__, which allows us to determine __those frequencies that appear particularly strong or important__. This enables us to find underlying periodicities.\n",
    "\n",
    "The spectral intensities are plotted against time.  \n",
    "\n",
    "$$\\begin{aligned}\n",
    "x_{t} = \\sum_{k} a_{k} sin(2 \\pi n ft) + b_{k} cos(2 \\pi nft)\n",
    "\\end{aligned}$$\n",
    "\n",
    "where:  \n",
    "* `f` is the frequency\n",
    "* `k` = `1/f` is the period of seasonality\n",
    "* ($a_{k}$) and ($b_{k}$) are coefficients which can be calculated as $a_{k} = \\sum_{t} x_{t} sin(2 \\pi n ft)$  and  $b_{k} = \\sum_{t} x_{t} cos(2 \\pi n ft)$.\n",
    "\n",
    "The coefficient are usually used to generate spectrum ($s_{k}$) of the data to find out importance of each frequency (`f`). Spectrum is calculated as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "s_{k} = \\frac{1}{2}(a_{k}^2 + b_{k}^2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "For large ($s_{k}$),  $\\frac{k}{n}\\$ is important.\n",
    "\n",
    "__Power-Spectral-Density (PSD)__ analysis is a type of frequency-domain analysis in which a structure is subjected to a probabilistic spectrum of harmonic loading to obtain probabilistic distributions for dynamic response measures.\n",
    "\n",
    "__Interpretation of the spectral plot__ : The spectral intensity will be plotted against `n` times the frequency `f` (`f`,`2f`,`3f`, ..., etc.). The spikes in the spectral plot appear farther along the X axis if the number of seasonal repetitions is greater in the given time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:33.762106Z",
     "start_time": "2020-06-03T14:59:33.702058Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing PSD using periodogram\n",
    "value=\"PSD\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            \n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                freqs, psd = periodogram(panel_data[target_var], window=('tukey', 0.25), detrend='linear')\n",
    "                vis = True if each_id == selected_panels[0] else False\n",
    "                \n",
    "                fig.add_trace(go.Bar(x=freqs, y=psd, width=0.002, visible=vis))\n",
    "                fig.add_trace(go.Scatter(x=freqs, y=psd, mode='markers',opacity=0.8, visible=vis))\n",
    "            \n",
    "            tab_dict_list = []\n",
    "            for each_id in selected_panels:\n",
    "                vis_check = [[True]*2 if i==each_id else [False]*2 for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args = [{\"visible\": vis_check_flat},\n",
    "                                                 {\"title\": \"PSD plots for panel: {}\".format(each_id)}],\n",
    "                                            label=each_id, method=\"update\"))\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                direction=\"right\",\n",
    "                                                x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                                  showlegend=False,title_x=0.5)\n",
    "            \n",
    "        else:\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            freqs, psd = periodogram(df[target_var], window=('tukey', 0.25), detrend='linear')\n",
    "            fig.add_trace(go.Bar(x=freqs, y=psd, width=0.002))\n",
    "            fig.add_trace(go.Scatter(x=freqs, y=psd, mode='markers',opacity=0.8, visible=vis))\n",
    "            \n",
    "        fig.update_xaxes(title_text='Frequency')\n",
    "        fig.update_yaxes(title_text= 'PSD')\n",
    "        \n",
    "        clear_output()\n",
    "        display(Markdown('__PSD plots generated!__'))\n",
    "        fig.show(config={'displaylogo': False})\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Multivariate Analysis\n",
    "\n",
    "First, the user will be allowed to enter the regressor i.e., the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:59:51.356439Z",
     "start_time": "2020-06-03T14:59:36.939003Z"
    }
   },
   "outputs": [],
   "source": [
    "# selecting the regressor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    display(Markdown('\\nEnter the name of column to be used as the __regressor__.'))\n",
    "\n",
    "    # the list of columns in dataframe\n",
    "    original_cols = df.columns\n",
    "    cols_vis = ' || '.join(original_cols)\n",
    "    print(colored(\"\\nColumn Names:\",'grey',attrs=['bold']),\"\\n{}\".format(cols_vis))\n",
    "    print('_'*75)\n",
    "\n",
    "    while True:\n",
    "        reg_var = input(\"\\nEnter the column name: \")\n",
    "        if reg_var not in original_cols:\n",
    "            print(colored(\"\\nPlease enter the column name properly!\",\"red\",attrs=['bold']))\n",
    "            continue\n",
    "        elif reg_var == target_var:\n",
    "            print(colored(\"\\nRegressor and Target variable CANNOT BE SAME!\",\"red\",attrs=['bold']))\n",
    "            continue\n",
    "        else:\n",
    "            clear_output()\n",
    "            display(Markdown(\"The column which will be considered as the __regressor__ is: __{}__\".format(reg_var)))\n",
    "            break\n",
    "else:\n",
    "    reg_var = 'adr'\n",
    "    display(Markdown(\"The column which will be considered as the __regressor__ is: __{}__\".format(reg_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality\n",
    "\n",
    "Granger causality is a way to investigate causality between two variables in a time series, i.e., does one variable directly cause the other.\n",
    "\n",
    "It is based on the idea that if `X` causes `Y`, then the forecast of `Y` based on previous values of `Y` AND the previous values of `X` should outperform the forecast of `Y` based on previous values of `Y` _alone_.\n",
    "\n",
    "* __Null Hypothesis ($H_0$)__ : The __lagged value__ of a regressor does NOT affect the value of the target variable. So, no causation.\n",
    "* __Alternate Hypothesis ($H_1$)__ : The lagged value of a regressor affects the value of the target variable.\n",
    "\n",
    "__HOW TO INTERPRET__ : If the __p-value__ obtained from the test is __less than the significance level of 0.05__, then we fail to accept the $H_0$, i.e., __there is causation__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:03:07.892630Z",
     "start_time": "2020-06-03T14:59:53.358119Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing causality test between two selected series\n",
    "value = 'causality'\n",
    "\n",
    "\n",
    "def caus_analysis(data, each_id):\n",
    "    data_rqrd = data[[target_var, reg_var]]\n",
    "    data_rqrd.index = pd.DatetimeIndex(data[date_time_col].astype(str).to_list())\n",
    "\n",
    "    # getting the correct order using AIC method\n",
    "    order_range = range(1, len(data[target_var]))\n",
    "    rho = [aryule(data[target_var], i, norm='unbiased')[1] for i in order_range]\n",
    "    aic = AIC(len(data[target_var]), rho, order_range).tolist()\n",
    "    min_aic = min(aic)\n",
    "    min_rho_index = aic.index(min_aic)\n",
    "\n",
    "    final_data = data_rqrd.diff(order_range[min_rho_index]).dropna()\n",
    "\n",
    "    model = VAR(final_data)\n",
    "    results = model.fit()\n",
    "    caus_result = results.test_causality(target_var, reg_var, kind='wald')\n",
    "\n",
    "    # assigning whether the null hypothesis is accepted\n",
    "    caus_check = 1 if caus_result.pvalue < 0.05 else 0\n",
    "\n",
    "    # storing the result in a dataframe\n",
    "    df_caus_output = pd.DataFrame([caus_result.test_statistic, caus_result.pvalue, caus_check],\n",
    "                                   index=['Test Statistic','p-value', 'Causality Check']).reset_index()\n",
    "    df_caus_output.columns = (['Panel','value'])\n",
    "    df_caus_output['index'] = each_id\n",
    "\n",
    "    df_caus_output = df_caus_output.pivot(columns = 'Panel',values = 'value',index = 'index')\n",
    "    df_caus_output.index = [x for x in df_caus_output.index]\n",
    "    \n",
    "    df_caus_output['Causality Check'] = df_caus_output['Causality Check'].astype(int)\n",
    "    \n",
    "    df_caus_output.insert(loc=0, column='Panels', value=df_caus_output.index)\n",
    "    \n",
    "    return df_caus_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            df_caus = pd.DataFrame()\n",
    "\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                df_caus_output = caus_analysis(panel_data, each_id)\n",
    "\n",
    "                df_caus = df_caus.append(df_caus_output)\n",
    "\n",
    "        else:\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "            df_caus = caus_analysis(df,0)\n",
    "\n",
    "        clear_output()        \n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "\n",
    "    else:\n",
    "        clear_output()\n",
    "        display(Markdown('__Result of Granger Causality Test between `{}` and `{}`__'.format(target_var, reg_var)))\n",
    "        if df_caus.shape[0] == 1:\n",
    "            display(df_caus)\n",
    "        else:\n",
    "            display_data(df_caus.round(4))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cointegration Test\n",
    "\n",
    "Lets define the __order of integration `d`__ which is the number of differencing required to make a non-stationary time series stationary. Consider a pair of time series, both of which are non-stationary. If we take a particular linear combination of theses series, it can sometimes lead to a stationary series. Such a pair of series would then be termed __cointegrated__, and the `d` is less than that of the individual series.\n",
    "\n",
    "* __Null Hypothesis ($H_0$)__ : There is __NO cointegration__ between the pair of series.\n",
    "* __Alternative Hypoethesis ($H_1$)__: The pair of series are cointegrated.\n",
    "\n",
    "The `trend` parameter included in regression for cointegrating equation has 4 options:\n",
    "* `nc` - `N`o `C`onstant trend\n",
    "* `c` - `C`onstant trend (default) \n",
    "* `ct` - `C`onstant and linear `T`ime trend\n",
    "* `ctt` - `C`onstant and linear `T`ime and quadratic `T`ime trends\n",
    "\n",
    "__HOW TO INTERPRET__ : If the __p-value__ obtained from the test is __less than the significance level of 0.05__, then we fail to accept the $H_0$, i.e., __the pair of series are cointegrated__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:03:36.892660Z",
     "start_time": "2020-06-03T15:03:36.739594Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing cointegration test between 2 selected series\n",
    "value = 'coint test'\n",
    "\n",
    "def coint_analysis(coint_test, each_id):\n",
    "    # assigning whether the null hypothesis is accepted\n",
    "    coint_check = 1 if coint_test[1] < 0.05 else 0\n",
    "\n",
    "    coint_test_stats = list(coint_test[:2])\n",
    "    coint_test_stats.append(coint_check)\n",
    "    coint_conf_intrvl = coint_test[2].tolist()\n",
    "\n",
    "    # storing the result in a dataframe\n",
    "    df_coint_output = pd.DataFrame(coint_test_stats+coint_conf_intrvl,\n",
    "                                   index=['Test Statistic','p-value','Cointegration Check',\n",
    "                                          'Critical Value at 1%','Critical Value at 5%', 'Critical Value at 10%']).reset_index()\n",
    "    df_coint_output.columns = (['Panel','value'])\n",
    "    df_coint_output['index'] = each_id\n",
    "\n",
    "    df_coint_output = df_coint_output.pivot(columns = 'Panel',values = 'value',index = 'index')\n",
    "    df_coint_output.index = [x for x in df_coint_output.index]\n",
    "    \n",
    "    df_coint_output['Cointegration Check'] = df_coint_output['Cointegration Check'].astype(int)\n",
    "    \n",
    "    df_coint_output.insert(loc=0, column='Panels', value=df_coint_output.index)\n",
    "    \n",
    "    return df_coint_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if dataset_type.lower() == 'mp':\n",
    "            df_coint = pd.DataFrame()\n",
    "\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the result</h2></div>'))\n",
    "\n",
    "            # performing the operation on panel level\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "\n",
    "                # perfroming the cointegration test\n",
    "                coint_test = coint(panel_data[panel_data[panel_col] == each_id][target_var],\n",
    "                                   panel_data[panel_data[panel_col] == each_id][reg_var])\n",
    "\n",
    "                df_coint_output = coint_analysis(coint_test, each_id)\n",
    "\n",
    "                df_coint = df_coint.append(df_coint_output)\n",
    "\n",
    "        else:\n",
    "            # perfroming the cointegration test\n",
    "            coint_test = coint(df[target_var], df[reg_var])\n",
    "\n",
    "            df_coint = coint_analysis(coint_test, 0)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "\n",
    "    else:\n",
    "        clear_output()\n",
    "        display(Markdown('__Result of Augmented Engle-Granger 2-step Cointegration Test between `{}` and `{}`__'.format(target_var, reg_var)))\n",
    "        if df_coint.shape[0] == 1:\n",
    "            display(df_coint)\n",
    "        else:\n",
    "            display_data(df_coint.round(4))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-correlation\n",
    "\n",
    "Here, the user will get an option to select another column that is different from the target column. This code will generate correlation value of __lag/lead differentiated series__ against the target series to find out if there is any correlation available between the lagged/lead version of one series against the target series.\n",
    "\n",
    "The maximum number of lags against which this can be performed is 1 less than the length of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:03:41.455035Z",
     "start_time": "2020-06-03T15:03:38.957159Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing cross correlation\n",
    "value='cross-corr'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    def cross_corr_analysis(data, cross_cor_lag, vis):\n",
    "        d1 = data[target_var]\n",
    "        d2 = data[reg_var]\n",
    "\n",
    "        # calculating the cross correlation between the 2 series against a range of lag\n",
    "        cross_corr_res = [d1.corr(d2.shift(lag)) for lag in range(-cross_cor_lag,cross_cor_lag+1)]\n",
    "        cross_corr_res = [0 if isnan(x) else x for x in cross_corr_res]\n",
    "\n",
    "        # generating the plot\n",
    "        fig.add_trace(go.Bar(x=list(range(-cross_cor_lag,cross_cor_lag+1)), \n",
    "                             y=cross_corr_res, width=0.25, visible=vis))\n",
    "        fig.add_trace(go.Scatter(x=list(range(-cross_cor_lag,cross_cor_lag+1)), \n",
    "                                 y=cross_corr_res, mode='markers',opacity=0.8, visible=vis))\n",
    "    \n",
    "    try:\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "\n",
    "            cross_corr_plot_slots = [Panel() for i in range(len(selected_panels))]\n",
    "\n",
    "            all_panel_length = []\n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                all_panel_length.append(panel_data.shape[0])\n",
    "\n",
    "            lag_thresh = 30\n",
    "            if min(all_panel_length) < lag_thresh:\n",
    "                lag_thresh = min(all_panel_length)\n",
    "                display(Markdown('Length of the smallest panel data: __{}__'.format(min(all_panel_length))))\n",
    "\n",
    "            # asking the user to add the lag\n",
    "            while True:\n",
    "                cross_cor_lag = int(input('Enter the lag (should be less than {}): '.format(lag_thresh)))\n",
    "                if cross_cor_lag <= lag_thresh:\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored('Please enter the right lag value.','red',attrs=['bold']))\n",
    "                    continue\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "\n",
    "            # performing the operation on panel level\n",
    "            for each_id in selected_panels:\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                vis = True if each_id == selected_panels[0] else False\n",
    "                cross_corr_plot = cross_corr_analysis(panel_data, cross_cor_lag, vis)\n",
    "            \n",
    "            tab_dict_list = []\n",
    "            for each_id in selected_panels:\n",
    "                vis_check = [[True]*2 if i==each_id else [False]*2 for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args=[{\"visible\": vis_check_flat},\n",
    "                                                {\"title\": \"Decomposition plots for panel: {}\".format(each_id)}],\n",
    "                                          label=each_id, method=\"update\"))\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                                  showlegend=False, title_x=0.5)\n",
    "\n",
    "        else:\n",
    "            lag_thresh = 30\n",
    "            while True:\n",
    "                cross_cor_lag = int(input('Enter the lag (at max 30): '))\n",
    "                if cross_cor_lag <= lag_thresh:\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored('Please enter the right lag value.','red',attrs=['bold']))\n",
    "                    continue\n",
    "\n",
    "            cross_corr_plot_layout = cross_corr_analysis(df, cross_cor_lag, True)\n",
    "            fig.update_layout(showlegend=False)\n",
    "\n",
    "        fig.update_xaxes(title='Lags')\n",
    "        fig.update_yaxes(title='Corr. Coeff.')\n",
    "        clear_output()\n",
    "        display(Markdown('__The cross-correlation plots between `{}` and `{}` are generated.__'.format(target_var, reg_var)))\n",
    "        display(Markdown('__Range of lags selected__: -{0} to {0}'.format(cross_cor_lag)))\n",
    "        fig.show(config={'displaylogo':False})\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-stationary to Stationary Conversion\n",
    "\n",
    "## Data Differencing\n",
    "\n",
    "In order to remove non-stationarity of a time-series, one of the simplest and most popular way is to take a difference of the observed series with it's $k^{th}$  lag. This technique is used to eliminate trend stationarity from the time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:04:07.558806Z",
     "start_time": "2020-06-03T15:03:44.464823Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing data differencing to remove non-stationarity\n",
    "value = 'data diff'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "\n",
    "        def data_diff_generate(data, differ_order, vis):\n",
    "            diff_panel_data = data[target_var].diff(periods = differ_order)\n",
    "\n",
    "            # storing the value in a dataframe\n",
    "            target_var_mod = target_var+'_mod'\n",
    "            diff_panel_data_df = pd.DataFrame({target_var_mod:diff_panel_data})\n",
    "\n",
    "            # Add traces\n",
    "            fig.add_trace(go.Scatter(x=data[date_time_col], y=data[target_var], \n",
    "                                     name=\"actual {}\".format(target_var), visible=vis), secondary_y=False)\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=data[date_time_col], y=diff_panel_data, \n",
    "                                     name=\"differenced {}\".format(target_var), visible=vis), secondary_y=True)\n",
    "\n",
    "            fig.update_layout(hovermode='x unified')\n",
    "\n",
    "            fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=-0.1))\n",
    "\n",
    "            # Set y-axes titles\n",
    "            fig.update_yaxes(title_text=\"<b>actual</b> observation\", secondary_y=False)\n",
    "            fig.update_yaxes(title_text=\"<b>differenced</b> observation\", secondary_y=True)\n",
    "\n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        \n",
    "        if dataset_type.lower() == 'mp':\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "            # H453700C08, H346600C03, H136400C09, H635900C03\n",
    "\n",
    "            all_panel_length = []\n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                all_panel_length.append(panel_data.shape[0])\n",
    "\n",
    "            lag_thresh = 30\n",
    "            if min(all_panel_length) < lag_thresh:\n",
    "                lag_thresh = min(all_panel_length)\n",
    "                display(Markdown('Length of the smallest panel data: __{}__'.format(min(all_panel_length))))\n",
    "\n",
    "            # asking the user to add the lag\n",
    "            while True:\n",
    "                differ_order = int(input('Enter the lag (should be less than {}): '.format(lag_thresh)))\n",
    "                if differ_order <= lag_thresh:\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored('Please enter the right lag value.','red',attrs=['bold']))\n",
    "                    continue\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "            \n",
    "            # performing the operation on panel level\n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col).get_group(each_id)\n",
    "                visible = True if each_id == selected_panels[0] else False\n",
    "                # calling the function to generate the plot\n",
    "                data_diff_generate(panel_data, differ_order,visible)\n",
    "                \n",
    "            tab_dict_list = []\n",
    "            for each_id in selected_panels:\n",
    "                vis_check = [[True]*2 if i==each_id else [False]*2 for i in selected_panels]\n",
    "                vis_check_flat = [i for sublist in vis_check for i in sublist]\n",
    "                tab_dict_list.append(dict(args=[{\"visible\": vis_check_flat},\n",
    "                                                {\"title\": \"Data differenced plot for panel: {}\".format(each_id)}],\n",
    "                                          label=each_id, method=\"update\"))\n",
    "                fig.update_layout(updatemenus=[dict(buttons=list(tab_dict_list),\n",
    "                                                    direction=\"right\", x=0, xanchor=\"left\", y=1.11, yanchor=\"top\")],\n",
    "                                  showlegend=False, title_x=0.5)\n",
    "\n",
    "        else:\n",
    "\n",
    "            while True:\n",
    "                differ_order = int(input('Enter the lag (at max 30): '))\n",
    "                if differ_order <= 30:\n",
    "                    break\n",
    "                else:\n",
    "                    print(colored('Please enter the right lag value.','red',attrs=['bold']))\n",
    "                    continue\n",
    "\n",
    "            display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Generating the plots</h2></div>'))\n",
    "\n",
    "            # calling the function to generate the plot\n",
    "            data_diff_generate(df, differ_order, True)\n",
    "            \n",
    "        clear_output()\n",
    "        display(Markdown('__Data Differencing Plots for `{}` generated!__'.format(target_var)))\n",
    "        display(Markdown('__Order of differencing__ : {}'.format(differ_order)))\n",
    "        fig.show(config={'displaylogo': False})\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Dynamic Time Warping\n",
    "\n",
    "Dynamic time warping (DTW) is a family of algorithms which compute the local stretch or compression to apply to the time axes of 2 time-series in order to optimally map one (query) onto the other (reference). DTW computes the euclidean distance at each frame across every other frames to compute the minimum path that will match the two signals. \n",
    "\n",
    "The greatest advantage of this method is that it can also deal with signals of different length. One downside is that it cannot deal with missing values so you would need to interpolate beforehand if you have missing data points.\n",
    "\n",
    "At the core of the algorithms, the model proceeds as follows:\n",
    "\n",
    "1. Divide the two series into equal points.\n",
    "2. Calculate the __Euclidean distance__ between the $1^{st}$ point in the $1^{st}$ series and every point in the $2^{nd}$ series. Store the minimum distance calculated. This is the __time warp__ stage.\n",
    "3. Move to the $2^{nd}$ point and __repeat `2`__. Accordingly, move step by step along with points and _repeat 2_ till all points are exhausted.\n",
    "4. Now __repeat `2` and `3`__ but with the $2^{nd}$ series as a reference point.\n",
    "5. Add up all the minimum distances that were stored and this is a true measure of similarity between the two series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DTW Clustering\n",
    "\n",
    "Here, we are performing agglomerative clustering against the target variable to show which panels exhibits similar characteristics, and which one of them possess abnormal behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:04:27.992627Z",
     "start_time": "2020-06-03T15:04:24.904996Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# performing dtw and see which series are approximately similar\n",
    "value = 'dtw'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        def add_distance(ddata, dist_threshold=None, fontsize=8):\n",
    "            '''\n",
    "            Function to plot cluster points & distance labels in dendrogram\n",
    "\n",
    "            Arguments\n",
    "                ddata: scipy dendrogram output\n",
    "                dist_threshold: distance threshold where label will be drawn, if None, 1/10 from base leafs will not be labelled to prevent clustter\n",
    "                fontsize: size of distance labels\n",
    "            '''\n",
    "            if dist_threshold==None:\n",
    "                # add labels except for 1/10 from base leaf nodes\n",
    "                dist_threshold = max([a for i in ddata['dcoord'] for a in i])/10\n",
    "\n",
    "            for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "                y = sum(i[1:3])/2\n",
    "                x = d[1]\n",
    "                # only label above distance threshold\n",
    "                if x > dist_threshold:\n",
    "                    plt.plot(x, y, 'o', c=c, markeredgewidth=0)\n",
    "                    plt.annotate(int(x), (x, y), xytext=(15, 3),\n",
    "                                 textcoords='offset points', va='top', \n",
    "                                 ha='center', fontsize=15)\n",
    "\n",
    "\n",
    "        def maxclust_draw(data):\n",
    "            '''\n",
    "            Function to draw agglomerative clustering dendrogram based on maximum cluster criteron\n",
    "\n",
    "            input:\n",
    "                data       : dataframe or arrays of timeseries\n",
    "                max_cluster: maximum cluster size to flatten cluster\n",
    "\n",
    "            return:\n",
    "                Plot of Dendrogram with timeseries graphs on the side\n",
    "            '''\n",
    "            max_cluster = data.shape[0]\n",
    "            # define gridspec space\n",
    "            gs = gridspec.GridSpec(max_cluster,60)\n",
    "\n",
    "            # add dendrogram to gridspec\n",
    "            fig_height = 40\n",
    "            if max_cluster > 50:\n",
    "                fig_height = max_cluster\n",
    "\n",
    "            plt.figure(figsize=(25, fig_height), facecolor=\"w\")\n",
    "            plt.subplot(gs[:, 0:35])\n",
    "            plt.xlabel('Distance', fontsize=15)\n",
    "            plt.ylabel('Cluster', fontsize=15)\n",
    "\n",
    "            data_series = np.matrix(data)\n",
    "\n",
    "            # Custom Hierarchical clustering\n",
    "            model = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
    "            # Augment Hierarchical object to keep track of the full tree\n",
    "            h_model = clustering.HierarchicalTree(model, method='average')\n",
    "\n",
    "            # Fit Model:\n",
    "            h_model.fit(series=data_series)\n",
    "            \n",
    "            linkage_val = h_model.linkage\n",
    "\n",
    "            ddata = dendrogram(linkage_val, orientation='left',\n",
    "                               leaf_font_size=15, labels=data.T.columns)\n",
    "\n",
    "            # add distance labels in dendrogram\n",
    "            add_distance(ddata)\n",
    "\n",
    "            # add timeseries graphs to gridspec\n",
    "            for cluster in range(1,max_cluster+1):\n",
    "                reverse_plot = max_cluster+1-cluster\n",
    "                plt.subplot(gs[reverse_plot-1:reverse_plot, 45:60])\n",
    "\n",
    "                cluster_id = ddata['ivl'][cluster-1]\n",
    "                plt.plot(data.T[cluster_id])\n",
    "\n",
    "                plt.tick_params(axis='y', which='both', labelleft=False, labelright=True)\n",
    "                if cluster-1 != 0:\n",
    "                    plt.xticks([])\n",
    "                else:\n",
    "                    plt.xticks(rotation=90)\n",
    "\n",
    "        if dataset_type.lower() == 'mp':\n",
    "\n",
    "            selected_panels = panel_selection(panel_ids)\n",
    "\n",
    "            # creating the dataframe to be used to create dendogram\n",
    "            dendogram_df = pd.DataFrame()\n",
    "            for idx,each_id in enumerate(selected_panels):\n",
    "                panel_data = df.groupby(panel_col)[date_time_col,target_var].get_group(each_id)\n",
    "                dendogram_df[each_id] = panel_data[target_var].to_list()\n",
    "\n",
    "            dendogram_df.index = panel_data[date_time_col]\n",
    "\n",
    "            clear_output()\n",
    "            display(Markdown('__DTW Clustering Plot for `{}` generated!__'.format(target_var)))\n",
    "            maxclust_draw(dendogram_df.T)\n",
    "\n",
    "        else:\n",
    "            df_dtw = df[num_cols].copy()\n",
    "            for each_col in num_cols:\n",
    "                df_dtw[each_col] = (df[each_col] - df[each_col].mean())/df[each_col].std()\n",
    "            \n",
    "            display(Markdown('__DTW Clustering Plot for all numerical columns generated!__'))\n",
    "            maxclust_draw(df_dtw.T)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Factor Analysis\n",
    "\n",
    "Factor Analysis provides the tool for analyzing the structure of the interrelationships among a large number of variables by defining a set of highly interrelated variables, known as __factors__ or __components__. It helps in data interpretations by reducing the number of variables.\n",
    "\n",
    "There are 2 types of factor analysis:\n",
    "* __Exploratory Factor Analysis (EFA)__: It is the most popular factor analysis approach among social and management researchers. Its basic assumption is that any observed variable is directly associated with any factor.\n",
    "* __Confirmatory Factor Analysis (CFA)__: Its basic assumption is that each factor is associated with a particular set of observed variables. CFA confirms what is expected on the basis.\n",
    "\n",
    "_In this notebook, we will be covering EFA._\n",
    "\n",
    "##### How does factor analysis work?\n",
    "As mentioned, the primary objective of factor analysis is to reduce the number of observed variables and find unobservable variables, and it can be achieved in 3 steps:\n",
    "* __Step 1 - Adequacy Test__: We need to evaluate the factorability of our dataset. Factorability means \"can we found the factors in the dataset?\" If yes, then only we can proceed for the remaining steps, else ignore.\n",
    "* __Step 2 - Factor Extraction__: In this step, the number of factors and factor loading is performed.\n",
    "* __Step 3 - Factor Rotation__: In this step, rotation tries to convert factors into __uncorrelated factors__ i.e., making the factors independent of each other. There are lots of rotation methods available, which are performed [here](#fact_rot), such as:\n",
    "    <a name='rot_types'></a>\n",
    "    * __Orthogonal__ rotation: _varimax, oblimax, quartimax, equamax_\n",
    "    * __Oblique__ rotation: _promax, oblimin, quartimin_\n",
    "\n",
    "\n",
    "##### Step 1. : Is Factor Analysis necessary?\n",
    "\n",
    "1. _If a visual inspection reveals no substantial number of correlations greater than .30, then factor analysis is __probably inappropriate__._ The correlations among variables can also be analyzed by computing the __partial correlations__ among variables. A partial correlation is a correlation that is unexplained when the effects of other variables are taken into account. _If true factors exist in the data, the partial correlation should be small, because the variable can be explained by the variables loading on the factors. __If the partial correlations are high, indicating no underlying factors, then factor analysis is inappropriate__._\n",
    "\n",
    "2. __Bartletts Test of Sphericity__ _(a measure of how closely the shape of an object resembles that of a perfect sphere)_ checks whether or not an observed matrix is significantly different from an identity matrix. Small values __(less than 0.05)__ of the significance level indicate that __factor analysis may be useful with the data__.\n",
    "\n",
    "3. __Measure of Sampling Adequacy (MSA)__ checks if it is possible to factorize the main variables efficiently. It ranges from 0 to 1, reaching 1 when each variable is _perfectly predicted without error by the other variables_. The measure can be interpreted with the following guidelines: \n",
    "    * => 0.80 -> meritorious\n",
    "    * => 0.70 -> middling\n",
    "    * => 0.60 -> mediocre\n",
    "    * => 0.50 -> miserable\n",
    "    * < 0.50  -> unacceptable\n",
    "    \n",
    "   The MSA increases as \n",
    "   * the sample size increases, OR\n",
    "   * the average correlations increase, OR\n",
    "   * the number of variables increases, OR\n",
    "   * the number of factors decreases\n",
    "       \n",
    "   __MSA values must exceed .50__ for both the overall test and each variable - variables with values less than .50 should be omitted from the factor analysis one at a time, with the smallest one being omitted each time. __Kaiser-Meyer-Olkin (KMO) Test__ is a measure of the adequacy of sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for factor analysis\n",
    "\n",
    "In this section the categorical columns are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:31.510432Z",
     "start_time": "2020-06-03T15:05:31.099100Z"
    }
   },
   "outputs": [],
   "source": [
    "# finding the variances of the columns\n",
    "value=\"FA data preprocess\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # create a copy of the dataframe\n",
    "        df_fact_anal = df.copy()\n",
    "        cols_to_be_used = num_cols.copy()\n",
    "        cols_to_be_used.extend(cat_cols)\n",
    "        df_fact_anal = df_fact_anal[cols_to_be_used]\n",
    "        \n",
    "        if cat_cols == []:\n",
    "            pass\n",
    "        else:\n",
    "            # perform one hot encoding to the categorical columns\n",
    "            df_fact_anal = pd.get_dummies(df_fact_anal, prefix=cat_cols)\n",
    "            df_fact_anal = df_fact_anal.rename(columns = lambda x: (x.replace(' ','_')))\n",
    "        \n",
    "        if num_cols == []:\n",
    "            pass\n",
    "        else:\n",
    "            if dataset_type.lower() == 'mp':\n",
    "                df_fact_anal[panel_col] = df[panel_col]\n",
    "                selected_panels = panel_selection(panel_ids)\n",
    "                \n",
    "                display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Processing</h2></div>'))\n",
    "                \n",
    "                def func_scale(x):\n",
    "                    return (x - x.mean())/x.std()\n",
    "                \n",
    "                df_fact_anal_final = pd.DataFrame()\n",
    "                for idx,each_id in enumerate(selected_panels):\n",
    "                    panel_data = df_fact_anal.groupby(panel_col).get_group(each_id)\n",
    "                    \n",
    "                    # perform scaling\n",
    "                    panel_data[num_cols] = panel_data[num_cols].apply(func_scale, axis=0)\n",
    "                    df_fact_anal_final = df_fact_anal_final.append(panel_data.fillna(0), ignore_index=True)\n",
    "                \n",
    "                df_fact_anal = df_fact_anal_final.copy()\n",
    "                df_fact_anal.drop(panel_col, axis=1, inplace=True)\n",
    "            else:\n",
    "                for each_col in num_cols:\n",
    "                    df_fact_anal[each_col] = (df[each_col] - df[each_col].mean())/df[each_col].std()\n",
    "        \n",
    "        # lower-case and replace any white-space/dot/comma seperator with '_' for every column name\n",
    "        df_fact_anal = df_fact_anal.rename(columns = lambda x: x.replace(' ','_').replace('.','_').replace(',','_').replace(\"'\",\"\"))\n",
    "        \n",
    "        df_fact_anal = df_fact_anal.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        clear_output()\n",
    "        display(Markdown('Displaying the __preprocessed dataframe__ :'))\n",
    "        display_data(df_fact_anal.round(4))\n",
    "        \n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance check for scaled variables\n",
    "\n",
    "Here, we will calculate the variance of each column and if we notice any column giving variance 0, then those columns will be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:36.036786Z",
     "start_time": "2020-06-03T15:05:35.973009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# performing variance check on the scaled dataset\n",
    "value='variance check fa'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # calculating the variance for each column and storing it in dataframe\n",
    "        df_fact_anal_var = pd.DataFrame(data = df_fact_anal.var().round(4),\n",
    "                                        columns = ['variance']).reset_index()\n",
    "        df_fact_anal_var.rename(columns = {'index': 'column_name'}, inplace=True)\n",
    "\n",
    "        # filtering out the rows having variance 0\n",
    "        df_fact_anal_var_0 = df_fact_anal_var[df_fact_anal_var['variance']==0]\n",
    "        \n",
    "        if df_fact_anal_var_0.shape[0] == 0:\n",
    "            display(Markdown('__No columns have been dropped in this process!__'))\n",
    "        else:\n",
    "            display(display_data(df_fact_anal_var_0.round(4)))\n",
    "\n",
    "            # drop the columns having 0 variance\n",
    "            var_0_cols = df_fact_anal_var_0['column_name'].to_list()\n",
    "            df_fact_anal.drop(var_0_cols, axis=1, inplace=True)\n",
    "\n",
    "            display(Markdown('The above listed __{} columns have been dropped__ from the scaled dataframe.'.format(df_fact_anal_var_0.shape[0])))\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T12:37:22.491110Z",
     "start_time": "2020-04-13T12:37:22.486957Z"
    }
   },
   "source": [
    "## Singularity check for variables\n",
    "\n",
    "Here, we need to check if data is singular (i.e. determinant is zero). This can be checked and handled using __Variance Inflation Factor (VIF)__ which will help us to drop the columns causing singularity.\n",
    "\n",
    "VIF estimates how much the variance of a coefficient is _\"inflated\"_ because of __linear dependence with other predictors__. Thus, a __VIF of 1.4__ tells us that the variance of a particular coefficient is __40% larger__ than it would be if that predictor was completely uncorrelated with all the other predictors.\n",
    "\n",
    "The VIF has a lower bound of 1 but the upper bound varies depending on the problem statement. In order to learn more about the threshold, check this [link](https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/).\n",
    "\n",
    "In order to know at what scenarios a high VIF is not a problem and can be safely ignored, check this [link](https://statisticalhorizons.com/multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:38.522812Z",
     "start_time": "2020-06-03T15:05:37.484406Z"
    }
   },
   "outputs": [],
   "source": [
    "# finding the columns responsible for singularity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "value=\"singularity check fa\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Processing</h2></div>'))\n",
    "        \n",
    "        vif_thresh = 5\n",
    "\n",
    "        fa_variables = list(range(df_fact_anal.shape[1]))\n",
    "        col_to_drop_for_fa = []\n",
    "\n",
    "        # list of columns and their index stored in `variable`\n",
    "        df_fa_cols = df_fact_anal.columns\n",
    "        fa_variables = np.arange(df_fact_anal.shape[1])\n",
    "\n",
    "        # enter infinite loop and iterate through it until all the columns causing singularity is removed\n",
    "        while True:    \n",
    "            # obtaining the matrix with updated column names\n",
    "            c = df_fact_anal[df_fa_cols[fa_variables]].values\n",
    "\n",
    "            # generating the VIF for the available columns\n",
    "            vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "\n",
    "            # check the max value of that specific VIF\n",
    "            maxloc = vif.index(max(vif))\n",
    "\n",
    "            # if it crosses the threshold, then remove the column generating that max value\n",
    "            if max(vif) > vif_thresh:\n",
    "                col_to_drop_for_fa.append(df_fact_anal.iloc[:, fa_variables].columns[maxloc])\n",
    "                # drop the column\n",
    "                fa_variables = np.delete(fa_variables, maxloc)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        clear_output()\n",
    "        display(Markdown('__Columns dropped for causing singularity:__'))\n",
    "        print('{}'.format(' || '.join(col_to_drop_for_fa)))\n",
    "\n",
    "        fact_anal_cols = df_fact_anal.columns\n",
    "\n",
    "        # columns on which factor analysis wiil be performed\n",
    "        col_to_retain_for_fa = set(fact_anal_cols) - set(col_to_drop_for_fa)\n",
    "\n",
    "        if col_to_drop_for_fa == []:\n",
    "            print('No columns to be eliminated!')\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                # removing columns entered by user for Bartlett's test only\n",
    "                df_fact_anal.drop(col_to_drop_for_fa, axis=1, inplace=True)\n",
    "\n",
    "                # display updated list of column names\n",
    "                updated_fact_anal_cols_vis = ' || '.join(col_to_retain_for_fa)\n",
    "                print(colored(\"\\nColumns Retained:\",'blue',attrs=['bold']),\"\\n{}\".format(updated_fact_anal_cols_vis))\n",
    "\n",
    "                track_cell(value, flag)\n",
    "            except Exception as err:\n",
    "                print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
    "                flag = 0\n",
    "                err = str(err)\n",
    "                track_cell(value, flag, err)\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "    else:\n",
    "        display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><i>CODE EXECUTED!</i> Continue executing the next cell for performing <b>Adequacy Test</b>.</span>'))\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adequacy Test\n",
    "\n",
    "The adequacy test is done to understand if the dataset has enough factors to go ahead with factor analysis.\n",
    "Henry Kaiser (1970) introduced a Measure of Sampling Adequacy (MSA) of factor analytic data matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:40.731044Z",
     "start_time": "2020-06-03T15:05:40.596050Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing Bartletts Test of Sphericity and  Kaiser-Meyer-Olkin (KMO) Test\n",
    "value=\"Bartlett's and KMO\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    try:\n",
    "        # performing Bartlett's test\n",
    "        chi_square_value, p_value = calculate_bartlett_sphericity(df_fact_anal)\n",
    "        display(Markdown('__Bartletts test of Sphericity__'))\n",
    "        display(Markdown('_Chi-Squared Value_ : __{}__'.format(chi_square_value.round(3))))\n",
    "        display(Markdown('_p-value_ : __{}__'.format(p_value)))\n",
    "\n",
    "        # performing Kaiser-Meyer-Olkin (KMO) Test\n",
    "        kmo_per_variable, kmo_total = calculate_kmo(df_fact_anal)\n",
    "        display(Markdown('__Kaiser-Meyer-Olkin (KMO) Test__'))\n",
    "        display(Markdown('_The KMO score overall (MSA)_  = __{}__'.format(kmo_total.round(3))))\n",
    "\n",
    "        if p_value > 0.05 or kmo_total < 0.5:\n",
    "            print(colored('DON\\'T PROCEED for Factor Analysis!','red',attrs=['bold']))\n",
    "        elif isnan(p_value) or isnan(kmo_total):\n",
    "            print(colored('You have singular matrix! CAN\\'T PROCEED for Factor Analysis!','red',attrs=['bold']))\n",
    "        else:\n",
    "            print(colored('PROCEED for Factor Analysis!','green',attrs=['bold']))\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree plot\n",
    "\n",
    "##### Step 2.1. : How to obtain the right number of factors?\n",
    "\n",
    "Any decision on the number of factors to be retained should be based on several considerations:\n",
    "\n",
    "* __Latent Root Criterion__: This is the most commonly used technique. The rationale for this criterion is that any individual factor should account for the variance of at least a single variable if it is to be retained for interpretation. Thus, only the factors having __eigen-values__ _(represent variance explained each factor from the total variance)_ greater than 1 are considered significant, and rest are not. We can perform this using __Scree Plot__. It is derived by plotting the eigen-values against the number of factors in their order of extraction, and the cut-off would be the point just before where eigen-value becomes less than 1.\n",
    "* __Priori Criterion__: A predetermined number of factors based on research objectives and/or prior research\n",
    "* __Percentage of Variance Criterion__: It is an approach based on achieving a specified cumulative percentage of total variance extracted by successive factors. The purpose is to ensure practical significance for the derived factors by ensuring that they explain at least a specified amount of variance.\n",
    "\n",
    "_Here, we would be performing Latent Root Criterion._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:42.771036Z",
     "start_time": "2020-06-03T15:05:42.546466Z"
    }
   },
   "outputs": [],
   "source": [
    "# scree plot\n",
    "value=\"Scree Plot\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING. Might take longer to process in case of factors greater than 200. </h2></div>'))\n",
    "        # generate the eigen value matrix\n",
    "        fa = FactorAnalyzer(rotation=None)\n",
    "        fa.fit(df_fact_anal)\n",
    "        ev, v = fa.get_eigenvalues()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # generate the scree plot\n",
    "        fig.add_trace(go.Scatter(x=list(range(1, df_fact_anal.shape[1]+1)), y=ev,\n",
    "                                 mode='markers+lines', marker_color='teal', name='Factor'))\n",
    "        \n",
    "        # highlight the point where latent root criterion is satisfied using green triangle\n",
    "        latent_fact = 0\n",
    "        eigen_val = 0\n",
    "        for i,val in enumerate(list(ev)):\n",
    "            if val < 1:\n",
    "                latent_fact = i\n",
    "                eigen_val = list(ev)[latent_fact-1]\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        fig.add_trace(go.Scatter(x=[latent_fact], y=[eigen_val], marker_symbol='x', marker_size=15,\n",
    "                                 opacity=0.75, mode='markers', marker_color='tan', name='Latent Root Factor'))\n",
    "        \n",
    "        # highlight the eigen value threshold\n",
    "        fig.add_shape(dict(type=\"line\", x0=0,y0=1, x1=df_fact_anal.shape[1]+1,y1=1,\n",
    "                           line=dict(color=\"maroon\",width=1,dash='dash')))\n",
    "        fig.add_trace(go.Scatter(x=[2.75], y=[0.9], text=[\"Eigen Value Threshold\"], \n",
    "                                 mode=\"text\", showlegend=False))\n",
    "        fig.update_layout(title='Scree Plot',title_x=0.5)\n",
    "        \n",
    "        fig.update_yaxes(title='Eigen Value')\n",
    "        fig.update_xaxes(title='Number of columns after scaling')\n",
    "        clear_output()\n",
    "        display(Markdown('__Highest number of factors__ that can be taken based on _Latent Root Criterion_ is __{}__'.format(latent_fact)))\n",
    "        fig.show(config={'displayLogo':False})\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor loading\n",
    "\n",
    "##### Step 2.2. : Generate the initial factor loading\n",
    "\n",
    "Once we figured the maximum number of factors required, we now need to obtain __factor loading__. It is a matrix that shows the relationship of each variable to the underlying factor. It shows the __correlation coefficient__ for observed variables and factors. We will generate a heat map to visualize the distribution of the factors across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:49.491232Z",
     "start_time": "2020-06-03T15:05:46.749198Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing factor loading\n",
    "value=\"Factor Loading\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
    "\n",
    "        if latent_fact>100 :\n",
    "            clear_output()\n",
    "            print(colored('Cannot display the result due to large number of factors!','red'))\n",
    "        else:    \n",
    "            # analyze the factors with the maximum number of factors obtained\n",
    "            fa_threshold_fact = FactorAnalyzer(n_factors = latent_fact)\n",
    "            fa_threshold_fact.fit(df_fact_anal)\n",
    "\n",
    "            # generate those factor values in a matrix\n",
    "            matrix_fa_threshold_fact = fa_threshold_fact.loadings_\n",
    "\n",
    "            # store the matrix in a dataframe\n",
    "            df_fa_threshold_fact = pd.DataFrame(data = matrix_fa_threshold_fact,\n",
    "                                                columns = ['fact_{}'.format(i) for i in range(latent_fact)],\n",
    "                                                index = df_fact_anal.columns)\n",
    "\n",
    "            # generate and display the plot\n",
    "            fig_height = 20\n",
    "            col_count = df_fa_threshold_fact.shape[0]\n",
    "            if col_count > 30:\n",
    "                fig_height = 30*((col_count/30) - (col_count//30)) + 30\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            plt.figure(figsize=(20,fig_height))\n",
    "            sns.heatmap(df_fa_threshold_fact.round(3), cmap='RdYlGn', annot=True, alpha=0.9)\n",
    "            clear_output()\n",
    "            # display the heatmap\n",
    "            display(Markdown('__Factor Loading Heatmap__ '))\n",
    "            plt.show()\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance explained\n",
    "\n",
    "Now, we need to obtain the __variance of each factor__.\n",
    "\n",
    "* The __SS loadings__ row is the sum of squared loadings. This is sometimes used to determine the value of a particular factor. _We say a factor is worth keeping if the __SS loading is greater than 1__._\n",
    "* __Variance__ is simply the proportion of variance explained by each factor.\n",
    "* __Cumulative Variance__ tells us about the total variance explained by all the factors put together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:51.537506Z",
     "start_time": "2020-06-03T15:05:51.462710Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculating the variance of the factors\n",
    "value=\"Variance of factors\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
    "        if latent_fact>100:\n",
    "            clear_output()\n",
    "            print(colored('Cannot display the result due to large number of factors!','red'))\n",
    "        else:\n",
    "            # generating the variances across each factors\n",
    "            matrix_fa_var = fa_threshold_fact.get_factor_variance()\n",
    "\n",
    "            # naming the indexes\n",
    "            index_names = ['Sum of Square (SS) Loading', 'Variance', 'Cumulative Variance']\n",
    "\n",
    "            # store the matrix in a dataframe\n",
    "            df_fa_var = pd.DataFrame(data = matrix_fa_var, \n",
    "                                     columns = ['fact_{}'.format(i) for i in range(latent_fact)],\n",
    "                                     index = index_names)\n",
    "            clear_output()\n",
    "            # displaying the necessary information\n",
    "            display(df_fa_var.head(10).round(3))\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness and Communality\n",
    "\n",
    "__Uniqueness__ is the variance that is 'unique' to the variable and not shared with other variables. It ranges from 0 to 1. _A high uniqueness value implies that it __doesn't fit neatly__ into our factors_.\n",
    "\n",
    "If we subtract the uniqueness from 1, we get __communality__. The communality of a variable is the proportion of variance of that variable contributed by the common factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:53.190451Z",
     "start_time": "2020-06-03T15:05:53.046153Z"
    }
   },
   "outputs": [],
   "source": [
    "# obtaining the factors\n",
    "value = 'uniqueness communality'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        fa_fact = Factor(endog=df_fact_anal, n_factor=latent_fact)\n",
    "        fa_fact.fit()\n",
    "\n",
    "        # obtaining results from factor\n",
    "        fa_fact_result = FactorResults(fa_fact)\n",
    "\n",
    "        # finding uniqueness and communality\n",
    "        df_fa_unq_comm = pd.DataFrame({'columns':df_fact_anal.columns, 'uniqueness':fa_fact_result.uniqueness,\n",
    "                                      'communality':fa_fact_result.communality})\n",
    "\n",
    "        display_data(df_fa_unq_comm.round(4))\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "        clear_output()\n",
    "        print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Rotation\n",
    "\n",
    "##### Step 3 : Factor Rotation\n",
    "\n",
    "Here, we can perform factor rotation using different rotation techniques. By default, it performs `varimax`. Refer to the available sets of rotations [here](#rot_types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:56.259752Z",
     "start_time": "2020-06-03T15:05:54.511648Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing factor rotation\n",
    "value=\"Factor Rotation\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; PROCESSING</h2></div>'))\n",
    "\n",
    "        if latent_fact>100 :\n",
    "            clear_output()\n",
    "            print(colored('Cannot display the result due to large number of factors!','red'))\n",
    "        else:\n",
    "            # default method = 'varimax'\n",
    "            fa_rotator = Rotator(method='varimax')\n",
    "            # generate those factor values in a matrix\n",
    "            matrix_rot_fa_threshold_fact = fa_rotator.fit_transform(fa_threshold_fact.loadings_)\n",
    "            # store the matrix in a dataframe\n",
    "            df_rot_fa_threshold_fact = pd.DataFrame(data = matrix_rot_fa_threshold_fact, \n",
    "                                                    columns = ['fact_{}'.format(i) for i in range(latent_fact)],\n",
    "                                                    index = df_fact_anal.columns)\n",
    "            clear_output()\n",
    "\n",
    "            # generate and display the plot\n",
    "            fig_height = 20\n",
    "            col_count = df_rot_fa_threshold_fact.shape[0]\n",
    "            if col_count > 30:\n",
    "                fig_height = 30*((col_count/30) - (col_count//30)) + 30\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            plt.figure(figsize=(20,fig_height))\n",
    "            sns.heatmap(df_rot_fa_threshold_fact.round(3), cmap='RdYlGn', annot=True, alpha=0.9)\n",
    "            clear_output()\n",
    "            # display the heatmap\n",
    "            display(Markdown('__Factor Rotation Heatmap__ '))\n",
    "            plt.show()        \n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtained, we compare the variances across all the factors between __unrotated factor loading__ and __rotated factor loading__ and check the sum of their variance. If we notice __changes in the individual variances__, then we can conclude that we have __successfully uncorrelated the factors__, else it was uncorrelated without any requirement of rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:05:58.072666Z",
     "start_time": "2020-06-03T15:05:57.984194Z"
    }
   },
   "outputs": [],
   "source": [
    "# validating if the variances of the factors have reduced after rotation\n",
    "value=\"Validating variance of factors\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
    "        if latent_fact > 100 :\n",
    "            clear_output()\n",
    "            print(colored('Cannot display the result due to large number of factors!','red'))\n",
    "        else:\n",
    "            df_fa_var = pd.DataFrame(data = df_fa_threshold_fact.var(),\n",
    "                                            columns = ['Unrotated FA Variance'])\n",
    "\n",
    "            # generate the series for the rotated \n",
    "            df_fa_rot_var = pd.DataFrame(data = df_rot_fa_threshold_fact.var(),\n",
    "                                            columns = ['Rotated FA Variance'])\n",
    "\n",
    "            # merge the 2 series\n",
    "            df_fa_result = pd.concat([df_fa_var, df_fa_rot_var], axis=1)\n",
    "\n",
    "            # generating the sum of variances for each columns\n",
    "            df_fa_result.loc['sum_of_variance',:] = df_fa_result.sum(axis=0)\n",
    "            clear_output()\n",
    "            # displaying the result\n",
    "            display(df_fa_result.T.round(3))\n",
    "\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        clear_output()\n",
    "        print(colored('ERROR!','red',attrs=['bold']), colored('{}'.format(err),'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "        track_cell(value, flag, err)\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Notes__:\n",
    " \n",
    "```\n",
    "*Add notes here*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generating HTML Report\n",
    "\n",
    "Execute the __following two cells__ to generate HTML report of this notebook. It will __not display any code cells__.\n",
    "\n",
    "__NOTE__ : \n",
    "1. Ensure you have __saved the notebook with the latest checkpoint__ before running this chunk.\n",
    "2. __Always__ execute these two cells if you are getting error while rendering.\n",
    "3. Depending on your __Jupyter nbconvert version__, you might need to change `html_embed` to `html` for proper rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:06:33.803078Z",
     "start_time": "2020-06-03T15:06:33.796847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute(`notebook_name = '${IPython.notebook.notebook_name}'`);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute(`notebook_name = '${IPython.notebook.notebook_name}'`);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:06:37.721737Z",
     "start_time": "2020-06-03T15:06:35.594095Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'track_cell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4a7a15a4e77e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Markdown' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4a7a15a4e77e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mtrack_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'track_cell' is not defined"
     ]
    }
   ],
   "source": [
    "value=\"Generating HTML report\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; LOADING</h2></div>'))\n",
    "\n",
    "        if platform.system() != 'Windows':\n",
    "            notebook_name = notebook_name.replace(' ','\\ ')\n",
    "\n",
    "        if platform.system() == 'Windows':\n",
    "            notebook_path = os.getcwd()+'\\{}'.format(notebook_name)\n",
    "            os.system('jupyter nbconvert \"{}\" --no-input --no-prompt --template toc2 --to=html'.format(notebook_path))\n",
    "\n",
    "        else:\n",
    "            notebook_path = os.getcwd()+'/{}'.format(notebook_name)\n",
    "            os.system('jupyter nbconvert {} --no-input --no-prompt --template toc2 --to=html'.format(notebook_path))\n",
    "\n",
    "        clear_output()\n",
    "        display(Markdown('<span style=\"color:darkgreen; font-size: 15px\"><b><i>HTML Report generated and saved in your current local folder!</i></b></span>'))\n",
    "        track_cell(value, flag)\n",
    "    except Exception as err:\n",
    "        # display the error\n",
    "#         print(colored('\\nERROR:','red',attrs=['bold']),colored(err,'grey'))\n",
    "        flag = 0\n",
    "        err = str(err)\n",
    "#         track_cell(value, flag, err)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Executive Summary\n",
    "\n",
    "```\n",
    "*Add summary here*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "    <i>&copy; 2020 Copyright Mu Sigma Inc.</i>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1588579185861,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "662.717px",
    "left": "35px",
    "top": "111.133px",
    "width": "381.922px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
